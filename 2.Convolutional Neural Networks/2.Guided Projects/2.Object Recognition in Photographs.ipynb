{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Recognition in Photographs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A difficult problem where traditional neural networks fall is called object recognition. It is where a model can identify objects in images. This lesson will discover how to develop and evaluate deep learning models for object recognition in Keras. After completing this step-by-step tutorial, you will know:\n",
    "\n",
    "* About the CIFAR-10 object recognition dataset and how to load and use it in Keras.\n",
    "* How to create a simple Convolutional Neural Network for object recognition.\n",
    "* How to lift performance by creating deeper Convolutional Neural Networks.\n",
    "\n",
    "Let's get started.\n",
    "\n",
    "**Note**: You may want to speed up the computation for this tutorial by using GPU rather than CPU hardware. This is a suggestion, not a requirement. The tutorial will work just fine on the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Photograph Object Recognition Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of automatically identifying objects in photographs is difficult because of the near-infinite number of permutations of objects, positions, lighting, etc. It's a tough problem. This is a well-studied problem in computer vision and, more recently, an important demonstration of deep learning capability. The Canadian Institute for Advanced Research developed a standard computer vision and deep learning dataset for this problem (CIFAR).\n",
    "\n",
    "The CIFAR-10 dataset consists of 60,000 photos divided into ten classes (hence the name CIFAR-10)<sup>[1](http://www.cs.toronto.edu/~kriz/cifar.html)</sup>. Classes include common objects such as airplanes, automobiles, birds, cats, and so on. The dataset is split in a standard way, where 50,000 images are used for training a model and the remaining 10,000 for evaluating its performance. The photos are in color with red, green, and blue channels but are small, measuring 32 x 32-pixel squares.\n",
    "\n",
    "The CIFAR-10 dataset consists of 60,000 photos divided into ten classes (hence the name CIFAR-10)1. Classes include common objects such as airplanes, automobiles, birds, cats, and so on. The dataset is split in a standard way, where 50,000 images are used for training a model and the remaining 10,000 for evaluating its performance. The photos are in color with red, green, and blue channels but are small, measuring 32 x 32-pixel squares.\n",
    "\n",
    "State-of-the-art results can be achieved using very large convolutional neural networks. You can learn about state-of-the-art results on CIFAR-10 on Rodrigo Benenson's webpage<sup>[2](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html)</sup>. Model performance is reported in classification accuracy, with very good performance above 90% with human performance on the problem at 94% and state-of-the-art results at 96% at the time of writing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading The CIFAR-10 Dataset in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CIFAR-10 dataset can easily be loaded in Keras. Keras has the facility to automatically download standard datasets like CIFAR-10 and store them in the `~/.keras/datasets` directory using the `cifar10.load_data()` function. This dataset is large at 163 megabytes, so it may take a few minutes to download. Once downloaded, subsequent calls to the function will load the dataset ready for use. \n",
    "\n",
    "The dataset is stored as Python pickled training and test sets, ready for use in Keras. Each image is represented as a three-dimensional matrix, with dimensions for red, green, blue, width, and height. We can plot images directly using the Matplotlib Python plotting library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Listing 21.1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code, create a 3 x 3 plot of photographs. The images have been scaled up from their small 32 x 32 sizes, but you can see trucks, horses, and cars. You can also see some distortion in the images that have been forced to the square aspect ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Figure 21.1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple CNN for CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CIFAR-10 problem is best solved using a convolutional neural network (CNN). We can quickly start by importing all of the classes and functions we will need in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Listing 21.2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is good practice, we next initialize the random number seed with a constant to ensure the results are reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Listing 21.3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can load the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Listing 21.4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pixel values range from 0 to 255 for each of the red, green, and blue channels. It is good practice to work with normalized data. Because the input values are well understood, we can easily normalize to the range 0 to 1 by dividing each value by the maximum observation, 255. Note, the data is loaded as integers, so we must cast it to float point values to perform the division."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Listing 21.5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output variables are defined as a vector of integers from 0 to 1 for each class. We can use one-hot encoding to transform them into a binary matrix to best model the classification problem. We know there are ten classes for this problem so that we can expect the binary matrix to have a width of 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Listing 21.6`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining a simple CNN structure as a baseline and evaluate how well it performs on the problem. We will use a structure with two convolutional layers followed by max-pooling and a flattening out of the network to fully connected layers to make predictions. Our baseline network structure can be summarized as follows:\n",
    "\n",
    "1. Convolutional input layer, 32 feature maps with a size of 3 \u0002 3, a rectifier activation function, and a weight constraint of max norm set to 3.\n",
    "2. Dropout set to 20%.\n",
    "3. Convolutional layer, 32 feature maps with a size of 3 x 3, a rectifier activation function, and a weight constraint of max norm set to 3.\n",
    "4. Max Pool layer with the size 2 x 2.\n",
    "5. Flatten layer.\n",
    "6. Fully connected layer with 512 units and a rectifier activation function.\n",
    "7. Dropout set to 50%.\n",
    "8. Fully connected output layer with ten units and a softmax activation function.\n",
    "\n",
    "A logarithmic loss function is used with the stochastic gradient descent optimization algorithm configured with a large momentum and weight decay, starting with a learning rate of 0.01. A visualization of the network structure is provided below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Figure 21.2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Listing 21.7`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit this model with 25 epochs and a batch size of 32. A small number of epochs was chosen to help keep this tutorial moving. Normally, the number of epochs would be one or two orders of magnitude larger for this problem. Once the model is fit, we evaluate it on the test dataset and print out the classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Listing 21.8`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full code listing is provided below for completeness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Listing 21.9`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification accuracy and loss are printed each epoch on both the training and test datasets. The model is evaluated on the test set and achieves an accuracy of 70.85%, which is good but not excellent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Listing 21.10`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve the accuracy significantly by creating a much deeper network. This is what we will look at in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Larger CNN for CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that a simple CNN performs poorly on this complex problem. In this section, we look at scaling up the size and complexity of our model. Let's design a deep version of the simple CNN above. We can introduce an additional round of convolutions with many more feature maps. We will use the same pattern of Convolutional, Dropout, Convolutional, and Max Pooling layers.\n",
    "\n",
    "This pattern will be repeated three times with 32, 64, and 128 feature maps. The effect will be an increasing number of feature maps with a smaller and smaller size given the max-pooling layers. Finally, an additional and larger Dense layer will be used at the network's output end to translate better the large number feature maps to class values. We can summarize a new network architecture as follows:\n",
    "\n",
    "1. A convolutional input layer, 32 feature maps with a size of 3 x 3, and a rectifier activation function.\n",
    "2. Dropout layer at 20%.\n",
    "3. Convolutional layer, 32 feature maps with a size of 3 x 3, and a rectifier activation function.\n",
    "4. Max Pool layer with size 2 x 2.\n",
    "5. Convolutional layer, 64 feature maps with a size of 3 x 3, and a rectifier activation function.\n",
    "6. Dropout layer at 20%.\n",
    "7. Convolutional layer, 64 feature maps with a size of 3 x 3, and a rectifier activation function.\n",
    "8. Max Pool layer with size 2 x 2.\n",
    "9. Convolutional layer, 128 feature maps with a size of 3 x 3, and a rectifier activation function.\n",
    "10. Dropout layer at 20%.\n",
    "11. Convolutional layer, 128 feature maps with a size of 3 x 3, and a rectifier activation function.\n",
    "12. Max Pool layer with size 2 x 2.\n",
    "13. Flatten layer.\n",
    "14. Dropout layer at 20%.\n",
    "15. Fully connected layer with 1,024 units and a rectifier activation function.\n",
    "16. Dropout layer at 20%.\n",
    "17. Fully connected layer with 512 units and a  rectifier activation function.\n",
    "18. Dropout layer at 20%.\n",
    "19. Fully connected output layer with ten units and a softmax activation function.\n",
    "\n",
    "This is a larger network and a bit unwieldy to visualize. We can fit and evaluate this model using the same procedure above and the same number of epochs but a larger batch size of 64, found through some minor experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Listing 21.11`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this example prints the classification accuracy and loss on the training and test datasets each epoch. The estimate of classification accuracy for the final model is 80.18%, which is nearly 10 points better than our simpler model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Listing 21.12`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions To Improve Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have achieved good results on this very difficult problem, but we are still a good way to achieve world-class results. Below are some ideas that you can try to extend upon the model and improve model performance.\n",
    "\n",
    "* **Train for More Epochs**. Each model was trained for a very small number of epochs, 25. It is common to train large convolutional neural networks for hundreds or thousands of epochs. I would expect that performance gains can be achieved by significantly raising the number of training epochs.\n",
    "* **Image Data Augmentation**. The objects in the image vary in their position. Another boost in model performance can likely be achieved by using some data augmentation. Methods such as standardization and random shifts and horizontal image flips may be beneficial.\n",
    "* **Deeper Network Topology**. The larger network presented is deep, but larger networks could be designed for the problem. This may involve more feature maps closer to the input and perhaps less aggressive pooling. Additionally, standard convolutional network topologies that have been shown useful may be adopted and evaluated on the problem.\n",
    "\n",
    "What accuracy can you achieve on this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, you discovered how to create deep learning models in Keras for object recognition in photographs. After working through this tutorial, you learned:\n",
    "\n",
    "* About the CIFAR-10 dataset and how to load it in Keras and plot ad hoc examples from the dataset.\n",
    "* How to train and evaluate a simple Convolutional Neural Network on the problem.\n",
    "* How to expand a simple convolutional neural network into a deep convolutional neural network to boost performance on the difficult problem.\n",
    "* How to use data augmentation to get a further boost on the difficult object recognition problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CIFAR-10 dataset does present a difficult challenge, and you now know how to develop much larger convolutional neural networks. A clever feature of this network is that it can be used to learn the spatial structure in other domains, such as in sequences of words. In the next chapter, you will work through the application of a one-dimensional convolutional neural network to a natural language processing problem of sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
