{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Deep-Learning-Challenge/challenge-notebooks/blob/master/2.Convolutional%20Neural%20Networks/1.Lessons/1.Crash%20Course%20In%20Convolutional%20Neural%20Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crash Course In Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks are a powerful artificial neural network technique. These networks preserve the problem's spatial structure and were developed for object recognition tasks such as handwritten digit recognition. They are popular because people are achieving state-of-the-art results on challenging computer vision and natural language processing tasks. In this lesson, you will discover Convolutional Neural Networks for deep learning, called ConvNets or CNNs. After completing this crash course, you will know:\n",
    "\n",
    "* The building blocks used in CNNs such as convolutional layers and pool layers.\n",
    "* How the building blocks fit together with a short worked example.\n",
    "* Best practices for configuring CNNs on your object recognition tasks.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Case for Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a dataset of grayscale images with the standardized size of 32 x 32 pixels each, a traditional feedforward neural network would require 1,024 input weights (plus one bias). This is fair enough, but the flattening of the image matrix of pixels to a long vector of pixel values loses all of the image's spatial structure. Unless all of the images are perfectly resized, the neural network will have great difficulty with the problem.\n",
    "\n",
    "Convolutional Neural Networks expect and preserve the spatial relationship between pixels by learning internal feature representations using small squares of input data. Features are learned and used across the whole image, allowing for the objects in the images to be shifted or translated in the scene and still detectable by the network. This is why the network is so useful for object recognition in photographs, picking out digits, faces, objects, and so on with varying orientations. In summary, below are some of the benefits of using convolutional neural networks:\n",
    "\n",
    "* They use fewer parameters (weights) to learn than a fully connected network.\n",
    "* They are designed to be invariant to object position and distortion in the scene.\n",
    "* They automatically learn and generalize features from the input domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Blocks of Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three types of layers in a Convolutional Neural Network:\n",
    "\n",
    "1. Convolutional Layers.\n",
    "2. Pooling Layers.\n",
    "3. Fully-Connected Layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional layers are comprised of filters and feature maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filters are essentially the neurons of the layer. They have both weighted inputs and generate an output value like a neuron. The input size is a fixed square called a patch or a receptive field. If the convolutional layer is an input layer, then the input patch will be pixel values. If they deeper in the network architecture, then the convolutional layer will take input from a feature map from the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature map is the output of one filter applied to the previous layer. A given filter is drawn across the entire previous layer and moved one pixel at a time. Each position results in activation of the neuron, and the output is collected in the feature map. You can see that if the the receptive field is moved one pixel from activation to activation, then the field will overlap with the previous activation by (field width - 1) input values.\n",
    "\n",
    "The distance that filter is moved across the input from the previous layer each activation is referred to as the stride. If the size of the previous layer is not cleanly divisible by the size of the filter's receptive field and the stride's size, it is possible for the receptive field to attempt to read off the input's edge feature map. In this case, techniques like zero padding can be used to invent mock inputs with zero values for the receptive field to read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pooling layers down-sample the previous layers feature map. Pooling layers follow a sequence of one or more convolutional layers and are intended to consolidate the features learned and expressed in the previous layer's feature map. As such, pooling may be considered a technique to compress or generalize feature representations and generally reduce the model's training data's overfitting.\n",
    "\n",
    "They, too, have a receptive field, often much smaller than the convolutional layer. Also, the stride or number of inputs that the receptive field is moved for each activation is often equal to the receptive field's size to avoid any overlap. Pooling layers are usually very simple, taking the average or maximum input value to create its feature map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully connected layers are the standard flat feedforward neural network layer. These layers may have a nonlinear activation function or a softmax activation to output probabilities of class predictions. Fully connected layers are used at the end of the network after feature extraction and consolidation have been performed by the convolutional and pooling layers. They are used to create final nonlinear combinations of features and for making predictions by the\n",
    "network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worked Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now know about convolutional, pooling, and fully connected layers. Let's make this more concrete by working through how these three layers may be connected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we have a dataset of grayscale images. Each image has the same size of 32 pixels wide and 32 pixels high, and pixel values are between 0 and 255, e.g., a matrix of `32 x 32 x 1` or 1,024 pixel values. Image input data is expressed as a 3-dimensional matrix of `width x height x channels`. If we were using color images in our example, we would have three channels for the red, green, and blue pixel values, e.g., 32 x 32 x 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a convolutional layer with ten filters and a receptive field 5 pixels wide and 5 pixels high and a stride length of 1. Because each filter can only get input from (i.e., see) 5 x 5 (25) pixels at a time, we can calculate that each will require 25 + 1 input weights (plus 1 for the bias input). Dragging the 5 x 5 receptive field across the input image data with a stride width of 1 will result in a feature map of 28 x 28 output values or 784 distinct activations per image.\n",
    "\n",
    "We have ten filters, so that is ten different 28 x 28 feature maps or 7,840 outputs created for one image. Finally, we know we have 26 inputs per filter, ten filters, and 28 x 28 output values to calculate per filter, therefore we have a total of `26 x 10 x 28 x 28` or 203,840 connections in our convolutional layer, we want to phrase it using traditional neural network nomenclature. Convolutional layers also use a nonlinear transfer function as part of the activation, and the rectifier activation function is the widespread default to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pool Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a pooling layer with a receptive field with a width of 2 inputs and a height of 2 inputs.\n",
    "We also use a stride of 2 to ensure that there is no overlap. This results in feature maps that are one-half the size of the input feature maps. From 10 different 28 x 28 feature maps as input to 10 different 14x14 feature maps as output. We will use a max() operation for each receptive\n",
    "the field so that the activation is the maximum input value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can flatten out the square feature maps into a traditional fully connected layer. We can define the fully connected layer with 200 hidden neurons, each with 10 x 14 x 14 input connections, or 1,960 + 1 weight per neuron. That is a total of 392,200 connections and weights to learn in this layer. We can use a sigmoid or softmax transfer function to output probabilities of class values directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know about the building blocks for a convolutional neural network and how the layers hang together, we can review some best practices to consider when applying them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Input Receptive Field Dimensions**: The default is 2D for images, but it could be 1D, such as for words in a sentence or 3D for a video that adds a time dimension.\n",
    "* **Receptive Field Size**: The patch should be as small as possible but large enough to see the input data features. It is common to use 3 x 3 on small images and 5 x 5 or 7 x 7 and larger image sizes.\n",
    "* **Stride Width**: Use the default stride of 1. It is easy to understand, and you don't need padding to handle the receptive field falling off the edge of your images. This could be increased to 2 or larger for larger images.\n",
    "* **Number of Filters**: Filters are the feature detectors. Fewer filters are generally used at the input layer, and increasingly more filters are used at deeper layers.\n",
    "* **Padding**: Set to zero and called zero padding when reading non-input data. This is useful when you cannot or do not want to standardize input image sizes or when you want to use receptive field and stride sizes that do not neatly divide up the input image size.\n",
    "* **Pooling**: Pooling is a destructive or generalization process to reduce overfitting. Receptive field size is almost always set to 2 x 2 with a stride of 2 to discard 75% of the activations from the previous layer's output.\n",
    "* **Data Preparation**: Consider standardizing input data, both the dimensions of the images and pixel values.\n",
    "* **Pattern Architecture**: It is common to pattern the layers in your network architecture. This might be one, two, or some number of convolutional layers followed by a pooling layer. This structure can then be repeated one or more times. Finally, fully connected layers are often only used at the output end and maybe stacked one, two, or deeper. \n",
    "* **Dropout**: CNNs have a habit of overfitting, even with pooling layers. Dropout should be used, such as between fully connected layers and perhaps after pooling layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, you discovered convolutional neural networks. You learned:\n",
    "* Why CNN's are needed to preserve spatial structure in your input data and the benefits they provide.\n",
    "* The building blocks of CNN including convolutional, pooling, and fully connected layers.\n",
    "* How the layers in a CNN hang together.\n",
    "* Best practices when applying CNN to your problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now know about convolutional neural networks. In the next section, you will discover how to develop your first convolutional neural network in Keras for a handwriting digit recognition problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
