{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Deep-Learning-Challenge/challenge-notebooks/blob/master/1.Multilayer%20Perceptrons/3.Advanced%20Lessons/1.Save%20Your%20Models%20For%20Later%20With%20Serialization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Your Models For Later With Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that deep learning models can take hours, days, and even weeks to train, it is important to know how to save and load them from disk. In this lesson, you will discover how you can save your Keras models to file and load them up again to make predictions. After completing this lesson, you will know:\n",
    "\n",
    "* How to save and load Keras model weights to HDF5 formatted files.\n",
    "* How to save and load the Keras model structure to JSON files.\n",
    "* How to save and load the Keras model structure to YAML files.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "dataset_name = \"pima-indians-diabetes.data.csv\"\n",
    "if 'google.colab' in sys.modules:\n",
    "    DATASET = f\"https://github.com/Deep-Learning-Challenge/challenge-notebooks/raw/master/datasets/{dataset_name}\n",
    "else:\n",
    "    DATASET = f\"../../datasets/{dataset_name}\"\n",
    "    \n",
    "DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras separates the concerns of saving your model architecture and saving your model weights. Model weights are saved to HDF5 format. This is a grid format that is ideal for storing multi-dimensional arrays of numbers. The model structure can be described and saved (and loaded) using two different formats: JSON and YAML.\n",
    "\n",
    "Each example will also demonstrate saving and loading your model weights to HDF5 formatted files. The examples will use the same simple network trained on the Pima Indians the onset of diabetes binary classification dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDF5 Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hierarchical Data Format, or HDF5 for short, is a flexible data storage format and is convenient for storing large arrays of real values, as we have in the weights of neural networks. You may need to install Python support for the HDF5 file format. You can do this using your preferred Python package management system such as Pip:\n",
    "\n",
    "`sudo pip install h5py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Your Neural Network Model to JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON is a simple file format for describing data hierarchically. Keras provides the ability to describe any model using JSON format with a `to_json()` function. This can be saved to file and later loaded via the `model_from_json()` function that will create a new model from the JSON specification.\n",
    "\n",
    "The weights are saved directly from the model using the `save_weights()` function and later loaded using the symmetrical `load_weights()` function. The example below trains and evaluates a simple model on the Pima Indians dataset. The model structure is then converted to JSON format and written to `model.json` in the local directory. The network weights are written to `model.h5` in the local directory.\n",
    "\n",
    "The model and weight data are loaded from the saved files, and a new model is created. It is important to compile the loaded model before it is used. This is so that the model's predictions can use the appropriate, efficient computation from the Keras backend. The model is evaluated in the same way printing the same evaluation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# MLP for Pima Indians Dataset Serialize to JSON and HDF5\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import numpy\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(DATASET, delimiter=\",\")\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, Y, epochs=150, batch_size=10, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load json and create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JSON format of the model looks like the following:\n",
    "\n",
    "```JSON\n",
    "{\n",
    "  \"class_name\":\"Sequential\",\n",
    "  \"config\":{\n",
    "    \"name\":\"sequential\",\n",
    "    \"layers\":[\n",
    "      {\n",
    "        \"class_name\":\"InputLayer\",\n",
    "        \"config\":{\n",
    "          \"batch_input_shape\":[\n",
    "            null,\n",
    "            8\n",
    "          ],\n",
    "          \"dtype\":\"float32\",\n",
    "          \"sparse\":false,\n",
    "          \"ragged\":false,\n",
    "          \"name\":\"dense_input\"\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"class_name\":\"Dense\",\n",
    "        \"config\":{\n",
    "          \"name\":\"dense\",\n",
    "          \"trainable\":true,\n",
    "          \"batch_input_shape\":[\n",
    "            null,\n",
    "            8\n",
    "          ],\n",
    "          \"dtype\":\"float32\",\n",
    "          \"units\":12,\n",
    "          \"activation\":\"relu\",\n",
    "          \"use_bias\":true,\n",
    "          \"kernel_initializer\":{\n",
    "            \"class_name\":\"RandomUniform\",\n",
    "            \"config\":{\n",
    "              \"minval\":-0.05,\n",
    "              \"maxval\":0.05,\n",
    "              \"seed\":null\n",
    "            }\n",
    "          },\n",
    "          \"bias_initializer\":{\n",
    "            \"class_name\":\"Zeros\",\n",
    "            \"config\":{\n",
    "\n",
    "            }\n",
    "          },\n",
    "          \"kernel_regularizer\":null,\n",
    "          \"bias_regularizer\":null,\n",
    "          \"activity_regularizer\":null,\n",
    "          \"kernel_constraint\":null,\n",
    "          \"bias_constraint\":null\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"class_name\":\"Dense\",\n",
    "        \"config\":{\n",
    "          \"name\":\"dense_1\",\n",
    "          \"trainable\":true,\n",
    "          \"dtype\":\"float32\",\n",
    "          \"units\":8,\n",
    "          \"activation\":\"relu\",\n",
    "          \"use_bias\":true,\n",
    "          \"kernel_initializer\":{\n",
    "            \"class_name\":\"RandomUniform\",\n",
    "            \"config\":{\n",
    "              \"minval\":-0.05,\n",
    "              \"maxval\":0.05,\n",
    "              \"seed\":null\n",
    "            }\n",
    "          },\n",
    "          \"bias_initializer\":{\n",
    "            \"class_name\":\"Zeros\",\n",
    "            \"config\":{\n",
    "\n",
    "            }\n",
    "          },\n",
    "          \"kernel_regularizer\":null,\n",
    "          \"bias_regularizer\":null,\n",
    "          \"activity_regularizer\":null,\n",
    "          \"kernel_constraint\":null,\n",
    "          \"bias_constraint\":null\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"class_name\":\"Dense\",\n",
    "        \"config\":{\n",
    "          \"name\":\"dense_2\",\n",
    "          \"trainable\":true,\n",
    "          \"dtype\":\"float32\",\n",
    "          \"units\":1,\n",
    "          \"activation\":\"sigmoid\",\n",
    "          \"use_bias\":true,\n",
    "          \"kernel_initializer\":{\n",
    "            \"class_name\":\"RandomUniform\",\n",
    "            \"config\":{\n",
    "              \"minval\":-0.05,\n",
    "              \"maxval\":0.05,\n",
    "              \"seed\":null\n",
    "            }\n",
    "          },\n",
    "          \"bias_initializer\":{\n",
    "            \"class_name\":\"Zeros\",\n",
    "            \"config\":{\n",
    "\n",
    "            }\n",
    "          },\n",
    "          \"kernel_regularizer\":null,\n",
    "          \"bias_regularizer\":null,\n",
    "          \"activity_regularizer\":null,\n",
    "          \"kernel_constraint\":null,\n",
    "          \"bias_constraint\":null\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  \"keras_version\":\"2.4.0\",\n",
    "  \"backend\":\"tensorflow\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Your Neural Network Model to YAML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is much the same as the above JSON example, except the YAML format is used for the model specification. The model is described using YAML, saved to file `model.yaml`, and later loaded into a new model via the `model_from_yaml()` function. Weights are handled in the same way as above in HDF5 format as `model.h5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# MLP for Pima Indians Dataset Serialize to JSON and HDF5\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import model_from_yaml\n",
    "import numpy\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(DATASET, delimiter=\",\")\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, Y, epochs=150, batch_size=10, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# serialize model to YAML\n",
    "model_yaml = model.to_yaml()\n",
    "with open(\"model.yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)    \n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load YAML and create model\n",
    "yaml_file = open('model.yaml', 'r')\n",
    "loaded_model_yaml = yaml_file.read()\n",
    "yaml_file.close()\n",
    "loaded_model = model_from_yaml(loaded_model_yaml)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model described in YAML format looks like the following:\n",
    "\n",
    "```YAML\n",
    "backend: tensorflow\n",
    "class_name: Sequential\n",
    "config:\n",
    "  layers:\n",
    "  - class_name: InputLayer\n",
    "    config:\n",
    "      batch_input_shape: !!python/tuple\n",
    "      - null\n",
    "      - 8\n",
    "      dtype: float32\n",
    "      name: dense_3_input\n",
    "      ragged: false\n",
    "      sparse: false\n",
    "  - class_name: Dense\n",
    "    config:\n",
    "      activation: relu\n",
    "      activity_regularizer: null\n",
    "      batch_input_shape: !!python/tuple\n",
    "      - null\n",
    "      - 8\n",
    "      bias_constraint: null\n",
    "      bias_initializer:\n",
    "        class_name: Zeros\n",
    "        config: {}\n",
    "      bias_regularizer: null\n",
    "      dtype: float32\n",
    "      kernel_constraint: null\n",
    "      kernel_initializer:\n",
    "        class_name: RandomUniform\n",
    "        config:\n",
    "          maxval: 0.05\n",
    "          minval: -0.05\n",
    "          seed: null\n",
    "      kernel_regularizer: null\n",
    "      name: dense_3\n",
    "      trainable: true\n",
    "      units: 12\n",
    "      use_bias: true\n",
    "  - class_name: Dense\n",
    "    config:\n",
    "      activation: relu\n",
    "      activity_regularizer: null\n",
    "      bias_constraint: null\n",
    "      bias_initializer:\n",
    "        class_name: Zeros\n",
    "        config: {}\n",
    "      bias_regularizer: null\n",
    "      dtype: float32\n",
    "      kernel_constraint: null\n",
    "      kernel_initializer:\n",
    "        class_name: RandomUniform\n",
    "        config:\n",
    "          maxval: 0.05\n",
    "          minval: -0.05\n",
    "          seed: null\n",
    "      kernel_regularizer: null\n",
    "      name: dense_4\n",
    "      trainable: true\n",
    "      units: 8\n",
    "      use_bias: true\n",
    "  - class_name: Dense\n",
    "    config:\n",
    "      activation: sigmoid\n",
    "      activity_regularizer: null\n",
    "      bias_constraint: null\n",
    "      bias_initializer:\n",
    "        class_name: Zeros\n",
    "        config: {}\n",
    "      bias_regularizer: null\n",
    "      dtype: float32\n",
    "      kernel_constraint: null\n",
    "      kernel_initializer:\n",
    "        class_name: RandomUniform\n",
    "        config:\n",
    "          maxval: 0.05\n",
    "          minval: -0.05\n",
    "          seed: null\n",
    "      kernel_regularizer: null\n",
    "      name: dense_5\n",
    "      trainable: true\n",
    "      units: 1\n",
    "      use_bias: true\n",
    "  name: sequential_1\n",
    "keras_version: 2.4.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving and loading models is an important capability for transplanting a deep learning model from research and development to operations. In this lesson, you discovered how to serialize your Keras deep learning models. You learned:\n",
    "\n",
    "* How to save model weights to HDF5 formatted files and load them again later.\n",
    "* How to save Keras model definitions to JSON files and load them again.\n",
    "* How to save Keras model definitions to YAML files and load them again."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
