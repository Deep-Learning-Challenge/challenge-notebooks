{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Deep-Learning-Challenge/challenge-notebooks/blob/master/1.Multilayer%20Perceptrons/3.Advanced%20Lessons/3.Understand%20Model%20Behavior%20During%20Training%20By%20Plotting%20History.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand Model Behavior During Training By Plotting History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can learn a lot about neural networks and deep learning models by observing their performance over time during training. In this lesson, you will discover how you can review and visualize deep learning models' performance during training in Python with Keras. After completing this lesson, you will know:\n",
    "\n",
    "* How to inspect the history metrics collected during training.\n",
    "* How to plot accuracy metrics on training and validation datasets during training.\n",
    "* How to plot model loss metrics on training and validation datasets during training.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "dataset_name = \"pima-indians-diabetes.data.csv\"\n",
    "if 'google.colab' in sys.modules:\n",
    "    DATASET = f\"https://github.com/Deep-Learning-Challenge/challenge-notebooks/raw/master/datasets/{dataset_name}\n",
    "else:\n",
    "    DATASET = f\"../../datasets/{dataset_name}\"\n",
    "    \n",
    "DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Model Training History in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides the capability to register callbacks when training a deep learning model. One of the default callbacks that is registered when training all deep learning models is the `History` callback. It records training metrics for each epoch. This includes the loss and the accuracy (for classification problems) and the loss and accuracy for the validation dataset if one is set.\n",
    "\n",
    "The history object is returned from calls to the `fit()` function used to train the model. Metrics are stored in a dictionary in the `history` member of the object returned. For example, you can list the metrics collected in a history object using the following snippet of code after a model is trained:\n",
    "\n",
    "```python\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "```\n",
    "\n",
    "For example, for a model trained on a classification problem with a validation dataset, this might produce the following listing:\n",
    "\n",
    "`['loss', 'accuracy', 'val_loss', 'val_accuracy']`\n",
    "\n",
    "We can use the data collected in the history object to create plots. The plots can provide an indication of useful things about the training of the model, such as:\n",
    "\n",
    "* Its speed of convergence over epochs (slope).\n",
    "* Whether the model may have already converged (plateau of the line).\n",
    "* Whether the model may be over-learning the training data (infection for validation line).\n",
    "\n",
    "And more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Model Training History in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create plots from the collected history data. In the example below, we create a small network to model the Pima Indians onset of diabetes binary classification problem. The example collects the history returned from training the model and creates two charts:\n",
    "\n",
    "1. A plot of accuracy on the training and validation datasets over training epochs.\n",
    "2. A plot of loss on the training and validation datasets over training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# MLP for Pima Indians Dataset Serialize to JSON and HDF5\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(DATASET, delimiter=\",\")\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, verbose=0)\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots are provided below. The history for the validation dataset is labeled test by convention as it is indeed a test dataset for the model. From the plot of accuracy we can see that the model could probably be trained a little more as the trend for accuracy on both datasets is still rising for the last few epochs. We can also see that the model has not yet over-learned the training dataset, showing comparable skill on both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model has comparable performance on both train and validation datasets (labeled test). If these parallel plots start to depart consistently, it might be a sign to stop training at an earlier epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, you discovered the importance of collecting and reviewing metrics during your deep learning models' training. You learned:\n",
    "\n",
    "* How to inspect a history object returned from training to discover the metrics that were collected.\n",
    "* How to extract model accuracy information for training and validation datasets and plot the data.\n",
    "* How to extract and plot the model loss information calculated from training and validation datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple yet very powerful technique for decreasing your model's overfitting to training data is called dropout. In the next lesson, you will discover the dropout technique, how to apply it to visible and hidden layers in Keras, and best practices for using it on your own problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
