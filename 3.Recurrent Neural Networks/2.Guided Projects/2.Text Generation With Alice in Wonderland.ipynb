{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation With Alice in Wonderland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent neural networks can also be used as generative models. This means that in addition to being used for predictive models (making predictions), they can learn the sequences of a problem and then generate entirely new plausible sequences for the problem domain. Generative models like this are helpful to study how well a model has learned a problem and learn more about the problem domain itself. In this project, you will discover how to create a generative model for text, character-by-character using LSTM recurrent neural networks in Python with Keras. After completing this project, you will know:\n",
    "\n",
    "* Where to download a free corpus of text that you can use to train generative text models.\n",
    "* How to frame the problem of text sequences to a recurrent neural network generative model.\n",
    "* How to develop an LSTM to generate plausible text sequences for a given problem.\n",
    "\n",
    "Let's get started.\n",
    "\n",
    "**Note:** You may want to speed up the computation for this tutorial by using GPU rather than CPU hardware. This is a suggestion, not a requirement. The tutorial will work just fine on the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description: Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the classical texts are no longer protected under copyright. This means that you can download all of the text for these books for free and use them in experiments, like creating generative models. Perhaps the best place to access free books that are no longer protected by copyright is Project Gutenberg <sup>[1](https://www.gutenberg.org/)</sup>. This tutorial will use a favorite book from childhood as the dataset: Alice's Adventures in Wonderland by Lewis Carroll<sup>[2](https://www.gutenberg.org/ebooks/11)</sup>.\n",
    "\n",
    "We will learn the dependencies between characters and the conditional probabilities of characters in sequences so that we can generate wholly new and original sequences of characters. This tutorial is a lot of fun, and I recommend repeating these experiments with other books from Project Gutenberg. These experiments are not limited to text; you can also experiment with other ASCII data, such as computer source code, marked-up documents in LaTeX, HTML or Markdown, and more.\n",
    "\n",
    "You can download the complete text in ASCII format (Plain Text UTF-8)<sup>[3](http://www.gutenberg.org/cache/epub/11/pg11.txt)</sup> for this book for free and place it in the `datasets` directory with the filename `wonderland.txt` (***already done for you***). Now we need to prepare the dataset ready for modeling. Project Gutenberg adds a standard header and footer to each book, which is not part of the original text. Open the file in a text editor and delete the header and footer. The header is obvious and ends with the text:\n",
    "\n",
    "`*** START OF THIS PROJECT GUTENBERG EBOOK ALICE'S ADVENTURES IN WONDERLAND ***`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The footer is all of the text after the line of text that says:\n",
    "\n",
    "`THE END`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be left with a text file that has about 3,330 lines of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop a Small LSTM Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will develop a simple LSTM network to learn sequences of characters from Alice in Wonderland. In the next section, we will use this model to generate new sequences of characters. Let's start by importing the classes and functions we intend to use to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to load the ASCII text for the book into memory and convert all of the characters to lowercase to reduce the network's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"../../datasets/wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the book is loaded, we must prepare the data for modeling by the neural network. We cannot model the characters directly; instead, we must convert the characters to integers. We can do this easily by first creating a set of all of the distinct characters in the book, then creating a map of each character to a unique integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers, and a reverse mapping\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the list of unique sorted lowercase characters in the book is as follows:\n",
    "\n",
    "```\n",
    "['\\n', '\\r', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', ':', ';', '?', '[', ']',\n",
    "'_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p',\n",
    "'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xbb', '\\xbf', '\\xef']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there may be some characters that we could remove to further clean up the dataset that will reduce the vocabulary and may improve the modeling process. Now that the book has been loaded and the mapping prepared, we can summarize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144431\n",
      "Total Vocab:  46\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the book has just under 150,000 characters and that when converted to lowercase, there are only 47 distinct characters in the vocabulary for the network to learnâ€”much more than the 26 in the alphabet. We now need to define the training data for the network. There is a lot of flexibility in choosing to break up the text and expose it to the network during training. This tutorial will split the book text up into subsequences with a fixed length of 100 characters, an arbitrary length. We could just as easily split the data up by sentences, pad the shorter sequences, and truncate the longer ones.\n",
    "\n",
    "Each training pattern of the network comprises 100-time steps of one character (X) followed by one character output (y). When creating these sequences, we slide this window along the whole book one character at a time, allowing each character a chance to be learned from the 100 characters that preceded it (except the first 100 characters, of course). For example, if the sequence length is 5 (for simplicity), then the first two training patterns would be as follows:\n",
    "\n",
    "```\n",
    "CHAPT -> E\n",
    "HAPTE -> R\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we split up the book into these sequences, we convert the characters to integers using our lookup table we prepared earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  144331\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code to this point shows us that when we split up the dataset into training data for the network to learn that we have just under 150,000 training pattens. This makes sense as excluding the first 100 characters; we have one training pattern to predict each of the remaining characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have prepared our training data, we need to transform it to be suitable for use with Keras. First, we must transform the list of input sequences into the form [samples, time steps, features] expected by an LSTM network. Next, we need to rescale the integers to the range 0-to-1 to make the patterns easier to learn by the LSTM network that uses the sigmoid\n",
    "activation function by default.\n",
    "\n",
    "Finally, we need to convert the output patterns (single characters converted to integers) into a one-hot encoding. This is to configure the network to predict the probability of each of the 47 different characters in the vocabulary (an easier representation) rather than trying to force it to predict precisely the next character. Each y value is converted into a sparse vector with a length of 47, full of zeros, except with a 1 in the column for the letter (integer) that the pattern represents. For example, when n (integer value 31) is one hot encoded, it looks as follows:\n",
    "\n",
    "```\n",
    "[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
    "0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
    "0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement these steps as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our LSTM model. Here we define a single hidden LSTM layer with 256 memory units. The network uses dropout with a probability of 20%. The output layer is a Dense layer using the softmax activation function to output a probability prediction for each of the 47 characters between 0 and 1. The problem is a single character classification problem with 47 classes and, as such, is defined as optimizing the log loss (cross-entropy), here using the Adam optimization algorithm for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no test dataset. We are modeling the entire training dataset to learn the probability of each character in a sequence. We are not interested in the most accurate (classification accuracy) model of the training dataset. This would be a model that predicts each character in the training dataset perfectly. Instead, we are interested in a generalization of the dataset\n",
    "that minimizes the chosen loss function. We are seeking a balance between generalization and overfitting but short of memorization.\n",
    "\n",
    "The network is slow to train (about 300 seconds per epoch on an Nvidia K520 GPU). Because of the slowness and our optimization requirements, we will use model checkpointing to record all of the network weights to file each time an improvement in loss is observed at the end of the epoch. We will use the best set of weights (lowest loss) to instantiate our generative model in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, \n",
    "                             monitor='loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fit our model to the data. Here we use a modest number of 20 epochs and a large batch size of 128 patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1128/1128 [==============================] - 10s 7ms/step - loss: 3.0545\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.96642, saving model to weights-improvement-01-2.9664.hdf5\n",
      "Epoch 2/20\n",
      "1128/1128 [==============================] - 8s 7ms/step - loss: 2.7966\n",
      "\n",
      "Epoch 00002: loss improved from 2.96642 to 2.76319, saving model to weights-improvement-02-2.7632.hdf5\n",
      "Epoch 3/20\n",
      "1128/1128 [==============================] - 8s 7ms/step - loss: 2.6842\n",
      "\n",
      "Epoch 00003: loss improved from 2.76319 to 2.66228, saving model to weights-improvement-03-2.6623.hdf5\n",
      "Epoch 4/20\n",
      "1128/1128 [==============================] - 8s 7ms/step - loss: 2.6022\n",
      "\n",
      "Epoch 00004: loss improved from 2.66228 to 2.58135, saving model to weights-improvement-04-2.5814.hdf5\n",
      "Epoch 5/20\n",
      "1128/1128 [==============================] - 8s 7ms/step - loss: 2.5206\n",
      "\n",
      "Epoch 00005: loss improved from 2.58135 to 2.51255, saving model to weights-improvement-05-2.5126.hdf5\n",
      "Epoch 6/20\n",
      "1128/1128 [==============================] - 8s 7ms/step - loss: 2.4666\n",
      "\n",
      "Epoch 00006: loss improved from 2.51255 to 2.45334, saving model to weights-improvement-06-2.4533.hdf5\n",
      "Epoch 7/20\n",
      "1128/1128 [==============================] - 8s 7ms/step - loss: 2.4052\n",
      "\n",
      "Epoch 00007: loss improved from 2.45334 to 2.40188, saving model to weights-improvement-07-2.4019.hdf5\n",
      "Epoch 8/20\n",
      "1128/1128 [==============================] - 8s 7ms/step - loss: 2.3545\n",
      "\n",
      "Epoch 00008: loss improved from 2.40188 to 2.35042, saving model to weights-improvement-08-2.3504.hdf5\n",
      "Epoch 9/20\n",
      "1128/1128 [==============================] - 8s 7ms/step - loss: 2.3019\n",
      "\n",
      "Epoch 00009: loss improved from 2.35042 to 2.30523, saving model to weights-improvement-09-2.3052.hdf5\n",
      "Epoch 10/20\n",
      "1128/1128 [==============================] - 8s 7ms/step - loss: 2.2581\n",
      "\n",
      "Epoch 00010: loss improved from 2.30523 to 2.26157, saving model to weights-improvement-10-2.2616.hdf5\n",
      "Epoch 11/20\n",
      "1128/1128 [==============================] - 8s 7ms/step - loss: 2.2223\n",
      "\n",
      "Epoch 00011: loss improved from 2.26157 to 2.22017, saving model to weights-improvement-11-2.2202.hdf5\n",
      "Epoch 12/20\n",
      "1128/1128 [==============================] - 8s 7ms/step - loss: 2.1733\n",
      "\n",
      "Epoch 00012: loss improved from 2.22017 to 2.17822, saving model to weights-improvement-12-2.1782.hdf5\n",
      "Epoch 13/20\n",
      "1128/1128 [==============================] - 8s 7ms/step - loss: 2.1323\n",
      "\n",
      "Epoch 00013: loss improved from 2.17822 to 2.13983, saving model to weights-improvement-13-2.1398.hdf5\n",
      "Epoch 14/20\n",
      "1128/1128 [==============================] - 8s 7ms/step - loss: 2.0967\n",
      "\n",
      "Epoch 00014: loss improved from 2.13983 to 2.10414, saving model to weights-improvement-14-2.1041.hdf5\n",
      "Epoch 15/20\n",
      "1128/1128 [==============================] - 8s 7ms/step - loss: 2.0623\n",
      "\n",
      "Epoch 00015: loss improved from 2.10414 to 2.07197, saving model to weights-improvement-15-2.0720.hdf5\n",
      "Epoch 16/20\n",
      "1128/1128 [==============================] - 8s 7ms/step - loss: 2.0290\n",
      "\n",
      "Epoch 00016: loss improved from 2.07197 to 2.04135, saving model to weights-improvement-16-2.0413.hdf5\n",
      "Epoch 17/20\n",
      "1128/1128 [==============================] - 8s 7ms/step - loss: 2.0054\n",
      "\n",
      "Epoch 00017: loss improved from 2.04135 to 2.01000, saving model to weights-improvement-17-2.0100.hdf5\n",
      "Epoch 18/20\n",
      "1128/1128 [==============================] - 8s 7ms/step - loss: 1.9705\n",
      "\n",
      "Epoch 00018: loss improved from 2.01000 to 1.98334, saving model to weights-improvement-18-1.9833.hdf5\n",
      "Epoch 19/20\n",
      "1128/1128 [==============================] - 8s 7ms/step - loss: 1.9421\n",
      "\n",
      "Epoch 00019: loss improved from 1.98334 to 1.95309, saving model to weights-improvement-19-1.9531.hdf5\n",
      "Epoch 20/20\n",
      "1128/1128 [==============================] - 8s 7ms/step - loss: 1.9140\n",
      "\n",
      "Epoch 00020: loss improved from 1.95309 to 1.92769, saving model to weights-improvement-20-1.9277.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8d50199668>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see different results because of the stochastic nature of the model and because it is hard to fix the random seed for LSTM models to get 100% reproducible results. This is not a concern for this generative model. After running the example, you should have a number of weight checkpoint files in the local directory. You can delete them all except the one with the smallest loss value. For example, below was the checkpoint with the smallest loss we achieved when we ran this example.\n",
    "\n",
    "`weights-improvement-20-1.9277.hdf5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network loss decreased almost every epoch, and I expect the network could benefit from training for many more epochs. In the next section, we will look at using this model to generate new text sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Text with an LSTM Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating text using the trained LSTM network is relatively straightforward. Firstly, we load the data and define the network in the same way, except the network weights are loaded from a checkpoint file, and the network does not need to be trained.\n",
    "\n",
    "**Note:** It seems that you might need to use the same machine/environment to generate text as was used to create the network weights (e.g., GPUs or CPUs); otherwise, the network might generate garbage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-20-1.9277.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When preparing the mapping of unique characters to integers, we must also create a reverse mapping that we can use to convert the integers back to characters to understand the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to make predictions. The simplest way to use the Keras LSTM model to make predictions is to start with a seed sequence as input, generate the next character, and then update the seed sequence to add the generated character to the end and trim off the first character. This process is repeated for as long as we want to predict new characters (e.g., a sequence of 1,000 characters in length). We can pick a random input pattern as our seed\n",
    "sequence, then print generated characters as we generate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\"  baby violently up and down, and the poor little thing howled so,\n",
      "that alice could hardly hear the w \"\n",
      "\n",
      "Generated:\n",
      "hite tablid thet sae oo the thitg, and she west on wirh a sorp of thicg so the thate, and she west on wirh a sore of thicg so the thate, and she west on wirh a sore of thicg so the thate, and she west on whrh a lott oi the seatin of the sabli, and she sest hn whlh a little toieted  and saed to the gorphon. \n",
      "'ie  you soonts ' said the manch hare.\n",
      "\n",
      "'ie ion't hate io wou taan ' said the manch hare.\n",
      "\n",
      "'ie ion't sate thing ' said the manch hare.\n",
      "\n",
      "'ie ion't sate thing ' said the manch hare.\n",
      "\n",
      "'ie ion't sate thing ' said the manch hare.\n",
      "\n",
      "'ie ion't sate thing ' said the manch hare.\n",
      "\n",
      "'ie ion't sate thing ' said the manch hare.\n",
      "\n",
      "'ie ion't sate thing ' said the manch hare.\n",
      "\n",
      "'ie ion't sate thing ' said the manch hare.\n",
      "\n",
      "'ie ion't sate thing ' said the manch hare.\n",
      "\n",
      "'ie ion't sate thing ' said the manch hare.\n",
      "\n",
      "'ie ion't sate thing ' said the manch hare.\n",
      "\n",
      "'ie ion't sate thing ' said the manch hare.\n",
      "\n",
      "'ie ion't sate thing ' said the manch hare.\n",
      "\n",
      "'ie ion't sate thing ' said the manch hare.\n",
      "\n",
      "'ie ion't sate \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "print(\"\\nGenerated:\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this example first outputs the selected random seed, then each character as it is generated. For example, below are the results from one run of this text generator. The random seed was:\n",
    "\n",
    "```\n",
    "baby violently up and down, and the poor little thing howled so,\n",
    "that alice could hardly hear the w\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated text with the random seed was:\n",
    "\n",
    "```\n",
    "hite tablid thet sae oo the thitg, and she west on wirh a sorp of thicg so the thate, and she west on wirh a sore of thicg so the thate, and she west on wirh a sore of thicg so the thate, and she west on whrh a lott oi the seatin of the sabli, and she sest hn whlh a little toieted  and saed to the gorphon. \n",
    "'ie  you soonts ' said the manch hare.\n",
    "\n",
    "'ie ion't hate io wou taan ' said the manch hare\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can note some observations about the generated text.\n",
    "\n",
    "* It generally conforms to the line format observed in the original text of fewer than 80 characters before a new line.\n",
    "* The characters are separated into word-like groups, and most groups are English words (e.g., the little and was), but many do not (e.g., lott, tiie, and taede).\n",
    "* Some of the words in sequence make sense(e.g., and the white rabbit), but many do not (e.g., wese tilel).\n",
    "\n",
    "The fact that this character-based model of the book produces output like this is very impressive. It gives you a sense of the learning capabilities of LSTM networks. The results are not perfect. In the next section, we look at improving the quality of results by developing a much larger LSTM network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Larger LSTM Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got results, but not excellent results in the previous section. Now, we can improve the quality of the generated text by creating a much larger network. We will keep the number of memory units the same at 256 but add a second layer.\n",
    "\n",
    "```\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also change the filename of the checkpointed weights to tell the difference between weights for this network and the previous (by appending the word bigger in the filename).\n",
    "\n",
    "`filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will increase the training epochs from 20 to 50 and decrease the batch size from 128 to 64 to give the network more of an opportunity to be updated and learn. The complete code listing is presented below for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144431\n",
      "Total Vocab:  46\n",
      "Total Patterns:  144331\n",
      "Epoch 1/50\n",
      "2256/2256 [==============================] - 31s 13ms/step - loss: 2.9515\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.78978, saving model to weights-improvement-01-2.7898-bigger.hdf5\n",
      "Epoch 2/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 2.4952\n",
      "\n",
      "Epoch 00002: loss improved from 2.78978 to 2.43368, saving model to weights-improvement-02-2.4337-bigger.hdf5\n",
      "Epoch 3/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 2.2721\n",
      "\n",
      "Epoch 00003: loss improved from 2.43368 to 2.23046, saving model to weights-improvement-03-2.2305-bigger.hdf5\n",
      "Epoch 4/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 2.1129\n",
      "\n",
      "Epoch 00004: loss improved from 2.23046 to 2.09418, saving model to weights-improvement-04-2.0942-bigger.hdf5\n",
      "Epoch 5/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 2.0049\n",
      "\n",
      "Epoch 00005: loss improved from 2.09418 to 1.99630, saving model to weights-improvement-05-1.9963-bigger.hdf5\n",
      "Epoch 6/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.9222\n",
      "\n",
      "Epoch 00006: loss improved from 1.99630 to 1.91818, saving model to weights-improvement-06-1.9182-bigger.hdf5\n",
      "Epoch 7/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.8574\n",
      "\n",
      "Epoch 00007: loss improved from 1.91818 to 1.85730, saving model to weights-improvement-07-1.8573-bigger.hdf5\n",
      "Epoch 8/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.8060\n",
      "\n",
      "Epoch 00008: loss improved from 1.85730 to 1.80349, saving model to weights-improvement-08-1.8035-bigger.hdf5\n",
      "Epoch 9/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.7540\n",
      "\n",
      "Epoch 00009: loss improved from 1.80349 to 1.75750, saving model to weights-improvement-09-1.7575-bigger.hdf5\n",
      "Epoch 10/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.7043\n",
      "\n",
      "Epoch 00010: loss improved from 1.75750 to 1.71942, saving model to weights-improvement-10-1.7194-bigger.hdf5\n",
      "Epoch 11/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.6695\n",
      "\n",
      "Epoch 00011: loss improved from 1.71942 to 1.67964, saving model to weights-improvement-11-1.6796-bigger.hdf5\n",
      "Epoch 12/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.6385\n",
      "\n",
      "Epoch 00012: loss improved from 1.67964 to 1.64573, saving model to weights-improvement-12-1.6457-bigger.hdf5\n",
      "Epoch 13/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.6111\n",
      "\n",
      "Epoch 00013: loss improved from 1.64573 to 1.61536, saving model to weights-improvement-13-1.6154-bigger.hdf5\n",
      "Epoch 14/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.5901\n",
      "\n",
      "Epoch 00014: loss improved from 1.61536 to 1.58989, saving model to weights-improvement-14-1.5899-bigger.hdf5\n",
      "Epoch 15/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.5459\n",
      "\n",
      "Epoch 00015: loss improved from 1.58989 to 1.55931, saving model to weights-improvement-15-1.5593-bigger.hdf5\n",
      "Epoch 16/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.5275\n",
      "\n",
      "Epoch 00016: loss improved from 1.55931 to 1.53705, saving model to weights-improvement-16-1.5370-bigger.hdf5\n",
      "Epoch 17/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.4974\n",
      "\n",
      "Epoch 00017: loss improved from 1.53705 to 1.51322, saving model to weights-improvement-17-1.5132-bigger.hdf5\n",
      "Epoch 18/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.4835\n",
      "\n",
      "Epoch 00018: loss improved from 1.51322 to 1.49289, saving model to weights-improvement-18-1.4929-bigger.hdf5\n",
      "Epoch 19/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.4568\n",
      "\n",
      "Epoch 00019: loss improved from 1.49289 to 1.47529, saving model to weights-improvement-19-1.4753-bigger.hdf5\n",
      "Epoch 20/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.4416\n",
      "\n",
      "Epoch 00020: loss improved from 1.47529 to 1.45376, saving model to weights-improvement-20-1.4538-bigger.hdf5\n",
      "Epoch 21/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.4315\n",
      "\n",
      "Epoch 00021: loss improved from 1.45376 to 1.43870, saving model to weights-improvement-21-1.4387-bigger.hdf5\n",
      "Epoch 22/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.4005\n",
      "\n",
      "Epoch 00022: loss improved from 1.43870 to 1.42127, saving model to weights-improvement-22-1.4213-bigger.hdf5\n",
      "Epoch 23/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.3945\n",
      "\n",
      "Epoch 00023: loss improved from 1.42127 to 1.40968, saving model to weights-improvement-23-1.4097-bigger.hdf5\n",
      "Epoch 24/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.3765\n",
      "\n",
      "Epoch 00024: loss improved from 1.40968 to 1.39857, saving model to weights-improvement-24-1.3986-bigger.hdf5\n",
      "Epoch 25/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.3678\n",
      "\n",
      "Epoch 00025: loss improved from 1.39857 to 1.38005, saving model to weights-improvement-25-1.3801-bigger.hdf5\n",
      "Epoch 26/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.3572\n",
      "\n",
      "Epoch 00026: loss improved from 1.38005 to 1.37009, saving model to weights-improvement-26-1.3701-bigger.hdf5\n",
      "Epoch 27/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.3464\n",
      "\n",
      "Epoch 00027: loss improved from 1.37009 to 1.36074, saving model to weights-improvement-27-1.3607-bigger.hdf5\n",
      "Epoch 28/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.3373\n",
      "\n",
      "Epoch 00028: loss improved from 1.36074 to 1.34822, saving model to weights-improvement-28-1.3482-bigger.hdf5\n",
      "Epoch 29/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.3224\n",
      "\n",
      "Epoch 00029: loss improved from 1.34822 to 1.33795, saving model to weights-improvement-29-1.3380-bigger.hdf5\n",
      "Epoch 30/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.3099\n",
      "\n",
      "Epoch 00030: loss improved from 1.33795 to 1.32511, saving model to weights-improvement-30-1.3251-bigger.hdf5\n",
      "Epoch 31/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.3013\n",
      "\n",
      "Epoch 00031: loss improved from 1.32511 to 1.31768, saving model to weights-improvement-31-1.3177-bigger.hdf5\n",
      "Epoch 32/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.2950\n",
      "\n",
      "Epoch 00032: loss improved from 1.31768 to 1.31138, saving model to weights-improvement-32-1.3114-bigger.hdf5\n",
      "Epoch 33/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.2914\n",
      "\n",
      "Epoch 00033: loss improved from 1.31138 to 1.30434, saving model to weights-improvement-33-1.3043-bigger.hdf5\n",
      "Epoch 34/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.2809\n",
      "\n",
      "Epoch 00034: loss improved from 1.30434 to 1.29611, saving model to weights-improvement-34-1.2961-bigger.hdf5\n",
      "Epoch 35/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.2662\n",
      "\n",
      "Epoch 00035: loss improved from 1.29611 to 1.28523, saving model to weights-improvement-35-1.2852-bigger.hdf5\n",
      "Epoch 36/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.2700\n",
      "\n",
      "Epoch 00036: loss improved from 1.28523 to 1.28334, saving model to weights-improvement-36-1.2833-bigger.hdf5\n",
      "Epoch 37/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.2603\n",
      "\n",
      "Epoch 00037: loss improved from 1.28334 to 1.28015, saving model to weights-improvement-37-1.2801-bigger.hdf5\n",
      "Epoch 38/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.2507\n",
      "\n",
      "Epoch 00038: loss improved from 1.28015 to 1.27020, saving model to weights-improvement-38-1.2702-bigger.hdf5\n",
      "Epoch 39/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.2475\n",
      "\n",
      "Epoch 00039: loss improved from 1.27020 to 1.26447, saving model to weights-improvement-39-1.2645-bigger.hdf5\n",
      "Epoch 40/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.2426\n",
      "\n",
      "Epoch 00040: loss improved from 1.26447 to 1.26017, saving model to weights-improvement-40-1.2602-bigger.hdf5\n",
      "Epoch 41/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.2409\n",
      "\n",
      "Epoch 00041: loss improved from 1.26017 to 1.25390, saving model to weights-improvement-41-1.2539-bigger.hdf5\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.2316\n",
      "\n",
      "Epoch 00042: loss improved from 1.25390 to 1.25174, saving model to weights-improvement-42-1.2517-bigger.hdf5\n",
      "Epoch 43/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.2311\n",
      "\n",
      "Epoch 00043: loss improved from 1.25174 to 1.24582, saving model to weights-improvement-43-1.2458-bigger.hdf5\n",
      "Epoch 44/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.2273\n",
      "\n",
      "Epoch 00044: loss improved from 1.24582 to 1.24230, saving model to weights-improvement-44-1.2423-bigger.hdf5\n",
      "Epoch 45/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.2321\n",
      "\n",
      "Epoch 00045: loss did not improve from 1.24230\n",
      "Epoch 46/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.2193\n",
      "\n",
      "Epoch 00046: loss improved from 1.24230 to 1.23469, saving model to weights-improvement-46-1.2347-bigger.hdf5\n",
      "Epoch 47/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.2134\n",
      "\n",
      "Epoch 00047: loss improved from 1.23469 to 1.22756, saving model to weights-improvement-47-1.2276-bigger.hdf5\n",
      "Epoch 48/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.2161\n",
      "\n",
      "Epoch 00048: loss did not improve from 1.22756\n",
      "Epoch 49/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.2069\n",
      "\n",
      "Epoch 00049: loss did not improve from 1.22756\n",
      "Epoch 50/50\n",
      "2256/2256 [==============================] - 30s 13ms/step - loss: 1.2009\n",
      "\n",
      "Epoch 00050: loss improved from 1.22756 to 1.22594, saving model to weights-improvement-50-1.2259-bigger.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8ce00b0208>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Larger LSTM Network to Generate Text for Alice in Wonderland\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"../../datasets/wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = utils.to_categorical(dataY)\n",
    "\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True,\n",
    "mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this example takes some time, at least 700 seconds per epoch. After running this example, you may achieve a loss of about 1.2. For example, the best result I achieved from running this model was stored in a checkpoint file with the name:\n",
    "\n",
    "`weights-improvement-50-1.2259-bigger.hdf5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Achieving a loss of 1.2219 at epoch 47. As in the previous section, we can use this best model from the run to generate text. The only change we need to make to the text generation script from the previous section is in the specification of the network topology and from which file to seed the network weights. The complete code listing is provided below for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144431\n",
      "Total Vocab:  46\n",
      "Total Patterns:  144331\n",
      "Seed:\n",
      "\" or?\"'\n",
      "\n",
      "'she boxed the queen's ears--' the rabbit began. alice gave a little\n",
      "scream of laughter. 'oh, \"\n",
      "\n",
      "Generated:\n",
      " downd you tell me,' she said to herself, as she spoke as the mock turtle, and the pueen say on tit off, and the pueen say on lind and looked at the mock turtle, and the pueen say on tit off, and the pueen say on lind and looked at the mock turtle, and the pueen say on tit off, and the pueen say on lind and looked at the mock turtle, and the pueen say on tit off, and the pueen say on lind and looked at the mock turtle, and the pueen say on tit off, and the pueen say on lind and looked at the mock turtle, and the pueen say on tit off, and the pueen say on lind and looked at the mock turtle, and the pueen say on tit off, and the pueen say on lind and looked at the mock turtle, and the pueen say on tit off, and the pueen say on lind and looked at the mock turtle, and the pueen say on tit off, and the pueen say on lind and looked at the mock turtle, and the pueen say on tit off, and the pueen say on lind and looked at the mock turtle, and the pueen say on tit off, and the pueen say on lind\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Load Larger LSTM network and generate text\n",
    "import sys\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"../../datasets/wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "# create mapping of unique chars to integers, and a reverse mapping\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = utils.to_categorical(dataY)\n",
    "\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "# load the network weights\n",
    "filename = \"weights-improvement-50-1.2259-bigger.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "# generate characters\n",
    "print(\"\\nGenerated:\")\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One example of running this text generation script produces the output below. The randomly chosen seed text was:\n",
    "\n",
    "```\n",
    "\" or?\"'\n",
    "\n",
    "'she boxed the queen's ears--' the rabbit began. alice gave a little\n",
    "scream of laughter. 'oh, \"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated text with the seed was:\n",
    "\n",
    "```\n",
    " downd you tell me,' she said to herself, as she spoke as the mock turtle, and the pueen say on tit off, and the pueen say on lind and looked at the mock turtle, and the pueen say on tit off, and the pueen say on lind and looked at the mock turtle, and the pueen say on tit off, and the pueen say on lind and looked at the mock turtle, and the pueen say on tit off, and the pueen say on lind and looked at the mock turtle, and the pueen say on tit off, and the pueen say on lind and looked at the mock turtle, and the pueen say on tit off, and the pueen say on lind and looked at the mock turtle, and the pueen say on tit off, and the pueen say on lind and looked at the mock turtle, and the pueen say on tit off, and the pueen say on lind and looked at the mock turtle, and the pueen say on tit off, and the pueen say on lind and looked at the mock turtle, and the pueen say on tit off, and the pueen say on lind and looked at the mock turtle, and the pueen say on tit off, and the pueen say on lind\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that generally, there are fewer spelling mistakes, and the text looks more realistic but is still quite nonsensical. For example, the same phrases get repeated again and again, like said to herself and little. Quotes are opened but not closed. These are better results but there is still much room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension Ideas to Improve the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a sample of ideas that you may want to investigate to improve the model further:\n",
    "* Predict fewer than 1,000 characters as output for a given seed.\n",
    "* Remove all punctuation from the source text, and therefore from the models' vocabulary.\n",
    "* Try a one-hot encoding for the input sequences.\n",
    "* Train the model on padded sentences rather than random sequences of characters.\n",
    "* Increase the number of training epochs to 100 or many hundreds.\n",
    "* Add dropout to the visible input layer and consider tuning the dropout percentage.\n",
    "* Tune the batch size, try a batch size of 1 as a (very slow) baseline, and larger sizes from there.\n",
    "* Add more memory units to the layers and/or more layers.\n",
    "* Experiment with scale factors (temperature) when interpreting the prediction probabilities.\n",
    "* Change the LSTM layers to be stateful to maintain state across batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, you discovered how to develop an LSTM recurrent neural network for text generation in Python with the Keras deep learning library. After completing this project, you know:\n",
    "\n",
    "* Where to download the ASCII text for classical books for free that you can use for training.\n",
    "* How to train an LSTM network on text sequences and use the trained network to generate new sequences.\n",
    "* How to develop stacked LSTM networks and lift the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
