{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Halt Training at the Right Time with Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major challenge in training neural networks is how long to train them. Too little training will mean that the model will underfit the train and the test sets. Too much training will mean that the model will overfit the training dataset and have poor performance on the test set. A compromise is to train on the training dataset but to stop training at the point when performance on a validation dataset starts to degrade. This simple, effective, and widely used approach to training neural networks is called early stopping. In this tutorial, you will discover that stopping the training of a neural network early before it has overfitted the training dataset can reduce overfitting and improve the generalization of deep neural networks. After reading this tutorial, you will know:\n",
    "\n",
    "* The challenge of training a neural network long enough to learn the mapping, but not so long that it overfits the training data.\n",
    "* Model performance on a holdout validation dataset can be monitored during training, and training stops when generalization error starts to increase.\n",
    "* The use of early stopping requires selecting a performance measure to monitor, a trigger to stop training, and a selection of the model weights to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, discover the problem of training a model for too long and the regularizing effect that halting the training process at the right time can have, as well as tips for using early stopping in your projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem of Training Just Enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training neural networks is challenging. When training a large network, there will be a point when the model stops generalizing and starts learning the statistical noise in the training dataset. This overfitting of the training dataset will result in an increase in generalization error, making the model less useful at making predictions on new data. The challenge is to train the network long enough to learn the mapping from inputs to outputs but not training the model so long that it overfits the training data.\n",
    "\n",
    "One approach to solving this problem is to treat the number of training epochs as a hyperparameter and train the model multiple times with different values, then select the number of epochs that result in the best performance on the train or a holdout test dataset. The downside of this approach is that it requires multiple models to be trained and discarded. This can be computationally inefficient and time-consuming, especially for large models trained on large datasets over days or weeks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Training When Generalization Error Increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative approach is to train the model once for a large number of training epochs. During training, the model is evaluated on a holdout validation dataset after each epoch. If the model's performance on the validation dataset starts to degrade (e.g., loss begins to increase or accuracy begins to decrease), then the training process is stopped.\n",
    "\n",
    "The model when training is stopped is then used and is known to have good generalization performance. This procedure is called early stopping and is perhaps one of the oldest and most widely used forms of neural network regularization.\n",
    "\n",
    "If regularization methods like weight decay that update the loss function to encourage less complex models are considered explicit regularization, then early stopping may be thought of as a type of implicit regularization, much like using a smaller network with less capacity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Stop Training Early"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping requires that you configure your network to be under constrained, meaning that it has more capacity than is required for the problem. When training the network, a larger number of training epochs is used than may normally be required to give the network plenty of opportunity to fit, then begin to overfit the training dataset. There are three elements to using early stopping; they are:\n",
    "\n",
    "* Monitoring model performance.\n",
    "* Trigger to stop training.\n",
    "* The choice of model to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monitoring Performance**\n",
    "\n",
    "The performance of the model must be monitored during training. This requires choosing a dataset used to evaluate the model and a metric used to evaluate the model. It is common to split the training dataset and use a subset, such as 30%, as a validation dataset used to monitor the model's performance during training. This validation set is not used to train the model. It is also common to use the loss on a validation dataset as the metric to monitor, although you may also use prediction error in the case of regression or accuracy in the case of classification.\n",
    "\n",
    "The model's performance is evaluated on the validation set at the end of each epoch, which adds an additional computational cost during training. This can be reduced by evaluating the model less frequently, such as every 2, 5, or 10 training epochs. The loss of the model on the training dataset will also be available as part of the training procedure, and additional metrics may also be calculated and monitored on the training dataset.\n",
    "\n",
    "**Early Stopping Trigger**\n",
    "\n",
    "Once a scheme for evaluating the model is selected, a trigger for stopping the training process must be chosen. The trigger will use a monitored performance metric to decide when to stop training. This is often the performance of the model on the holdout dataset, such as the loss. In the simplest case, training is stopped as soon as the performance on the validation dataset decreases as compared to the performance on the validation dataset at the prior training epoch (e.g., an increase in loss). More elaborate triggers may be required in practice. This is because the training of a neural network is stochastic and can be noisy. Plotted on a graph, the performance of a model on a validation dataset may go up and down many times. This means that the first sign of overfitting may not be a good place to stop training.\n",
    "\n",
    "Some more elaborate triggers may include:\n",
    "* No change in metric over a given number of epochs.\n",
    "* An absolute change in a metric.\n",
    "* A decrease in performance was observed over a given number of epochs.\n",
    "* The average change in metric over a given number of epochs.\n",
    "\n",
    "Some delay or patience in stopping is almost always a good idea.\n",
    "\n",
    "**Model Choice**\n",
    "\n",
    "When training is halted, the model is known to have a slightly worse generalization error than a model at a prior epoch. As such, some consideration may need to be given as to exactly which model is saved. Specifically, the training epoch from which weights in the model are saved to file. This will depend on the trigger chosen to stop the training process. For example, if the trigger is a simple decrease in performance from one epoch to the next, then the weights for the model at the prior epoch will be preferred. If the trigger is required to observe a decrease in performance over a fixed number of epochs, the model will be preferred at the beginning of the trigger period. Perhaps a simple approach is always to save the model weights if the model's performance on a holdout dataset is better than at the previous epoch. That way, you will always have the model with the best performance on the holdout set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips for Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section provides some tips for using early stopping regularization with your neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to Use Early Stopping**\n",
    "\n",
    "Early stopping is so easy to use, e.g., with the simplest trigger, that there is little reason not to use it when training neural networks. The use of early stopping may be a staple of the modern training of deep neural networks.\n",
    "\n",
    "**Plot Learning Curves to Select a Trigger**\n",
    "\n",
    "Before using early stopping, it may be interesting to fit an under-constrained model and monitor its performance on a train and validation dataset. Plotting the model's performance in real-time or at the end of a long run will show how noisy the training process is with your specific model and dataset. This may help in the choice of a trigger for early stopping.\n",
    "\n",
    "**Monitor an Important Metric**\n",
    "\n",
    "Loss is an easy metric to monitor during training and to trigger early stopping. The problem is that loss does not always capture the most important model to you and your project.\n",
    "\n",
    "It may be better to choose a performance metric to monitor that best defines the model's performance in terms of how you intend to use it. This may be the metric that you intend to use to report the performance of the model.\n",
    "\n",
    "**Suggested Training Epochs**\n",
    "\n",
    "A problem with early stopping is that the model does not make use of all available training data. It may be desirable to avoid overfitting and train on all possible data, especially on problems where training data is very limited. A recommended approach would be to treat the number of training epochs as a hyperparameter and to grid search a range of different values, perhaps using k-fold cross-validation. This will allow you to fix the number of training epochs and fit a final model on all available data.\n",
    "\n",
    "Early stopping could be used instead. The early stopping procedure could be repeated a number of times. The epoch number at which training was stopped could be recorded. Then, the average of the epoch number across all repeats of early stopping could be used when fitting a final model on all available training data. This process could be performed using a different training split into train and validation steps each time early stopping runs. An alternative might be to use early stopping with a validation dataset, then update the final model with further training on the held-out validation set.\n",
    "\n",
    "**Early Stopping With Cross-Validation**\n",
    "\n",
    "Early stopping could be used with k-fold cross-validation, although it is not recommended. The k-fold cross-validation procedure is designed to estimate the generalization error by repeatedly refitting and evaluating it on different subsets of a dataset. Early stopping is designed to monitor the generalization error of one model and stop training when generalization error begins to degrade. They are at odds because cross-validation assumes you do not know the generalization error, and early stopping is trying to give you the best model based on knowledge of generalization error.\n",
    "\n",
    "One possible point of confusion is that early stopping is sometimes referred to as cross-validated training. It may be desirable to use cross-validation to estimate the performance of models with different hyperparameter values, such as learning rate or network structure, while also using early stopping. In this case, if you have the resources to evaluate the model's performance repeatedly, then perhaps the number of training epochs may also be treated as a hyperparameter to be optimized instead of using early stopping. Instead of using cross-validation with early stopping, early stopping may be used directly without repeated evaluation when evaluating different hyperparameter values for the model (e.g., different learning rates). Further, research into early stopping that compares triggers may use cross-validation to compare the impact of different triggers.\n",
    "\n",
    "**Overfit Validation**\n",
    "\n",
    "Repeating the early stopping procedure many times may result in the model overfitting the validation dataset. This can happen just as easily as overfitting the training dataset. One approach is only to use early stopping once all other hyperparameters of the model have been chosen. Another strategy may be to use a different split of the training dataset into train and validation sets each time early stopping is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping Case Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
