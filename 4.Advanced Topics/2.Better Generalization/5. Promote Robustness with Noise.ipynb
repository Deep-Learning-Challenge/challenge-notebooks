{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Promote Robustness with Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a neural network with a small dataset can cause the network to memorize all training examples, leading to poor performance on a holdout dataset. Given the patchy or sparse sampling of points in the high-dimensional input space, small datasets may also represent a harder mapping problem for neural networks to learn. One approach to making the input space smoother and easier to learn is to add noise to inputs during training. In this tutorial, you will discover that adding noise to a neural network during training can improve the robustness of the network, resulting in better generalization and faster learning. After reading this tutorial, you will know:\n",
    "\n",
    "* Small datasets can make learning challenging for neural nets, and the examples can be memorized.\n",
    "* Adding noise during training can make the training process more robust and reduce generalization error.\n",
    "* Noise is traditionally added to the inputs but can also be added to weights, gradients, and even activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will discover the brittleness of large network weights and how the addition of statistical noise can provide a regularizing effect, as well as tips to help when adding noise to your neural network models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge of Small Training Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small datasets can introduce problems when training large neural networks. The first problem is that the network may effectively memorize the training dataset. Instead of learning a general mapping from inputs to outputs, the model may learn the specific input examples and their associated outputs. This will result in a model that performs well on the training dataset and poor on new data, such as a holdout dataset. The second problem is that a small dataset provides less opportunity to describe the structure of the input space and its relationship to the output. More training data provides a richer description of the problem from which the model may learn. Fewer data points mean that rather than a smooth input space, the points may represent a jarring and disjointed structure that may result in a difficult, if not unlearnable, the mapping function. It is not always possible to acquire more data. Further, getting a hold of more data may not address these problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Random Noise During Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One approach to improving generalization error and improving the mapping problem's structure is to add random noise.\n",
    "\n",
    "At first, this sounds like a recipe for making learning more challenging. It is a counter-intuitive suggestion to improving performance because one would expect noise to degrade the model's performance during training.\n",
    "\n",
    "The addition of noise during the training of a neural network model has a regularization effect and, in turn, improves the robustness of the model. It has been shown to have a similar impact on the loss function as the addition of a penalty term, as in the case of weight regularization\n",
    "methods.\n",
    "\n",
    "Each time a training sample is exposed to the model, random noise is added to the input variables making them different every time it is exposed. In this way, adding noise to input samples is a simple form of data augmentation. In effect, adding noise expands the size of the training dataset.\n",
    "\n",
    "Adding noise means that the network cannot memorize training samples because they are changing all of the time, resulting in smaller network weights and a more robust network with lower generalization error. The noise means that it is as though new samples are being drawn from the domain in the vicinity of known samples, smoothing the structure of the input space. This smoothing may mean that the mapping function is easier for the network to learn, resulting in better and faster learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How and Where to Add noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common type of noise used during training is the addition of Gaussian noise to input variables. Gaussian noise, or white noise, has a mean of zero and a standard deviation of one and can be generated as needed using a pseudorandom number generator. The addition of Gaussian noise to the inputs to a neural network was traditionally referred to as jitter or random jitter after using the term in signal processing to refer to the uncorrelated random noise in electrical circuits. The amount of noise added (e.g., the spread or standard deviation) is a configurable hyperparameter. Too little noise has no effect, whereas too much noise makes the mapping function challenging to learn.\n",
    "\n",
    "The standard deviation of the random noise controls the amount of spread and can be adjusted based on the scale of each input variable. It can be easier to configure if the scale of the input variables has first been normalized. Noise is only added during training. No noise is added during the evaluation of the model or when the model is used to make predictions on new data. The addition of noise is also an important part of automatic feature learning, such as in autoencoders, so-called denoising autoencoders that explicitly require models to learn robust features in the presence of noise added to inputs.\n",
    "\n",
    "Although additional noise to the inputs is the most common and widely studied approach, random noise can be added to other network parts during training. Some examples include:\n",
    "\n",
    "* Add noise to activations, i.e., the outputs of each layer.\n",
    "* Add noise to weights, i.e., an alternative to the inputs.\n",
    "* Add noise to the gradients, i.e., the direction to update weights.\n",
    "* Add noise to the outputs, i.e., the labels or target variables.\n",
    "\n",
    "The addition of noise to the layer activations allows noise to be used at any point in the network. This can be beneficial for very deep networks. Noise can be added to the layer outputs themselves, but this is more likely achieved via a noisy activation function. The addition of noise to weights allows the approach to be used throughout the network in a consistent way instead of adding noise to inputs and layer activations. This is particularly useful in recurrent neural networks.\n",
    "\n",
    "The addition of noise to gradients focuses more on improving the robustness of the optimization process itself rather than the structure of the input domain. The noise can start high at the beginning of training and decrease over time, much like a decaying learning rate.\n",
    "This approach has proven to be an effective method for very deep networks and various network types.\n",
    "\n",
    "Adding noise to the activations, weights, or gradients provides a more generic approach to adding noise that is invariant to the input variables provided to the model. If the problem domain is believed or expected to have mislabeled examples, then adding noise to the class label can improve the model's robustness to this type of error. Although, it can be easy to derail the learning process. Adding noise to a continuous target variable in the case of regression or time series forecasting is much like the addition of noise to the input variables and\n",
    "maybe a better use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips for Adding Noise During Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section provides some tips for adding noise during training with your neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem Types for Adding Noise**\n",
    "\n",
    "Noise can be added to training regardless of the type of problem that is being addressed. It is appropriate to try adding noise to both classification and regression type problems. The type of noise can be specialized to the types of data used as input to the model, for example, two-dimensional noise in images and signal noise in audio data.\n",
    "\n",
    "**Add Noise to Different Network Types**\n",
    "\n",
    "Adding noise during training is a generic method that can be used regardless of the neural network used. It was a method used primarily with Multilayer Perceptrons given their prior dominance, but it can be used with Convolutional and Recurrent Neural Networks.\n",
    "\n",
    "**Rescale Data First**\n",
    "\n",
    "It is important that the addition of noise has a consistent effect on the model. This requires that the input data is rescaled so that all variables have the same scale so that when noise is added to the inputs with a fixed variance, it has the same effect. It also applies to adding noise to weights and gradients as they are affected by the scale of the inputs. This can be achieved via standardization or normalization of input variables. If random noise is added after data scaling, the variables may need to be rescaled, perhaps per minibatch.\n",
    "\n",
    "**Test the Amount of Noise**\n",
    "You cannot know how much noise will benefit your specific model on your training dataset. Experiment with different amounts, and even different types of noise, to discover what works best. Be systematic and use controlled experiments, perhaps on smaller datasets across a range of values.\n",
    "\n",
    "**Noisy Training Only**\n",
    "\n",
    "Noise is only added during the training of your model. Be sure that any source of noise is not added during the evaluation of your model or when your model is used to make predictions on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Regularization Case Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
