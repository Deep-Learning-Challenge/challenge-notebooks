{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyclic Learning Rate and Snapshot Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model ensembles can achieve lower generalization errors than single models but are challenging to develop with deep learning neural networks, given the computational cost of training every model. An alternative is to train multiple model snapshots during a single training run and combine their predictions to make an ensemble prediction. A limitation of this approach is that the saved models will be similar, resulting in similar predictions and predictions errors and not offering many benefits from combining their predictions.\n",
    "\n",
    "Effective ensembles require a diverse set of skillful ensemble members that have different distributions of prediction errors. One approach to promoting a diversity of models saved during a single training run is to use an aggressive learning rate schedule that forces large changes in the model weights and, in turn, the nature of the model saved at each snapshot. In this tutorial, you will discover how to develop snapshot ensembles of models saved using an aggressive learning rate schedule over a single training run. After completing this tutorial, you will know:\n",
    "\n",
    "* Snapshot ensembles combine the predictions from multiple models saved during a single training run.\n",
    "* Diversity in model snapshots can be achieved by aggressively cycling the learning rate used during a single training run.\n",
    "* How to save model snapshots during a single run and load snapshot models to make ensemble predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snapshot Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem with ensemble learning with deep learning methods is the large computational cost of training multiple models. This is because of the use of very deep models and large datasets, which can result in model training times extending to days, weeks, or even months.\n",
    "\n",
    "One approach to ensemble learning for deep learning neural networks is to collect multiple models from a single training run. This addresses the computational cost of training multiple deep learning models as models can be selected and saved during training, then used to make an ensemble prediction. A key benefit of ensemble learning is improved performance compared to the predictions from single models. This can be achieved by selecting members with good skills but in different ways, providing a diverse set of predictions to be combined. A limitation of collecting multiple models during a single training run is that the models may be good but too similar.\n",
    "\n",
    "This can be addressed by changing the learning algorithm for the deep neural network to explore different network weights during a single training run that will result in models with differing performances. One way that this can be achieved is by aggressively changing the learning rate used during training. An approach to systematically and aggressively changing the learning rate during training to result in very different network weights is referred to as Stochastic Gradient Descent with Warm Restarts or SGDR for short, described by Ilya Loshchilov and Frank Hutter in their 2017 paper *SGDR: Stochastic Gradient Descent with Warm Restarts*.\n",
    "\n",
    "Their approach involves systematically changing the learning rate over training epochs, called cosine annealing. This approach requires the specification of two hyperparameters: the initial learning rate and the total number of training epochs. The cosine annealing method has the effect of starting with a large learning rate that is relatively rapidly decreased to a minimum value before being dramatically increased again. The model weights are subjected to the dramatic changes during training, having the effect of using good weights as the starting point for the subsequent learning rate cycle but allowing the learning algorithm to converge to a different solution.\n",
    "\n",
    "The resetting of the learning rate acts as a simulated restart of the learning process, and the re-use of good weights as the starting point of the restart is referred to as a warm restart, in contrast to a cold restart where a new set of small random numbers may be used as a starting point. The good weights at the bottom of each cycle can be saved to file, providing a snapshot of the model. These snapshots can be collected at the end of the run and used in a model averaging ensemble. The saving and use of these models during an aggressive learning rate schedule is referred to as a Snapshot Ensemble and was described by Gao Huang et al. in their 2017 paper titled *Snapshot Ensembles: Train 1, get M for free* and subsequently also used in an updated version of the Loshchilov and Hutter paper.\n",
    "\n",
    "The ensemble of models is created during training a single model, therefore, the authors claim that the ensemble forecast is provided at no additional cost.\n",
    "\n",
    "Although a cosine annealing schedule is used for the learning rate, other aggressive learning rate schedules could be used, such as the simpler cyclical learning rate schedule described by Leslie Smith in the 2017 paper titled Cyclical Learning Rates for Training Neural Networks. Now that we are familiar with the snapshot ensemble technique, we can look at how to implement it in Python with Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snapshot Ensembles Case Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
