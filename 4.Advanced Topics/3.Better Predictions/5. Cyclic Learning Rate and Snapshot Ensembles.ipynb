{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Deep-Learning-Challenge/challenge-notebooks/blob/master/4.Advanced%20Topics/3.Better%20Predictions/5.%20Cyclic%20Learning%20Rate%20and%20Snapshot%20Ensembles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyclic Learning Rate and Snapshot Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model ensembles can achieve lower generalization errors than single models but are challenging to develop with deep learning neural networks, given the computational cost of training every model. An alternative is to train multiple model snapshots during a single training run and combine their predictions to make an ensemble prediction. A limitation of this approach is that the saved models will be similar, resulting in similar predictions and predictions errors and not offering many benefits from combining their predictions.\n",
    "\n",
    "Effective ensembles require a diverse set of skillful ensemble members that have different distributions of prediction errors. One approach to promoting a diversity of models saved during a single training run is to use an aggressive learning rate schedule that forces large changes in the model weights and, in turn, the nature of the model saved at each snapshot. In this tutorial, you will discover how to develop snapshot ensembles of models saved using an aggressive learning rate schedule over a single training run. After completing this tutorial, you will know:\n",
    "\n",
    "* Snapshot ensembles combine the predictions from multiple models saved during a single training run.\n",
    "* Diversity in model snapshots can be achieved by aggressively cycling the learning rate used during a single training run.\n",
    "* How to save model snapshots during a single run and load snapshot models to make ensemble predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snapshot Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem with ensemble learning with deep learning methods is the large computational cost of training multiple models. This is because of the use of very deep models and large datasets, which can result in model training times extending to days, weeks, or even months.\n",
    "\n",
    "One approach to ensemble learning for deep learning neural networks is to collect multiple models from a single training run. This addresses the computational cost of training multiple deep learning models as models can be selected and saved during training, then used to make an ensemble prediction. A key benefit of ensemble learning is improved performance compared to the predictions from single models. This can be achieved by selecting members with good skills but in different ways, providing a diverse set of predictions to be combined. A limitation of collecting multiple models during a single training run is that the models may be good but too similar.\n",
    "\n",
    "This can be addressed by changing the learning algorithm for the deep neural network to explore different network weights during a single training run that will result in models with differing performances. One way that this can be achieved is by aggressively changing the learning rate used during training. An approach to systematically and aggressively changing the learning rate during training to result in very different network weights is referred to as Stochastic Gradient Descent with Warm Restarts or SGDR for short, described by Ilya Loshchilov and Frank Hutter in their 2017 paper *SGDR: Stochastic Gradient Descent with Warm Restarts*.\n",
    "\n",
    "Their approach involves systematically changing the learning rate over training epochs, called cosine annealing. This approach requires the specification of two hyperparameters: the initial learning rate and the total number of training epochs. The cosine annealing method has the effect of starting with a large learning rate that is relatively rapidly decreased to a minimum value before being dramatically increased again. The model weights are subjected to the dramatic changes during training, having the effect of using good weights as the starting point for the subsequent learning rate cycle but allowing the learning algorithm to converge to a different solution.\n",
    "\n",
    "The resetting of the learning rate acts as a simulated restart of the learning process, and the re-use of good weights as the starting point of the restart is referred to as a warm restart, in contrast to a cold restart where a new set of small random numbers may be used as a starting point. The good weights at the bottom of each cycle can be saved to file, providing a snapshot of the model. These snapshots can be collected at the end of the run and used in a model averaging ensemble. The saving and use of these models during an aggressive learning rate schedule is referred to as a Snapshot Ensemble and was described by Gao Huang et al. in their 2017 paper titled *Snapshot Ensembles: Train 1, get M for free* and subsequently also used in an updated version of the Loshchilov and Hutter paper.\n",
    "\n",
    "The ensemble of models is created during training a single model, therefore, the authors claim that the ensemble forecast is provided at no additional cost.\n",
    "\n",
    "Although a cosine annealing schedule is used for the learning rate, other aggressive learning rate schedules could be used, such as the simpler cyclical learning rate schedule described by Leslie Smith in the 2017 paper titled Cyclical Learning Rates for Training Neural Networks. Now that we are familiar with the snapshot ensemble technique, we can look at how to implement it in Python with Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snapshot Ensembles Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will demonstrate how to use the snapshot ensemble to reduce the variance of an MLP on a simple multiclass classification problem. This example provides a template for applying the snapshot ensemble to your neural network for classification and regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a small multiclass classification problem as the basis to demonstrate the snapshot ensemble. The scikit-learn class provides the `make_blobs()` function that can be used to create a multiclass classification problem with the prescribed number of samples, input variables, classes, and variance of samples within a class. We use this problem with 1,000 examples, with input variables (to represent the x and y coordinates of the points) and a standard deviation of 2.0 for points within each group. We will use the same random state (seed for the pseudorandom number generator) to ensure that we always get the same data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are the input and output elements of a dataset that we can model. In order to get a feeling for the complexity of the problem, we can graph each point on a two-dimensional scatter plot and color each point by class value. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib import pyplot\n",
    "from numpy import where\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "\n",
    "# scatter plot for each class value\n",
    "for class_value in range(3):\n",
    "    # select indices of points with the class label\n",
    "    row_ix = where(y == class_value)\n",
    "\n",
    "    # scatter plot for points with a different color\n",
    "    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n",
    "\n",
    "# show plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example creates a scatter plot of the entire dataset. We can see that the standard deviation of 2.0 means that the classes are not linearly separable (separable by a line), causing many ambiguous points. This is desirable because the problem is non-trivial and will allow a neural network model to find many different *good enough* candidate solutions resulting in a high variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we define a model, we need to contrive an appropriate problem for a horizontal voting ensemble. In our problem, the training dataset is relatively small. Specifically, there is a 10:1 ratio of examples in the training dataset to the holdout dataset. This mimics a situation where we may have a vast number of unlabeled examples and a small number of labeled examples with which to train a model. We will create 1,100 data points from the blobs problem. The model will be trained on the first 100 points, and the remaining 1,000 will be held back in a test dataset, unavailable to the model.\n",
    "\n",
    "The problem is a multiclass classification problem, and we will model it using a softmax activation function on the output layer. This means that the model will predict a vector with three elements with the probability that the sample belongs to each of the three classes. Therefore, the first step is to one-hot encode the class values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "\n",
    "# one hot encode output variable\n",
    "y = to_categorical(y)\n",
    "\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "print(trainX.shape, testX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can define and compile the model. The model will expect samples with two input variables. The model then has a single hidden layer with 25 nodes and a rectified linear activation function, an output layer with three nodes to predict the probability of each of the three classes, and a softmax activation function. Because the problem is multiclass, we will use the categorical cross-entropy loss function to optimize the model and stochastic gradient descent with a small learning rate and momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=2, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is fit for 200 training epochs, and we will evaluate each epoch on the test set, using the test set as a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=200, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the run, we will evaluate the model's performance on both the train and the test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then finally, we will plot model loss and accuracy learning curves over each training epoch on both the training and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss learning curves\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Cross-Entropy Loss', pad=-40)\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "\n",
    "# plot accuracy learning curves\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy', pad=-40)\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# develop an mlp for blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "\n",
    "# one hot encode output variable\n",
    "y = to_categorical(y)\n",
    "\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=2, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "opt = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=200, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example, it first prints the performance of the final model on the train and test datasets.\n",
    "\n",
    "**Note:** Your specific results may vary, given the stochastic nature of the learning algorithm. Consider running the example a few times and compare the average performance.\n",
    "\n",
    "In this case, we can see that the model achieved about 83% accuracy on the training dataset, which we know is optimistic, and about 79% on the test dataset, which we would expect to be more realistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A line plot also shows the learning curves for the model accuracy on the train and test sets over each training epoch. We can see that training accuracy is more optimistic over the whole run, as we noted with the final scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss learning curves\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Cross-Entropy Loss', pad=-40)\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "\n",
    "# plot accuracy learning curves\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy', pad=-40)\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can look at how to implement an aggressive learning rate schedule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Annealing Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An effective snapshot ensemble requires training a neural network with an aggressive learning rate schedule. The cosine annealing schedule is an example of an aggressive learning rate schedule where the learning rate starts high and is dropped relatively rapidly to a minimum value near zero before being increased again to the maximum. We can implement the schedule described in the 2017 paper \"Snapshot Ensembles: Train 1, get M for free\". The equation requires the total training epochs, maximum learning rate, number of cycles as arguments, and the current epoch number. The function then returns the learning rate for the given epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>$ \\alpha(t) = \\frac{\\alpha_0}{2}(cos(\\frac{\\pi mod(t-1, \\lfloor{T/M}\\rfloor)}{\\lfloor{T/M}\\rfloor})+1) $</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where `a(t)` is the learning rate at epoch `t,` `a0` is the maximum learning rate, `T` is the total epochs, M is the number of cycles, `mod` is the modulo operation, and square brackets indicate a floor operation. The function `cosine_annealing()` below implements the equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine annealing learning rate schedule\n",
    "def cosine_annealing(epoch, n_epochs, n_cycles, lrate_max):\n",
    "    epochs_per_cycle = floor(n_epochs/n_cycles)\n",
    "    cos_inner = (pi * (epoch % epochs_per_cycle)) / (epochs_per_cycle)\n",
    "    \n",
    "    return lrate_max/2 * (cos(cos_inner) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test this implementation by plotting the learning rate over 100 epochs with five cycles (e.g., 20 epochs long) and a maximum learning rate of 0.01. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a cosine annealing learning rate schedule\n",
    "from matplotlib import pyplot\n",
    "from math import pi\n",
    "from math import cos\n",
    "from math import floor\n",
    "\n",
    "# cosine annealing learning rate schedule\n",
    "def cosine_annealing(epoch, n_epochs, n_cycles, lrate_max):\n",
    "    epochs_per_cycle = floor(n_epochs/n_cycles)\n",
    "    cos_inner = (pi * (epoch % epochs_per_cycle)) / (epochs_per_cycle)\n",
    "    \n",
    "    return lrate_max/2 * (cos(cos_inner) + 1)\n",
    "\n",
    "# create learning rate series\n",
    "n_epochs = 100\n",
    "n_cycles = 5\n",
    "lrate_max = 0.01\n",
    "series = [cosine_annealing(i, n_epochs, n_cycles, lrate_max) for i in range(n_epochs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example creates a line plot of the learning rate schedule over 100 epochs. We can see that the learning rate starts at the maximum value at epoch 0 and decreases rapidly to epoch 19 before being reset at epoch 20, the start of the next cycle. The cycle is repeated five times as specified in the argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot series\n",
    "pyplot.plot(series)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement this schedule as a custom callback in Keras. This allows the schedule parameters to be specified and for the learning rate to be logged so we can ensure it had the desired effect. A custom callback can be defined as a Python class that extends the Keras Callback class. In the class constructor, we can take the required configuration as arguments and save them for use, specifically the total number of training epochs, the number of cycles for the learning rate schedule, and the maximum learning rate. We can use our `cosine_annealing()` defined above to calculate the learning rate for a given training epoch. The Callback class allows an `on_epoch_begin()` function to be overridden that will be called prior to each training epoch. We can override this function to calculate the learning rate for the current epoch and set it in the optimizer. We can also keep track of the learning rate in an internal list. The complete custom callback is defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "# define custom learning rate schedule\n",
    "class CosineAnnealingLearningRateSchedule(Callback):\n",
    "    # constructor\n",
    "    def __init__(self, n_epochs, n_cycles, lrate_max, verbose=0):\n",
    "        self.epochs = n_epochs\n",
    "        self.cycles = n_cycles\n",
    "        self.lr_max = lrate_max\n",
    "        self.lrates = list()\n",
    "\n",
    "    # calculate learning rate for an epoch\n",
    "    def cosine_annealing(self, epoch, n_epochs, n_cycles, lrate_max):\n",
    "        epochs_per_cycle = floor(n_epochs/n_cycles)\n",
    "        cos_inner = (pi * (epoch % epochs_per_cycle)) / (epochs_per_cycle)\n",
    "        \n",
    "        return lrate_max/2 * (cos(cos_inner) + 1)\n",
    "\n",
    "    # calculate and set learning rate at the start of the epoch\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # calculate learning rate\n",
    "        lr = self.cosine_annealing(epoch, self.epochs, self.cycles, self.lr_max)\n",
    "\n",
    "        # set learning rate\n",
    "        backend.set_value(self.model.optimizer.lr, lr)\n",
    "        \n",
    "        # log value\n",
    "        self.lrates.append(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create an instance of the callback and set the arguments. We will train the model for 400 epochs and set the number of cycles to be 50 epochs long, or 400/50 cycles, a suggestion made and configuration used throughout the snapshot ensembles paper.\n",
    "\n",
    "The paper also suggests that the learning rate can be set for each sample or minibatch instead of each epoch to give more nuance to the updates, but we will leave this as a future exercise.\n",
    "\n",
    "Once the callback is instantiated and configured, we can specify it as part of the list of callbacks to the call to the `fit()` function to train the model.\n",
    "\n",
    "```\n",
    "# define learning rate callback\n",
    "n_epochs = 400\n",
    "n_cycles = n_epochs / 50\n",
    "ca = CosineAnnealingLearningRateSchedule(n_epochs, n_cycles, 0.01)\n",
    "\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=n_epochs, verbose=0, callbacks=[ca])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the run, we can confirm that the learning rate schedule was performed by plotting the contents of the `lrates` list.\n",
    "\n",
    "```\n",
    "# plot learning rate\n",
    "pyplot.plot(ca.lrates)\n",
    "pyplot.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tying these elements together, the complete example of training an MLP on the blobs problem with a cosine annealing learning rate schedule is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp with cosine annealing learning rate schedule on blobs problem\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend\n",
    "\n",
    "from math import pi, cos, floor\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "\n",
    "# one hot encode output variable\n",
    "y = to_categorical(y)\n",
    "\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=2, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "opt = SGD(momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# define learning rate callback\n",
    "n_epochs = 400\n",
    "n_cycles = n_epochs / 50\n",
    "ca = CosineAnnealingLearningRateSchedule(n_epochs, n_cycles, 0.01)\n",
    "\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=n_epochs, verbose=0, callbacks=[ca])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first reports the accuracy of the model on the training and test sets.\n",
    "\n",
    "**Note:** Your specific results may vary, given the stochastic nature of the learning algorithm. Consider running the example a few times and compare the average performance.\n",
    "\n",
    "In this case, we do not see much difference in the final model's performance compared to the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A line plot of the learning rate schedule is created, showing eight cycles of 50 epochs each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning rate\n",
    "pyplot.plot(ca.lrates)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a line plot of model loss and accuracy on the train and test sets is created over each training epoch. We can see that although the learning rate was changed dramatically, there was not a dramatic effect on model performance, likely because the chosen classification problem is not very difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss learning curves\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Cross-Entropy Loss', pad=-40)\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "\n",
    "# plot accuracy learning curves\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy', pad=-40)\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to implement the cosine annealing learning schedule, we can use it to prepare a snapshot ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Snapshot Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can develop a snapshot ensemble in two parts. The first part involves creating a custom callback to save the model at the bottom of each learning rate schedule. The second part involves loading the saved models and using them to make an ensemble prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save Snapshot Models During Training**\n",
    "\n",
    "The `CosineAnnealingLearningRateSchedule` can be updated to override the on epoch end() function called at the end of each training epoch. In this function, we can check if the current epoch that ended was the end of a cycle. If so, we can save the model to a file. Below is the updated callback, named the `SnapshotEnsemble` class. A debug message is printed each time a model is saved as confirmation that models are being saved at the right time. For example, with 50-epoch long cycles, we would expect a model to be saved on epoch 49, 99, etc., and the learning rate reset at epoch 50, 100, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snapshot ensemble with custom learning rate schedule\n",
    "class SnapshotEnsemble(Callback):\n",
    "    # constructor\n",
    "    def __init__(self, n_epochs, n_cycles, lrate_max, verbose=0):\n",
    "        self.epochs = n_epochs\n",
    "        self.cycles = n_cycles\n",
    "        self.lr_max = lrate_max\n",
    "        self.lrates = list()\n",
    "\n",
    "    # calculate learning rate for epoch\n",
    "    def cosine_annealing(self, epoch, n_epochs, n_cycles, lrate_max):\n",
    "        epochs_per_cycle = floor(n_epochs/n_cycles)\n",
    "        cos_inner = (pi * (epoch % epochs_per_cycle)) / (epochs_per_cycle)\n",
    "        \n",
    "        return lrate_max/2 * (cos(cos_inner) + 1)\n",
    "\n",
    "    # calculate and set learning rate at the start of the epoch\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        # calculate learning rate\n",
    "        lr = self.cosine_annealing(epoch, self.epochs, self.cycles, self.lr_max)\n",
    "\n",
    "        # set learning rate\n",
    "        backend.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "        # log value\n",
    "        self.lrates.append(lr)\n",
    "\n",
    "    # save models at the end of each cycle\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # check if we can save model\n",
    "        epochs_per_cycle = floor(self.epochs / self.cycles)\n",
    "        \n",
    "        if epoch != 0 and (epoch + 1) % epochs_per_cycle == 0:\n",
    "            # save model to file\n",
    "            filename = \"snapshot_model_%d.h5\" % int((epoch + 1) / epochs_per_cycle)\n",
    "            self.model.save(filename)\n",
    "            print('>saved snapshot %s, epoch %d' % (filename, epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the model for 500 epochs to give ten models to choose from later in an ensemble prediction. The complete example of using this new snapshot ensemble to save models to a file is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of saving models for a snapshot ensemble\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend\n",
    "\n",
    "from math import pi, cos, floor\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "\n",
    "# one hot encode output variable\n",
    "y = to_categorical(y)\n",
    "\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_dim=2, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "opt = SGD(momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# create snapshot ensemble callback\n",
    "n_epochs = 500\n",
    "n_cycles = n_epochs / 50\n",
    "ca = SnapshotEnsemble(n_epochs, n_cycles, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example reports that ten models were saved for the 10-ends of the cosine annealing learning rate schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "model.fit(trainX, trainy, validation_data=(testX, testy), epochs=n_epochs, verbose=0, callbacks=[ca])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Models and Make Ensemble Prediction**\n",
    "\n",
    "Once the snapshot models have been saved to file, they can be loaded and used to make an ensemble prediction. The first step is to load the models into memory. For large models, this could be done one model at a time, make a prediction, and move on to the next model before combining predictions. In this case, the models are relatively small, and we can load all ten from le as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models from file\n",
    "def load_all_models(n_models):\n",
    "    all_models = list()\n",
    "    for i in range(n_models):\n",
    "        # define filename for this ensemble\n",
    "        filename = 'snapshot_model_' + str(i + 1) + '.h5'\n",
    "        \n",
    "        # load model from file\n",
    "        model = load_model(filename)\n",
    "\n",
    "        # add to list of members\n",
    "        all_models.append(model)\n",
    "        print('>loaded %s' % filename)\n",
    "    \n",
    "    return all_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would expect that models saved towards the end of the run may have better performance than models saved earlier in the run. As such, we can reverse the list of loaded models so that the older models are first.\n",
    "\n",
    "```\n",
    "# reverse loaded models so we build the ensemble with the last models first\n",
    "members = list(reversed(members))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't know how many snapshots are required to make a good prediction for this problem. We can explore the effect of the number of ensemble members on test set accuracy by creating ensembles of increasing size starting with the final model at epoch 499, then adding the model saved at epoch 449, and so on until all ten models are included. First, we require a function to make a prediction given a list of models. Given that each model predicts the probabilities of each of the output classes, we can sum the predicted probabilities across the models and select the class with the most support via the `argmax()` function. The `ensemble_predictions()` function below implements this functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an ensemble prediction for multiclass classification\n",
    "def ensemble_predictions(members, testX):\n",
    "    # make predictions\n",
    "    yhats = [model.predict(testX) for model in members]\n",
    "    yhats = array(yhats)\n",
    "\n",
    "    # sum across ensemble members\n",
    "    summed = numpy.sum(yhats, axis=0)\n",
    "\n",
    "    # argmax across classes\n",
    "    result = argmax(summed, axis=1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then evaluate an ensemble of a given size by selecting the first `n` members from the list of models, making a prediction by calling the `ensemble_predictions()` function, and then calculating and returning the accuracy of the prediction. The `evaluate_n_members()` function below implements this behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a specific number of members in an ensemble\n",
    "def evaluate_n_members(members, n_members, testX, testy):\n",
    "    # select a subset of members\n",
    "    subset = members[:n_members]\n",
    "\n",
    "    # make prediction\n",
    "    yhat = ensemble_predictions(subset, testX)\n",
    "\n",
    "    # calculate accuracy\n",
    "    return accuracy_score(testy, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each ensemble's performance can also be contrasted with the performance of each standalone model and the average performance of all standalone models.\n",
    "\n",
    "```\n",
    "# evaluate different numbers of ensembles on hold out set\n",
    "single_scores, ensemble_scores = list(), list()\n",
    "for i in range(1, len(members)+1):\n",
    "    # evaluate model with i members\n",
    "    ensemble_score = evaluate_n_members(members, i, testX, testy)\n",
    "    \n",
    "    # evaluate the i'th model standalone\n",
    "    testy_enc = to_categorical(testy)\n",
    "    _, single_score = members[i-1].evaluate(testX, testy_enc, verbose=0)\n",
    "\n",
    "    # summarize this step\n",
    "    print('> %d: single=%.3f, ensemble=%.3f' % (i, single_score, ensemble_score))\n",
    "    ensemble_scores.append(ensemble_score)\n",
    "    single_scores.append(single_score)\n",
    "\n",
    "# summarize average accuracy of a single final model\n",
    "print('Accuracy %.3f (%.3f)' % (mean(single_scores), std(single_scores)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot each snapshot model (blue dots) compared to the performance of an ensemble that includes all models up to and including each model (orange line)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# plot score vs number of ensemble members\n",
    "x_axis = [i for i in range(1, len(members)+1)]\n",
    "pyplot.plot(x_axis, single_scores, marker='o', linestyle='None')\n",
    "pyplot.plot(x_axis, ensemble_scores, marker='o')\n",
    "pyplot.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete example of making snapshot ensemble predictions with different sized ensembles is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models and make a snapshot ensemble prediction\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from numpy import mean, std, array, argmax\n",
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first loads all ten models into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models in order\n",
    "members = load_all_models(10)\n",
    "print('Loaded %d models' % len(members))\n",
    "\n",
    "# reverse loaded models so we build the ensemble with the last models first\n",
    "members = list(reversed(members))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, each snapshot model is evaluated on the test dataset, and the accuracy is reported. This is contrasted with the accuracy of a snapshot ensemble that includes all snapshot models working backward from the end of the run, including the single model. The results show that as we work backward from the end of the run, the performance of the snapshot models gets worse, as we might expect.\n",
    "\n",
    "Combining snapshot models into an ensemble shows that performance increases up to and including the last 3-to-5 models, reaching about 82%. This can be compared to the average performance of a snapshot model of about 80% test set accuracy.\n",
    "\n",
    "**Note:** Your specific results may vary, given the stochastic nature of the learning algorithm. Consider running the example a few times and compare the average performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate different numbers of ensembles on hold out set\n",
    "single_scores, ensemble_scores = list(), list()\n",
    "for i in range(1, len(members)+1):\n",
    "    # evaluate model with i members\n",
    "    ensemble_score = evaluate_n_members(members, i, testX, testy)\n",
    "    \n",
    "    # evaluate the i'th model standalone\n",
    "    testy_enc = to_categorical(testy)\n",
    "    _, single_score = members[i-1].evaluate(testX, testy_enc, verbose=0)\n",
    "    \n",
    "    # summarize this step\n",
    "    print('> %d: single=%.3f, ensemble=%.3f' % (i, single_score, ensemble_score))\n",
    "    ensemble_scores.append(ensemble_score)\n",
    "    single_scores.append(single_score)\n",
    "\n",
    "# summarize average accuracy of a single final model\n",
    "print('Accuracy %.3f (%.3f)' % (mean(single_scores), std(single_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a line plot is created, plotting the same test set accuracy scores. We can set the performance of each snapshot model as a blue dot and the snapshot ensemble of increasing size (number of members) from 1 to 10 members as an orange line. At least on this run, we can see that the snapshot ensemble quickly out-performs the final model at 82.2% with three members and all other saved models before performance degrades back down to about the same as the final model at 81.3%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot score vs number of ensemble members\n",
    "x_axis = [i for i in range(1, len(members)+1)]\n",
    "pyplot.plot(x_axis, single_scores, marker='o', linestyle='None')\n",
    "pyplot.plot(x_axis, ensemble_scores, marker='o')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extensions This section lists some ideas for extending the tutorial that you may wish to explore.\n",
    "\n",
    "* **Vary Cycle Length**. Update the example to use a shorter or longer cycle length and compare results.\n",
    "* **Vary Maximum Learning Rate**. Update the example to use a larger or smaller maximum learning rate and compare results.\n",
    "* **Update Learning Rate Per Batch**. Update the example to calculate the learning rate per batch instead of per-epoch.\n",
    "* **Repeated Evaluation**. Update the example to repeat the evaluation of the model to confirm that the approach indeed leads to improved performance over the final model on the blobs problem.\n",
    "* **Cyclic Learning Rate**. Update the example to use a cyclic learning rate schedule and compare results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial taught you to develop snapshot ensembles of models saved using an aggressive learning rate schedule over a single training run. Specifically, you learned:\n",
    "\n",
    "* Snapshot ensembles combine the predictions from multiple models saved during a single training run.\n",
    "* Diversity in model snapshots can be achieved by aggressively cycling the learning rate used during a single training run.\n",
    "* How to save model snapshots during a single run and load snapshot models to make ensemble predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
