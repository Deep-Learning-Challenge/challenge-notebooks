{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Model Parameters With Average Model Weights Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process of neural networks is a challenging optimization process that can often fail to converge. This can mean that the model at the end of training may not be a stable or best-performing set of weights to use as a final model. One approach to address this problem is to use an average of the weights from multiple models seen toward the end of the training run. This is called Polyak-Ruppert averaging and can be further improved by using a linearly or exponentially decreasing weighted average of the model weights. In addition to resulting in a more stable model, the performance of the averaged model weights can also result in better performance. In this tutorial, you will discover how to combine the weights from multiple different models into a single model for making predictions. After completing this tutorial, you will know:\n",
    "\n",
    "* The stochastic and challenging nature of training neural networks can mean that the optimization process does not converge.\n",
    "* Creating a model with the average weights from models observed towards the end of a training run can result in a more stable and sometimes better-performing solution.\n",
    "* How to develop final models created with the equal, linearly, and exponentially weighted average of model parameters from multiple saved models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Model Weight Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning the weights for a deep neural network model requires solving a high-dimensional non-convex optimization problem. In stochastic optimization, this is referred to as problems with the convergence of the optimization algorithm on a solution, where a set of specific weight values defines a solution. A challenge with solving this optimization problem is that there are many good solutions, and the learning algorithm can bounce around and fail to settle in on one.\n",
    "\n",
    "A symptom you may see if you have a problem with the convergence of your model is train and/or test loss value that shows higher than expected variance, e.g. it thrashes or bounces up and down over training epochs. One approach to address this problem is to combine the weights collected towards the end of the training process. Generally, this might be referred to as temporal averaging and is known as Polyak Averaging or Polyak-Ruppert averaging, named for the original developers of the method.\n",
    "\n",
    "Averaging multiple noisy sets of weights during the learning process may paradoxically sound less desirable than tuning the optimization process itself. However, it may prove a desirable solution, especially for very large neural networks that may take days, weeks, or even months to train.\n",
    "\n",
    "Averaging the weights of multiple models from a single training run can calm down the noisy optimization process that may be noisy because of the choice of learning hyperparameters (e.g., learning rate) or the shape of the mapping function that is being learned. The result is a final model or set of weights that may offer a more stable and perhaps more accurate result.\n",
    "\n",
    "The simplest implementation of Polyak-Ruppert averaging involves calculating the average of the models' weights over the last few training epochs.\n",
    "\n",
    "This can be improved by calculating a weighted average, where more weight is applied to more recent models, linearly decreased through prior epochs. An alternative and more widely used approach is to use an exponential decay in the weighted average.\n",
    "\n",
    "Using an average or weighted average of model weights in the final model is a common technique in practice for ensuring the very best results are achieved from the training run. The approach is one of many tricks used in the Google Inception V2 and V3 deep convolutional neural network models for photo classification, a milestone in the field of deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Model Weight Ensemble Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will demonstrate how to use the average model weight ensemble to reduce the variance of an MLP on a simple multiclass classification problem. This example provides a template for applying the average model weight ensemble to your neural network for classification and regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classification Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Multiple Models to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Model With Average Models Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting With an Average Model Weight Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearly and Exponentially Decreasing Weighted Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section lists some ideas for extending the tutorial that you may wish to explore.\n",
    "\n",
    "* **Number of Models**. Evaluate the effect of many more models contributing their weights to the final model.\n",
    "* **Decay Rate**. Evaluate the effect on test performance of using different decay rates for an exponentially weighted average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you discovered how to combine the weights from multiple different models into a single model for making predictions. Specifically, you learned:\n",
    "\n",
    "* The stochastic and challenging nature of training neural networks can mean that the optimization process does not converge.\n",
    "* Creating a model with the average of the weights from models observed towards the end of a training run can result in a more stable and sometimes better-performing solution.\n",
    "* How to develop final models created with the equal, linearly, and exponentially weighted average of model parameters from multiple saved models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
