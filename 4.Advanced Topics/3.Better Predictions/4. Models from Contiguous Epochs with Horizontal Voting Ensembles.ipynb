{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models from Contiguous Epochs With Horizontal Voting Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictive modeling problems where the training dataset is small relative to the number of unlabeled examples are challenging. Neural networks can perform well on these types of problems, although they can suffer from high variance in model performance as measured on training or hold-out validation datasets. This makes choosing which model to use as the final model risky, as there is no clear signal as to which model is better than another toward the end of the training run. The horizontal voting ensemble is a simple method to address this issue. A collection of models saved over contiguous training epochs towards the end of a training run are saved and used as an ensemble that results in more stable and better performance on average randomly choosing a single final model. In this tutorial, you will discover how to reduce the variance of a final deep learning neural network model using a horizontal voting ensemble. After completing this tutorial, you will know:\n",
    "\n",
    "* It is challenging to choose a final neural network model with high variance on a training dataset.\n",
    "* Horizontal voting ensembles provide a way to reduce variance and improve average model performance for models with high variance using a single training run.\n",
    "* How to develop a horizontal voting ensemble in Python using Keras to improve the performance of a final Multilayer Perceptron model for multiclass classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horizontal Voting Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble learning combines the predictions from multiple models. A challenge when using ensemble learning when using deep learning methods is that given very large datasets and large models, a training run may take days, weeks, or even months. Training multiple models may not be feasible. An alternative source of models that may contribute to an ensemble is the state of a single model at different points during training. Horizontal voting is an ensemble method proposed by Jingjing Xie et al. in their 2013 paper *Horizontal and Vertical Ensemble with Deep Representation for Classification*.\n",
    "\n",
    "The method involves using multiple models from the end of a contiguous block of epochs before the end of training in an ensemble to make predictions. The approach was developed specifically for those predictive modeling problems where the training dataset is relatively small compared to the number of predictions required by the model. This results in a model that has a high variance in performance during training. In this situation, using the final model or any given model toward the end of the training process is risky given the variance in performance.\n",
    "\n",
    "Instead, the authors suggest using all models in an ensemble from a contiguous block of epochs during training, such as models from the last 200 epochs. The result is predictions by the ensemble that are as good as or better than any single model in the ensemble.\n",
    "\n",
    "As such, the horizontal voting ensemble method provides an ideal method for both cases where a given model requires vast computational resources to train and/or cases where final model selection is challenging given the high variance of training due to the use of a relatively small training dataset. Now that we are familiar with horizontal voting, we can implement the procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horizontal Voting Ensembles Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will demonstrate how to use the horizontal voting ensemble to reduce the variance of an MLP on a simple multiclass classification problem. This example provides a template for applying the horizontal voting ensemble to your neural network for classification and regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a small multiclass classification problem as the basis to demonstrate a horizontal voting ensemble. The scikit-learn class provides the `make_blobs()` function that can be used to create a multiclass classification problem with the prescribed number of samples, input variables, classes, and variance of samples within a class. We use this problem with 1,000 examples, with input variables (to represent the x and y coordinates of the points) and a standard deviation of 2.0 for points within each group. We will use the same random state (seed for the pseudorandom number generator) to ensure that we always get the same data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are the input and output elements of a dataset that we can model. In order to get a feeling for the complexity of the problem, we can graph each point on a two-dimensional scatter plot and color each point by class value. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# scatter plot of blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib import pyplot\n",
    "from numpy import where\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "\n",
    "# scatter plot for each class value\n",
    "for class_value in range(3):\n",
    "    # select indices of points with the class label\n",
    "    row_ix = where(y == class_value)\n",
    "\n",
    "    # scatter plot for points with a different color\n",
    "    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n",
    "\n",
    "# show plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example creates a scatter plot of the entire dataset. We can see that the standard deviation of 2.0 means that the classes are not linearly separable (separable by a line), causing many ambiguous points. This is desirable because the problem is non-trivial and will allow a neural network model to find many different *good enough* candidate solutions resulting in a high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we define a model, we need to contrive an appropriate problem for a horizontal voting ensemble. In our problem, the training dataset is relatively small. Specifically, there is a 10:1 ratio of examples in the training dataset to the holdout dataset. This mimics a situation where we may have a vast number of unlabeled examples and a small number of labeled examples with which to train a model. We will create 1,100 data points from the blobs problem. The model will be trained on the first 100 points, and the remaining 1,000 will be held back in a test dataset, unavailable to the model.\n",
    "\n",
    "The problem is a multiclass classification problem, and we will model it using a softmax activation function on the output layer. This means that the model will predict a vector with three elements with the probability that the sample belongs to each of the three classes. Therefore, the first step is to one-hot encode the class values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2) (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "# scatter plot of blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "\n",
    "# one hot encode output variable\n",
    "y = to_categorical(y)\n",
    "\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "print(trainX.shape, testX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can define and compile the model. The model will expect samples with two input variables. The model then has a single hidden layer with 25 nodes and a rectified linear activation function, an output layer with three nodes to predict the probability of each of the three classes, and a softmax activation function. Because the problem is multiclass, we will use the categorical cross-entropy loss function to optimize the model and the efficient Adam flavor of stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=2, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is fit for 1000 training epochs, and we will evaluate each epoch on the test set, using the test set as a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4/4 [==============================] - 1s 98ms/step - loss: 1.3494 - accuracy: 0.2170 - val_loss: 1.2928 - val_accuracy: 0.2000\n",
      "Epoch 2/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 1.2909 - accuracy: 0.2338 - val_loss: 1.2491 - val_accuracy: 0.2100\n",
      "Epoch 3/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.2287 - accuracy: 0.2493 - val_loss: 1.2054 - val_accuracy: 0.2280\n",
      "Epoch 4/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.1781 - accuracy: 0.2837 - val_loss: 1.1652 - val_accuracy: 0.2830\n",
      "Epoch 5/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.1515 - accuracy: 0.3087 - val_loss: 1.1282 - val_accuracy: 0.3970\n",
      "Epoch 6/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.1192 - accuracy: 0.3680 - val_loss: 1.0969 - val_accuracy: 0.4350\n",
      "Epoch 7/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.0928 - accuracy: 0.4722 - val_loss: 1.0698 - val_accuracy: 0.4480\n",
      "Epoch 8/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.0732 - accuracy: 0.4493 - val_loss: 1.0448 - val_accuracy: 0.4430\n",
      "Epoch 9/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.0082 - accuracy: 0.5030 - val_loss: 1.0225 - val_accuracy: 0.4430\n",
      "Epoch 10/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.9925 - accuracy: 0.5101 - val_loss: 1.0022 - val_accuracy: 0.4500\n",
      "Epoch 11/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.9554 - accuracy: 0.5424 - val_loss: 0.9843 - val_accuracy: 0.4550\n",
      "Epoch 12/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.9360 - accuracy: 0.5238 - val_loss: 0.9690 - val_accuracy: 0.4540\n",
      "Epoch 13/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.9474 - accuracy: 0.5057 - val_loss: 0.9541 - val_accuracy: 0.4630\n",
      "Epoch 14/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.9141 - accuracy: 0.5554 - val_loss: 0.9400 - val_accuracy: 0.4730\n",
      "Epoch 15/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.8797 - accuracy: 0.5763 - val_loss: 0.9258 - val_accuracy: 0.4800\n",
      "Epoch 16/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.8923 - accuracy: 0.5410 - val_loss: 0.9125 - val_accuracy: 0.4860\n",
      "Epoch 17/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.8473 - accuracy: 0.5513 - val_loss: 0.9007 - val_accuracy: 0.4910\n",
      "Epoch 18/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.8516 - accuracy: 0.5707 - val_loss: 0.8896 - val_accuracy: 0.5000\n",
      "Epoch 19/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.8363 - accuracy: 0.5667 - val_loss: 0.8777 - val_accuracy: 0.5160\n",
      "Epoch 20/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.7957 - accuracy: 0.6091 - val_loss: 0.8648 - val_accuracy: 0.5320\n",
      "Epoch 21/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.7985 - accuracy: 0.6242 - val_loss: 0.8532 - val_accuracy: 0.5430\n",
      "Epoch 22/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.8089 - accuracy: 0.5982 - val_loss: 0.8426 - val_accuracy: 0.5510\n",
      "Epoch 23/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.7502 - accuracy: 0.6294 - val_loss: 0.8332 - val_accuracy: 0.5600\n",
      "Epoch 24/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.7822 - accuracy: 0.6086 - val_loss: 0.8235 - val_accuracy: 0.5690\n",
      "Epoch 25/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.7812 - accuracy: 0.6095 - val_loss: 0.8148 - val_accuracy: 0.5730\n",
      "Epoch 26/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.7747 - accuracy: 0.6022 - val_loss: 0.8062 - val_accuracy: 0.5780\n",
      "Epoch 27/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.7876 - accuracy: 0.6204 - val_loss: 0.7992 - val_accuracy: 0.5850\n",
      "Epoch 28/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.7536 - accuracy: 0.6505 - val_loss: 0.7915 - val_accuracy: 0.5900\n",
      "Epoch 29/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.7381 - accuracy: 0.6661 - val_loss: 0.7835 - val_accuracy: 0.5980\n",
      "Epoch 30/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.7170 - accuracy: 0.6671 - val_loss: 0.7752 - val_accuracy: 0.6070\n",
      "Epoch 31/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.7296 - accuracy: 0.6637 - val_loss: 0.7675 - val_accuracy: 0.6150\n",
      "Epoch 32/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.7119 - accuracy: 0.6699 - val_loss: 0.7599 - val_accuracy: 0.6230\n",
      "Epoch 33/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.6933 - accuracy: 0.7060 - val_loss: 0.7525 - val_accuracy: 0.6320\n",
      "Epoch 34/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.6880 - accuracy: 0.6968 - val_loss: 0.7456 - val_accuracy: 0.6400\n",
      "Epoch 35/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6803 - accuracy: 0.6793 - val_loss: 0.7394 - val_accuracy: 0.6450\n",
      "Epoch 36/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6612 - accuracy: 0.7177 - val_loss: 0.7335 - val_accuracy: 0.6540\n",
      "Epoch 37/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.6753 - accuracy: 0.7090 - val_loss: 0.7290 - val_accuracy: 0.6550\n",
      "Epoch 38/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.6942 - accuracy: 0.6843 - val_loss: 0.7245 - val_accuracy: 0.6560\n",
      "Epoch 39/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.6426 - accuracy: 0.7031 - val_loss: 0.7199 - val_accuracy: 0.6600\n",
      "Epoch 40/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.7090 - accuracy: 0.6416 - val_loss: 0.7160 - val_accuracy: 0.6610\n",
      "Epoch 41/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6527 - accuracy: 0.6906 - val_loss: 0.7116 - val_accuracy: 0.6610\n",
      "Epoch 42/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.6614 - accuracy: 0.6843 - val_loss: 0.7079 - val_accuracy: 0.6650\n",
      "Epoch 43/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6494 - accuracy: 0.6979 - val_loss: 0.7046 - val_accuracy: 0.6670\n",
      "Epoch 44/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6641 - accuracy: 0.6862 - val_loss: 0.7029 - val_accuracy: 0.6700\n",
      "Epoch 45/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.6661 - accuracy: 0.6923 - val_loss: 0.7008 - val_accuracy: 0.6700\n",
      "Epoch 46/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6817 - accuracy: 0.6861 - val_loss: 0.6983 - val_accuracy: 0.6680\n",
      "Epoch 47/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.6494 - accuracy: 0.7132 - val_loss: 0.6950 - val_accuracy: 0.6700\n",
      "Epoch 48/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.6568 - accuracy: 0.6955 - val_loss: 0.6911 - val_accuracy: 0.6700\n",
      "Epoch 49/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6510 - accuracy: 0.6748 - val_loss: 0.6874 - val_accuracy: 0.6730\n",
      "Epoch 50/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5832 - accuracy: 0.7508 - val_loss: 0.6835 - val_accuracy: 0.6760\n",
      "Epoch 51/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.6270 - accuracy: 0.6967 - val_loss: 0.6798 - val_accuracy: 0.6820\n",
      "Epoch 52/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.6273 - accuracy: 0.6913 - val_loss: 0.6768 - val_accuracy: 0.6830\n",
      "Epoch 53/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.6500 - accuracy: 0.6913 - val_loss: 0.6741 - val_accuracy: 0.6840\n",
      "Epoch 54/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.6334 - accuracy: 0.7027 - val_loss: 0.6715 - val_accuracy: 0.6850\n",
      "Epoch 55/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.6165 - accuracy: 0.6975 - val_loss: 0.6692 - val_accuracy: 0.6860\n",
      "Epoch 56/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6242 - accuracy: 0.7142 - val_loss: 0.6676 - val_accuracy: 0.6870\n",
      "Epoch 57/1000\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.6439 - accuracy: 0.6861 - val_loss: 0.6661 - val_accuracy: 0.6870\n",
      "Epoch 58/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.6737 - accuracy: 0.6725 - val_loss: 0.6653 - val_accuracy: 0.6880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.6024 - accuracy: 0.7246 - val_loss: 0.6659 - val_accuracy: 0.6840\n",
      "Epoch 60/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.6444 - accuracy: 0.6850 - val_loss: 0.6658 - val_accuracy: 0.6820\n",
      "Epoch 61/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5702 - accuracy: 0.7413 - val_loss: 0.6645 - val_accuracy: 0.6810\n",
      "Epoch 62/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5825 - accuracy: 0.7309 - val_loss: 0.6633 - val_accuracy: 0.6800\n",
      "Epoch 63/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.6303 - accuracy: 0.7007 - val_loss: 0.6638 - val_accuracy: 0.6790\n",
      "Epoch 64/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.6205 - accuracy: 0.7100 - val_loss: 0.6631 - val_accuracy: 0.6790\n",
      "Epoch 65/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.6268 - accuracy: 0.6955 - val_loss: 0.6625 - val_accuracy: 0.6790\n",
      "Epoch 66/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.6208 - accuracy: 0.6871 - val_loss: 0.6627 - val_accuracy: 0.6800\n",
      "Epoch 67/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.6211 - accuracy: 0.6892 - val_loss: 0.6632 - val_accuracy: 0.6830\n",
      "Epoch 68/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6178 - accuracy: 0.6850 - val_loss: 0.6640 - val_accuracy: 0.6800\n",
      "Epoch 69/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.6026 - accuracy: 0.6831 - val_loss: 0.6640 - val_accuracy: 0.6810\n",
      "Epoch 70/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.6162 - accuracy: 0.6861 - val_loss: 0.6627 - val_accuracy: 0.6810\n",
      "Epoch 71/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5942 - accuracy: 0.7225 - val_loss: 0.6614 - val_accuracy: 0.6840\n",
      "Epoch 72/1000\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.5867 - accuracy: 0.7059 - val_loss: 0.6614 - val_accuracy: 0.6820\n",
      "Epoch 73/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5586 - accuracy: 0.7350 - val_loss: 0.6596 - val_accuracy: 0.6830\n",
      "Epoch 74/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5950 - accuracy: 0.7068 - val_loss: 0.6575 - val_accuracy: 0.6810\n",
      "Epoch 75/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.6119 - accuracy: 0.7069 - val_loss: 0.6540 - val_accuracy: 0.6830\n",
      "Epoch 76/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5987 - accuracy: 0.7121 - val_loss: 0.6521 - val_accuracy: 0.6850\n",
      "Epoch 77/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5835 - accuracy: 0.7298 - val_loss: 0.6496 - val_accuracy: 0.6870\n",
      "Epoch 78/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5610 - accuracy: 0.7236 - val_loss: 0.6467 - val_accuracy: 0.6910\n",
      "Epoch 79/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6095 - accuracy: 0.6798 - val_loss: 0.6425 - val_accuracy: 0.6900\n",
      "Epoch 80/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5620 - accuracy: 0.7236 - val_loss: 0.6394 - val_accuracy: 0.6930\n",
      "Epoch 81/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.6278 - accuracy: 0.6725 - val_loss: 0.6365 - val_accuracy: 0.6940\n",
      "Epoch 82/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5654 - accuracy: 0.7194 - val_loss: 0.6334 - val_accuracy: 0.6970\n",
      "Epoch 83/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5734 - accuracy: 0.7286 - val_loss: 0.6306 - val_accuracy: 0.7030\n",
      "Epoch 84/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.6070 - accuracy: 0.6974 - val_loss: 0.6277 - val_accuracy: 0.7070\n",
      "Epoch 85/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5918 - accuracy: 0.7088 - val_loss: 0.6259 - val_accuracy: 0.7080\n",
      "Epoch 86/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5858 - accuracy: 0.7140 - val_loss: 0.6239 - val_accuracy: 0.7080\n",
      "Epoch 87/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5609 - accuracy: 0.7161 - val_loss: 0.6222 - val_accuracy: 0.7080\n",
      "Epoch 88/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5696 - accuracy: 0.7203 - val_loss: 0.6209 - val_accuracy: 0.7090\n",
      "Epoch 89/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5759 - accuracy: 0.7068 - val_loss: 0.6193 - val_accuracy: 0.7100\n",
      "Epoch 90/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5697 - accuracy: 0.7130 - val_loss: 0.6177 - val_accuracy: 0.7100\n",
      "Epoch 91/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5923 - accuracy: 0.7047 - val_loss: 0.6165 - val_accuracy: 0.7090\n",
      "Epoch 92/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5336 - accuracy: 0.7349 - val_loss: 0.6155 - val_accuracy: 0.7090\n",
      "Epoch 93/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6033 - accuracy: 0.6786 - val_loss: 0.6142 - val_accuracy: 0.7100\n",
      "Epoch 94/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5388 - accuracy: 0.7318 - val_loss: 0.6126 - val_accuracy: 0.7110\n",
      "Epoch 95/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5447 - accuracy: 0.7099 - val_loss: 0.6108 - val_accuracy: 0.7130\n",
      "Epoch 96/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5572 - accuracy: 0.7088 - val_loss: 0.6093 - val_accuracy: 0.7150\n",
      "Epoch 97/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6084 - accuracy: 0.6682 - val_loss: 0.6071 - val_accuracy: 0.7160\n",
      "Epoch 98/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5518 - accuracy: 0.7297 - val_loss: 0.6050 - val_accuracy: 0.7170\n",
      "Epoch 99/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5302 - accuracy: 0.7432 - val_loss: 0.6039 - val_accuracy: 0.7180\n",
      "Epoch 100/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5478 - accuracy: 0.7130 - val_loss: 0.6053 - val_accuracy: 0.7190\n",
      "Epoch 101/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5511 - accuracy: 0.7193 - val_loss: 0.6058 - val_accuracy: 0.7200\n",
      "Epoch 102/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5450 - accuracy: 0.7255 - val_loss: 0.6047 - val_accuracy: 0.7200\n",
      "Epoch 103/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5627 - accuracy: 0.7140 - val_loss: 0.6038 - val_accuracy: 0.7200\n",
      "Epoch 104/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5429 - accuracy: 0.7193 - val_loss: 0.6028 - val_accuracy: 0.7180\n",
      "Epoch 105/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5467 - accuracy: 0.7151 - val_loss: 0.6014 - val_accuracy: 0.7200\n",
      "Epoch 106/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5210 - accuracy: 0.7432 - val_loss: 0.5989 - val_accuracy: 0.7240\n",
      "Epoch 107/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.5454 - accuracy: 0.7203 - val_loss: 0.5964 - val_accuracy: 0.7240\n",
      "Epoch 108/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5216 - accuracy: 0.7286 - val_loss: 0.5946 - val_accuracy: 0.7260\n",
      "Epoch 109/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5133 - accuracy: 0.7463 - val_loss: 0.5930 - val_accuracy: 0.7280\n",
      "Epoch 110/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5244 - accuracy: 0.7255 - val_loss: 0.5915 - val_accuracy: 0.7300\n",
      "Epoch 111/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5499 - accuracy: 0.7130 - val_loss: 0.5912 - val_accuracy: 0.7270\n",
      "Epoch 112/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5233 - accuracy: 0.7307 - val_loss: 0.5912 - val_accuracy: 0.7300\n",
      "Epoch 113/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5703 - accuracy: 0.6984 - val_loss: 0.5920 - val_accuracy: 0.7310\n",
      "Epoch 114/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4993 - accuracy: 0.7443 - val_loss: 0.5928 - val_accuracy: 0.7290\n",
      "Epoch 115/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5358 - accuracy: 0.7088 - val_loss: 0.5934 - val_accuracy: 0.7280\n",
      "Epoch 116/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4915 - accuracy: 0.7380 - val_loss: 0.5912 - val_accuracy: 0.7310\n",
      "Epoch 117/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5173 - accuracy: 0.7255 - val_loss: 0.5877 - val_accuracy: 0.7330\n",
      "Epoch 118/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5011 - accuracy: 0.7660 - val_loss: 0.5849 - val_accuracy: 0.7380\n",
      "Epoch 119/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5124 - accuracy: 0.7462 - val_loss: 0.5837 - val_accuracy: 0.7390\n",
      "Epoch 120/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5473 - accuracy: 0.7045 - val_loss: 0.5826 - val_accuracy: 0.7390\n",
      "Epoch 121/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5055 - accuracy: 0.7368 - val_loss: 0.5821 - val_accuracy: 0.7360\n",
      "Epoch 122/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5131 - accuracy: 0.7483 - val_loss: 0.5812 - val_accuracy: 0.7320\n",
      "Epoch 123/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5122 - accuracy: 0.7410 - val_loss: 0.5810 - val_accuracy: 0.7330\n",
      "Epoch 124/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5110 - accuracy: 0.7347 - val_loss: 0.5804 - val_accuracy: 0.7340\n",
      "Epoch 125/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5303 - accuracy: 0.7285 - val_loss: 0.5801 - val_accuracy: 0.7330\n",
      "Epoch 126/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5247 - accuracy: 0.7316 - val_loss: 0.5790 - val_accuracy: 0.7330\n",
      "Epoch 127/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5560 - accuracy: 0.7024 - val_loss: 0.5781 - val_accuracy: 0.7340\n",
      "Epoch 128/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5440 - accuracy: 0.7097 - val_loss: 0.5764 - val_accuracy: 0.7350\n",
      "Epoch 129/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5242 - accuracy: 0.7326 - val_loss: 0.5751 - val_accuracy: 0.7390\n",
      "Epoch 130/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5053 - accuracy: 0.7264 - val_loss: 0.5735 - val_accuracy: 0.7390\n",
      "Epoch 131/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5022 - accuracy: 0.7326 - val_loss: 0.5728 - val_accuracy: 0.7400\n",
      "Epoch 132/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4941 - accuracy: 0.7378 - val_loss: 0.5719 - val_accuracy: 0.7410\n",
      "Epoch 133/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5077 - accuracy: 0.7233 - val_loss: 0.5725 - val_accuracy: 0.7390\n",
      "Epoch 134/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5056 - accuracy: 0.7368 - val_loss: 0.5732 - val_accuracy: 0.7340\n",
      "Epoch 135/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5027 - accuracy: 0.7602 - val_loss: 0.5722 - val_accuracy: 0.7340\n",
      "Epoch 136/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5214 - accuracy: 0.7696 - val_loss: 0.5710 - val_accuracy: 0.7350\n",
      "Epoch 137/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4864 - accuracy: 0.7738 - val_loss: 0.5691 - val_accuracy: 0.7400\n",
      "Epoch 138/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5108 - accuracy: 0.7530 - val_loss: 0.5656 - val_accuracy: 0.7500\n",
      "Epoch 139/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4887 - accuracy: 0.7594 - val_loss: 0.5607 - val_accuracy: 0.7570\n",
      "Epoch 140/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5190 - accuracy: 0.7375 - val_loss: 0.5576 - val_accuracy: 0.7610\n",
      "Epoch 141/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5064 - accuracy: 0.7438 - val_loss: 0.5553 - val_accuracy: 0.7600\n",
      "Epoch 142/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5113 - accuracy: 0.7509 - val_loss: 0.5534 - val_accuracy: 0.7580\n",
      "Epoch 143/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5086 - accuracy: 0.7901 - val_loss: 0.5518 - val_accuracy: 0.7640\n",
      "Epoch 144/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4922 - accuracy: 0.7590 - val_loss: 0.5508 - val_accuracy: 0.7660\n",
      "Epoch 145/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5072 - accuracy: 0.7632 - val_loss: 0.5499 - val_accuracy: 0.7670\n",
      "Epoch 146/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5352 - accuracy: 0.7507 - val_loss: 0.5493 - val_accuracy: 0.7690\n",
      "Epoch 147/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4993 - accuracy: 0.7736 - val_loss: 0.5491 - val_accuracy: 0.7680\n",
      "Epoch 148/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4885 - accuracy: 0.7747 - val_loss: 0.5501 - val_accuracy: 0.7660\n",
      "Epoch 149/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4872 - accuracy: 0.7809 - val_loss: 0.5511 - val_accuracy: 0.7600\n",
      "Epoch 150/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.4937 - accuracy: 0.7519 - val_loss: 0.5524 - val_accuracy: 0.7580\n",
      "Epoch 151/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4985 - accuracy: 0.7682 - val_loss: 0.5529 - val_accuracy: 0.7570\n",
      "Epoch 152/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4425 - accuracy: 0.8181 - val_loss: 0.5531 - val_accuracy: 0.7550\n",
      "Epoch 153/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4726 - accuracy: 0.7983 - val_loss: 0.5538 - val_accuracy: 0.7540\n",
      "Epoch 154/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4972 - accuracy: 0.7754 - val_loss: 0.5519 - val_accuracy: 0.7580\n",
      "Epoch 155/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4943 - accuracy: 0.7714 - val_loss: 0.5496 - val_accuracy: 0.7610\n",
      "Epoch 156/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4899 - accuracy: 0.7714 - val_loss: 0.5472 - val_accuracy: 0.7640\n",
      "Epoch 157/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4933 - accuracy: 0.7723 - val_loss: 0.5450 - val_accuracy: 0.7650\n",
      "Epoch 158/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4685 - accuracy: 0.7849 - val_loss: 0.5427 - val_accuracy: 0.7690\n",
      "Epoch 159/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4683 - accuracy: 0.7807 - val_loss: 0.5415 - val_accuracy: 0.7670\n",
      "Epoch 160/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4845 - accuracy: 0.7931 - val_loss: 0.5395 - val_accuracy: 0.7690\n",
      "Epoch 161/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4451 - accuracy: 0.8263 - val_loss: 0.5375 - val_accuracy: 0.7760\n",
      "Epoch 162/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4753 - accuracy: 0.8085 - val_loss: 0.5351 - val_accuracy: 0.7800\n",
      "Epoch 163/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4695 - accuracy: 0.8148 - val_loss: 0.5329 - val_accuracy: 0.7800\n",
      "Epoch 164/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4676 - accuracy: 0.7775 - val_loss: 0.5312 - val_accuracy: 0.7840\n",
      "Epoch 165/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4582 - accuracy: 0.8013 - val_loss: 0.5299 - val_accuracy: 0.7830\n",
      "Epoch 166/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4552 - accuracy: 0.8002 - val_loss: 0.5283 - val_accuracy: 0.7820\n",
      "Epoch 167/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4400 - accuracy: 0.8304 - val_loss: 0.5266 - val_accuracy: 0.7840\n",
      "Epoch 168/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4672 - accuracy: 0.7867 - val_loss: 0.5245 - val_accuracy: 0.7860\n",
      "Epoch 169/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4570 - accuracy: 0.8221 - val_loss: 0.5224 - val_accuracy: 0.7910\n",
      "Epoch 170/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4674 - accuracy: 0.8077 - val_loss: 0.5209 - val_accuracy: 0.7880\n",
      "Epoch 171/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4412 - accuracy: 0.8098 - val_loss: 0.5196 - val_accuracy: 0.7870\n",
      "Epoch 172/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4747 - accuracy: 0.7660 - val_loss: 0.5192 - val_accuracy: 0.7890\n",
      "Epoch 173/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4531 - accuracy: 0.7993 - val_loss: 0.5189 - val_accuracy: 0.7900\n",
      "Epoch 174/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4882 - accuracy: 0.7844 - val_loss: 0.5190 - val_accuracy: 0.7910\n",
      "Epoch 175/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4483 - accuracy: 0.8209 - val_loss: 0.5209 - val_accuracy: 0.7890\n",
      "Epoch 176/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4789 - accuracy: 0.8061 - val_loss: 0.5221 - val_accuracy: 0.7860\n",
      "Epoch 177/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4673 - accuracy: 0.8310 - val_loss: 0.5232 - val_accuracy: 0.7840\n",
      "Epoch 178/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4792 - accuracy: 0.8216 - val_loss: 0.5233 - val_accuracy: 0.7850\n",
      "Epoch 179/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4653 - accuracy: 0.8547 - val_loss: 0.5225 - val_accuracy: 0.7850\n",
      "Epoch 180/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4338 - accuracy: 0.8454 - val_loss: 0.5217 - val_accuracy: 0.7850\n",
      "Epoch 181/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4578 - accuracy: 0.8214 - val_loss: 0.5195 - val_accuracy: 0.7860\n",
      "Epoch 182/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4443 - accuracy: 0.8330 - val_loss: 0.5182 - val_accuracy: 0.7870\n",
      "Epoch 183/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4212 - accuracy: 0.8528 - val_loss: 0.5170 - val_accuracy: 0.7870\n",
      "Epoch 184/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4403 - accuracy: 0.8495 - val_loss: 0.5157 - val_accuracy: 0.7890\n",
      "Epoch 185/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4751 - accuracy: 0.8122 - val_loss: 0.5141 - val_accuracy: 0.7890\n",
      "Epoch 186/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4445 - accuracy: 0.8270 - val_loss: 0.5125 - val_accuracy: 0.7900\n",
      "Epoch 187/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4796 - accuracy: 0.8113 - val_loss: 0.5106 - val_accuracy: 0.7890\n",
      "Epoch 188/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4586 - accuracy: 0.8084 - val_loss: 0.5096 - val_accuracy: 0.7880\n",
      "Epoch 189/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4496 - accuracy: 0.7908 - val_loss: 0.5090 - val_accuracy: 0.7870\n",
      "Epoch 190/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4680 - accuracy: 0.7844 - val_loss: 0.5093 - val_accuracy: 0.7870\n",
      "Epoch 191/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4446 - accuracy: 0.8082 - val_loss: 0.5084 - val_accuracy: 0.7880\n",
      "Epoch 192/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4407 - accuracy: 0.8155 - val_loss: 0.5072 - val_accuracy: 0.7900\n",
      "Epoch 193/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4451 - accuracy: 0.8124 - val_loss: 0.5069 - val_accuracy: 0.7890\n",
      "Epoch 194/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4264 - accuracy: 0.8382 - val_loss: 0.5066 - val_accuracy: 0.7880\n",
      "Epoch 195/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4367 - accuracy: 0.8330 - val_loss: 0.5054 - val_accuracy: 0.7880\n",
      "Epoch 196/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4158 - accuracy: 0.8455 - val_loss: 0.5047 - val_accuracy: 0.7880\n",
      "Epoch 197/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4281 - accuracy: 0.8382 - val_loss: 0.5055 - val_accuracy: 0.7920\n",
      "Epoch 198/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4385 - accuracy: 0.8226 - val_loss: 0.5043 - val_accuracy: 0.7900\n",
      "Epoch 199/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4221 - accuracy: 0.8073 - val_loss: 0.5013 - val_accuracy: 0.7900\n",
      "Epoch 200/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4284 - accuracy: 0.8033 - val_loss: 0.4979 - val_accuracy: 0.7960\n",
      "Epoch 201/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4219 - accuracy: 0.8054 - val_loss: 0.4957 - val_accuracy: 0.7950\n",
      "Epoch 202/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3876 - accuracy: 0.8170 - val_loss: 0.4951 - val_accuracy: 0.7920\n",
      "Epoch 203/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4450 - accuracy: 0.7920 - val_loss: 0.4947 - val_accuracy: 0.7930\n",
      "Epoch 204/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.4651 - accuracy: 0.7710 - val_loss: 0.4943 - val_accuracy: 0.7930\n",
      "Epoch 205/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4320 - accuracy: 0.8032 - val_loss: 0.4947 - val_accuracy: 0.7940\n",
      "Epoch 206/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4472 - accuracy: 0.7877 - val_loss: 0.4955 - val_accuracy: 0.7900\n",
      "Epoch 207/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4170 - accuracy: 0.7929 - val_loss: 0.4966 - val_accuracy: 0.7880\n",
      "Epoch 208/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4569 - accuracy: 0.7875 - val_loss: 0.4986 - val_accuracy: 0.7900\n",
      "Epoch 209/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4309 - accuracy: 0.8155 - val_loss: 0.4986 - val_accuracy: 0.7890\n",
      "Epoch 210/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4327 - accuracy: 0.8030 - val_loss: 0.4983 - val_accuracy: 0.7900\n",
      "Epoch 211/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4641 - accuracy: 0.7988 - val_loss: 0.4969 - val_accuracy: 0.7910\n",
      "Epoch 212/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4245 - accuracy: 0.8061 - val_loss: 0.4947 - val_accuracy: 0.7910\n",
      "Epoch 213/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4128 - accuracy: 0.8145 - val_loss: 0.4928 - val_accuracy: 0.7920\n",
      "Epoch 214/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4115 - accuracy: 0.8186 - val_loss: 0.4916 - val_accuracy: 0.7930\n",
      "Epoch 215/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4516 - accuracy: 0.7966 - val_loss: 0.4910 - val_accuracy: 0.7930\n",
      "Epoch 216/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4098 - accuracy: 0.8247 - val_loss: 0.4904 - val_accuracy: 0.7950\n",
      "Epoch 217/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4300 - accuracy: 0.8091 - val_loss: 0.4900 - val_accuracy: 0.7970\n",
      "Epoch 218/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3936 - accuracy: 0.8424 - val_loss: 0.4894 - val_accuracy: 0.7960\n",
      "Epoch 219/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4271 - accuracy: 0.8216 - val_loss: 0.4888 - val_accuracy: 0.7950\n",
      "Epoch 220/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4486 - accuracy: 0.8061 - val_loss: 0.4882 - val_accuracy: 0.7960\n",
      "Epoch 221/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3978 - accuracy: 0.8061 - val_loss: 0.4891 - val_accuracy: 0.7960\n",
      "Epoch 222/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4368 - accuracy: 0.8063 - val_loss: 0.4926 - val_accuracy: 0.7900\n",
      "Epoch 223/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4165 - accuracy: 0.8115 - val_loss: 0.4954 - val_accuracy: 0.7920\n",
      "Epoch 224/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4477 - accuracy: 0.8018 - val_loss: 0.4968 - val_accuracy: 0.7910\n",
      "Epoch 225/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3907 - accuracy: 0.8424 - val_loss: 0.4956 - val_accuracy: 0.7920\n",
      "Epoch 226/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3891 - accuracy: 0.8403 - val_loss: 0.4909 - val_accuracy: 0.7940\n",
      "Epoch 227/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3679 - accuracy: 0.8455 - val_loss: 0.4880 - val_accuracy: 0.7970\n",
      "Epoch 228/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4220 - accuracy: 0.8185 - val_loss: 0.4865 - val_accuracy: 0.7970\n",
      "Epoch 229/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4359 - accuracy: 0.8143 - val_loss: 0.4863 - val_accuracy: 0.7950\n",
      "Epoch 230/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4099 - accuracy: 0.7997 - val_loss: 0.4863 - val_accuracy: 0.7960\n",
      "Epoch 231/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4096 - accuracy: 0.8330 - val_loss: 0.4858 - val_accuracy: 0.7960\n",
      "Epoch 232/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3837 - accuracy: 0.8435 - val_loss: 0.4850 - val_accuracy: 0.7960\n",
      "Epoch 233/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4138 - accuracy: 0.8216 - val_loss: 0.4844 - val_accuracy: 0.7950\n",
      "Epoch 234/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3961 - accuracy: 0.8299 - val_loss: 0.4843 - val_accuracy: 0.7950\n",
      "Epoch 235/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3995 - accuracy: 0.8422 - val_loss: 0.4842 - val_accuracy: 0.7960\n",
      "Epoch 236/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4049 - accuracy: 0.8183 - val_loss: 0.4836 - val_accuracy: 0.7940\n",
      "Epoch 237/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4195 - accuracy: 0.8082 - val_loss: 0.4824 - val_accuracy: 0.7930\n",
      "Epoch 238/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4151 - accuracy: 0.8061 - val_loss: 0.4821 - val_accuracy: 0.7940\n",
      "Epoch 239/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3894 - accuracy: 0.8311 - val_loss: 0.4826 - val_accuracy: 0.7950\n",
      "Epoch 240/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4111 - accuracy: 0.8113 - val_loss: 0.4829 - val_accuracy: 0.7940\n",
      "Epoch 241/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3815 - accuracy: 0.8528 - val_loss: 0.4835 - val_accuracy: 0.7950\n",
      "Epoch 242/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3962 - accuracy: 0.8207 - val_loss: 0.4868 - val_accuracy: 0.7940\n",
      "Epoch 243/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4225 - accuracy: 0.8237 - val_loss: 0.4891 - val_accuracy: 0.7950\n",
      "Epoch 244/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3831 - accuracy: 0.8424 - val_loss: 0.4917 - val_accuracy: 0.7920\n",
      "Epoch 245/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4049 - accuracy: 0.8341 - val_loss: 0.4903 - val_accuracy: 0.7950\n",
      "Epoch 246/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4169 - accuracy: 0.8185 - val_loss: 0.4875 - val_accuracy: 0.7950\n",
      "Epoch 247/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4055 - accuracy: 0.8278 - val_loss: 0.4857 - val_accuracy: 0.7950\n",
      "Epoch 248/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3643 - accuracy: 0.8497 - val_loss: 0.4844 - val_accuracy: 0.7930\n",
      "Epoch 249/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4109 - accuracy: 0.8216 - val_loss: 0.4827 - val_accuracy: 0.7940\n",
      "Epoch 250/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4076 - accuracy: 0.8216 - val_loss: 0.4804 - val_accuracy: 0.7930\n",
      "Epoch 251/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3917 - accuracy: 0.8370 - val_loss: 0.4795 - val_accuracy: 0.7940\n",
      "Epoch 252/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4027 - accuracy: 0.8360 - val_loss: 0.4791 - val_accuracy: 0.7940\n",
      "Epoch 253/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4010 - accuracy: 0.8362 - val_loss: 0.4777 - val_accuracy: 0.7950\n",
      "Epoch 254/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4009 - accuracy: 0.8018 - val_loss: 0.4758 - val_accuracy: 0.7970\n",
      "Epoch 255/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3769 - accuracy: 0.8310 - val_loss: 0.4739 - val_accuracy: 0.7970\n",
      "Epoch 256/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3697 - accuracy: 0.8424 - val_loss: 0.4728 - val_accuracy: 0.8010\n",
      "Epoch 257/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3599 - accuracy: 0.8488 - val_loss: 0.4716 - val_accuracy: 0.8050\n",
      "Epoch 258/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3733 - accuracy: 0.8238 - val_loss: 0.4694 - val_accuracy: 0.8100\n",
      "Epoch 259/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3925 - accuracy: 0.8268 - val_loss: 0.4685 - val_accuracy: 0.8070\n",
      "Epoch 260/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3773 - accuracy: 0.8278 - val_loss: 0.4674 - val_accuracy: 0.8070\n",
      "Epoch 261/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3813 - accuracy: 0.8207 - val_loss: 0.4666 - val_accuracy: 0.8060\n",
      "Epoch 262/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4201 - accuracy: 0.7999 - val_loss: 0.4658 - val_accuracy: 0.8070\n",
      "Epoch 263/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4190 - accuracy: 0.7811 - val_loss: 0.4650 - val_accuracy: 0.8070\n",
      "Epoch 264/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3680 - accuracy: 0.8622 - val_loss: 0.4650 - val_accuracy: 0.8070\n",
      "Epoch 265/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3928 - accuracy: 0.8164 - val_loss: 0.4666 - val_accuracy: 0.8080\n",
      "Epoch 266/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3560 - accuracy: 0.8216 - val_loss: 0.4689 - val_accuracy: 0.8050\n",
      "Epoch 267/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4004 - accuracy: 0.8204 - val_loss: 0.4698 - val_accuracy: 0.8020\n",
      "Epoch 268/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3893 - accuracy: 0.8431 - val_loss: 0.4697 - val_accuracy: 0.8010\n",
      "Epoch 269/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3811 - accuracy: 0.8504 - val_loss: 0.4691 - val_accuracy: 0.8000\n",
      "Epoch 270/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3768 - accuracy: 0.8525 - val_loss: 0.4682 - val_accuracy: 0.8010\n",
      "Epoch 271/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3838 - accuracy: 0.8412 - val_loss: 0.4683 - val_accuracy: 0.8000\n",
      "Epoch 272/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4156 - accuracy: 0.8152 - val_loss: 0.4675 - val_accuracy: 0.8010\n",
      "Epoch 273/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3783 - accuracy: 0.8289 - val_loss: 0.4673 - val_accuracy: 0.8040\n",
      "Epoch 274/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3559 - accuracy: 0.8537 - val_loss: 0.4672 - val_accuracy: 0.8000\n",
      "Epoch 275/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3812 - accuracy: 0.8278 - val_loss: 0.4660 - val_accuracy: 0.8060\n",
      "Epoch 276/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3433 - accuracy: 0.8549 - val_loss: 0.4664 - val_accuracy: 0.8040\n",
      "Epoch 277/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3831 - accuracy: 0.8370 - val_loss: 0.4673 - val_accuracy: 0.8010\n",
      "Epoch 278/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3864 - accuracy: 0.8433 - val_loss: 0.4675 - val_accuracy: 0.8020\n",
      "Epoch 279/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3841 - accuracy: 0.8454 - val_loss: 0.4677 - val_accuracy: 0.8020\n",
      "Epoch 280/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3590 - accuracy: 0.8443 - val_loss: 0.4683 - val_accuracy: 0.8030\n",
      "Epoch 281/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3522 - accuracy: 0.8464 - val_loss: 0.4679 - val_accuracy: 0.8030\n",
      "Epoch 282/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3779 - accuracy: 0.8339 - val_loss: 0.4683 - val_accuracy: 0.8030\n",
      "Epoch 283/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3650 - accuracy: 0.8466 - val_loss: 0.4699 - val_accuracy: 0.7990\n",
      "Epoch 284/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3869 - accuracy: 0.8297 - val_loss: 0.4720 - val_accuracy: 0.7970\n",
      "Epoch 285/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3742 - accuracy: 0.8546 - val_loss: 0.4721 - val_accuracy: 0.7990\n",
      "Epoch 286/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3657 - accuracy: 0.8516 - val_loss: 0.4708 - val_accuracy: 0.8000\n",
      "Epoch 287/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3612 - accuracy: 0.8299 - val_loss: 0.4708 - val_accuracy: 0.8010\n",
      "Epoch 288/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3544 - accuracy: 0.8445 - val_loss: 0.4699 - val_accuracy: 0.8000\n",
      "Epoch 289/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3714 - accuracy: 0.8320 - val_loss: 0.4659 - val_accuracy: 0.8040\n",
      "Epoch 290/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3801 - accuracy: 0.8207 - val_loss: 0.4627 - val_accuracy: 0.8080\n",
      "Epoch 291/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3586 - accuracy: 0.8061 - val_loss: 0.4619 - val_accuracy: 0.8080\n",
      "Epoch 292/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3754 - accuracy: 0.8174 - val_loss: 0.4605 - val_accuracy: 0.8080\n",
      "Epoch 293/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3447 - accuracy: 0.8372 - val_loss: 0.4586 - val_accuracy: 0.8080\n",
      "Epoch 294/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3622 - accuracy: 0.8185 - val_loss: 0.4584 - val_accuracy: 0.8080\n",
      "Epoch 295/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3465 - accuracy: 0.8507 - val_loss: 0.4589 - val_accuracy: 0.8090\n",
      "Epoch 296/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3745 - accuracy: 0.8247 - val_loss: 0.4585 - val_accuracy: 0.8080\n",
      "Epoch 297/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3563 - accuracy: 0.8455 - val_loss: 0.4582 - val_accuracy: 0.8090\n",
      "Epoch 298/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3998 - accuracy: 0.7872 - val_loss: 0.4575 - val_accuracy: 0.8090\n",
      "Epoch 299/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3571 - accuracy: 0.8301 - val_loss: 0.4571 - val_accuracy: 0.8080\n",
      "Epoch 300/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3753 - accuracy: 0.8185 - val_loss: 0.4569 - val_accuracy: 0.8080\n",
      "Epoch 301/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3723 - accuracy: 0.8216 - val_loss: 0.4570 - val_accuracy: 0.8060\n",
      "Epoch 302/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3869 - accuracy: 0.7966 - val_loss: 0.4571 - val_accuracy: 0.8070\n",
      "Epoch 303/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3607 - accuracy: 0.8403 - val_loss: 0.4570 - val_accuracy: 0.8070\n",
      "Epoch 304/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3564 - accuracy: 0.8237 - val_loss: 0.4565 - val_accuracy: 0.8110\n",
      "Epoch 305/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3568 - accuracy: 0.8277 - val_loss: 0.4561 - val_accuracy: 0.8110\n",
      "Epoch 306/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3553 - accuracy: 0.8454 - val_loss: 0.4564 - val_accuracy: 0.8130\n",
      "Epoch 307/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3839 - accuracy: 0.8235 - val_loss: 0.4566 - val_accuracy: 0.8130\n",
      "Epoch 308/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3511 - accuracy: 0.8299 - val_loss: 0.4575 - val_accuracy: 0.8120\n",
      "Epoch 309/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3542 - accuracy: 0.8494 - val_loss: 0.4583 - val_accuracy: 0.8110\n",
      "Epoch 310/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3326 - accuracy: 0.8775 - val_loss: 0.4584 - val_accuracy: 0.8100\n",
      "Epoch 311/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3378 - accuracy: 0.8588 - val_loss: 0.4585 - val_accuracy: 0.8090\n",
      "Epoch 312/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3709 - accuracy: 0.8421 - val_loss: 0.4579 - val_accuracy: 0.8100\n",
      "Epoch 313/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3506 - accuracy: 0.8556 - val_loss: 0.4576 - val_accuracy: 0.8100\n",
      "Epoch 314/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3370 - accuracy: 0.8494 - val_loss: 0.4570 - val_accuracy: 0.8100\n",
      "Epoch 315/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3422 - accuracy: 0.8671 - val_loss: 0.4567 - val_accuracy: 0.8100\n",
      "Epoch 316/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3747 - accuracy: 0.8245 - val_loss: 0.4570 - val_accuracy: 0.8060\n",
      "Epoch 317/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3787 - accuracy: 0.8245 - val_loss: 0.4592 - val_accuracy: 0.8060\n",
      "Epoch 318/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3835 - accuracy: 0.8205 - val_loss: 0.4604 - val_accuracy: 0.8040\n",
      "Epoch 319/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3493 - accuracy: 0.8341 - val_loss: 0.4610 - val_accuracy: 0.8050\n",
      "Epoch 320/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3415 - accuracy: 0.8310 - val_loss: 0.4603 - val_accuracy: 0.8040\n",
      "Epoch 321/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3722 - accuracy: 0.8091 - val_loss: 0.4605 - val_accuracy: 0.8040\n",
      "Epoch 322/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3623 - accuracy: 0.8204 - val_loss: 0.4598 - val_accuracy: 0.8060\n",
      "Epoch 323/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3395 - accuracy: 0.8424 - val_loss: 0.4576 - val_accuracy: 0.8080\n",
      "Epoch 324/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3228 - accuracy: 0.8610 - val_loss: 0.4564 - val_accuracy: 0.8100\n",
      "Epoch 325/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3678 - accuracy: 0.8287 - val_loss: 0.4557 - val_accuracy: 0.8070\n",
      "Epoch 326/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3497 - accuracy: 0.8382 - val_loss: 0.4553 - val_accuracy: 0.8080\n",
      "Epoch 327/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3644 - accuracy: 0.8310 - val_loss: 0.4553 - val_accuracy: 0.8080\n",
      "Epoch 328/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3598 - accuracy: 0.8391 - val_loss: 0.4553 - val_accuracy: 0.8080\n",
      "Epoch 329/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3622 - accuracy: 0.8546 - val_loss: 0.4542 - val_accuracy: 0.8100\n",
      "Epoch 330/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3824 - accuracy: 0.8329 - val_loss: 0.4531 - val_accuracy: 0.8090\n",
      "Epoch 331/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3670 - accuracy: 0.8341 - val_loss: 0.4514 - val_accuracy: 0.8100\n",
      "Epoch 332/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3398 - accuracy: 0.8185 - val_loss: 0.4508 - val_accuracy: 0.8110\n",
      "Epoch 333/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3585 - accuracy: 0.8494 - val_loss: 0.4509 - val_accuracy: 0.8110\n",
      "Epoch 334/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3714 - accuracy: 0.8430 - val_loss: 0.4514 - val_accuracy: 0.8100\n",
      "Epoch 335/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3675 - accuracy: 0.8515 - val_loss: 0.4514 - val_accuracy: 0.8140\n",
      "Epoch 336/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3672 - accuracy: 0.8421 - val_loss: 0.4509 - val_accuracy: 0.8120\n",
      "Epoch 337/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3636 - accuracy: 0.8504 - val_loss: 0.4502 - val_accuracy: 0.8110\n",
      "Epoch 338/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3548 - accuracy: 0.8483 - val_loss: 0.4498 - val_accuracy: 0.8120\n",
      "Epoch 339/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3598 - accuracy: 0.8471 - val_loss: 0.4503 - val_accuracy: 0.8120\n",
      "Epoch 340/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3747 - accuracy: 0.8461 - val_loss: 0.4508 - val_accuracy: 0.8110\n",
      "Epoch 341/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3411 - accuracy: 0.8648 - val_loss: 0.4513 - val_accuracy: 0.8110\n",
      "Epoch 342/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3530 - accuracy: 0.8740 - val_loss: 0.4523 - val_accuracy: 0.8100\n",
      "Epoch 343/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3547 - accuracy: 0.8492 - val_loss: 0.4530 - val_accuracy: 0.8100\n",
      "Epoch 344/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3399 - accuracy: 0.8494 - val_loss: 0.4534 - val_accuracy: 0.8090\n",
      "Epoch 345/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3704 - accuracy: 0.8214 - val_loss: 0.4537 - val_accuracy: 0.8080\n",
      "Epoch 346/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3524 - accuracy: 0.8700 - val_loss: 0.4548 - val_accuracy: 0.8080\n",
      "Epoch 347/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3426 - accuracy: 0.8607 - val_loss: 0.4561 - val_accuracy: 0.8060\n",
      "Epoch 348/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3201 - accuracy: 0.8794 - val_loss: 0.4560 - val_accuracy: 0.8060\n",
      "Epoch 349/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3302 - accuracy: 0.8709 - val_loss: 0.4550 - val_accuracy: 0.8070\n",
      "Epoch 350/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3356 - accuracy: 0.8678 - val_loss: 0.4534 - val_accuracy: 0.8080\n",
      "Epoch 351/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3661 - accuracy: 0.8369 - val_loss: 0.4529 - val_accuracy: 0.8100\n",
      "Epoch 352/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3503 - accuracy: 0.8400 - val_loss: 0.4530 - val_accuracy: 0.8100\n",
      "Epoch 353/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3699 - accuracy: 0.8223 - val_loss: 0.4540 - val_accuracy: 0.8110\n",
      "Epoch 354/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3612 - accuracy: 0.8390 - val_loss: 0.4555 - val_accuracy: 0.8100\n",
      "Epoch 355/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3545 - accuracy: 0.8482 - val_loss: 0.4557 - val_accuracy: 0.8100\n",
      "Epoch 356/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3696 - accuracy: 0.8388 - val_loss: 0.4571 - val_accuracy: 0.8090\n",
      "Epoch 357/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3524 - accuracy: 0.8636 - val_loss: 0.4586 - val_accuracy: 0.8040\n",
      "Epoch 358/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3364 - accuracy: 0.8700 - val_loss: 0.4593 - val_accuracy: 0.8040\n",
      "Epoch 359/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3675 - accuracy: 0.8336 - val_loss: 0.4573 - val_accuracy: 0.8050\n",
      "Epoch 360/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3289 - accuracy: 0.8740 - val_loss: 0.4562 - val_accuracy: 0.8060\n",
      "Epoch 361/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3332 - accuracy: 0.8753 - val_loss: 0.4552 - val_accuracy: 0.8070\n",
      "Epoch 362/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3395 - accuracy: 0.8403 - val_loss: 0.4542 - val_accuracy: 0.8060\n",
      "Epoch 363/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3495 - accuracy: 0.8277 - val_loss: 0.4541 - val_accuracy: 0.8050\n",
      "Epoch 364/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3303 - accuracy: 0.8454 - val_loss: 0.4542 - val_accuracy: 0.8050\n",
      "Epoch 365/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3250 - accuracy: 0.8525 - val_loss: 0.4545 - val_accuracy: 0.8080\n",
      "Epoch 366/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3313 - accuracy: 0.8483 - val_loss: 0.4552 - val_accuracy: 0.8060\n",
      "Epoch 367/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3251 - accuracy: 0.8485 - val_loss: 0.4563 - val_accuracy: 0.8080\n",
      "Epoch 368/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3579 - accuracy: 0.8226 - val_loss: 0.4560 - val_accuracy: 0.8060\n",
      "Epoch 369/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3384 - accuracy: 0.8339 - val_loss: 0.4557 - val_accuracy: 0.8040\n",
      "Epoch 370/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3619 - accuracy: 0.8369 - val_loss: 0.4551 - val_accuracy: 0.8070\n",
      "Epoch 371/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3374 - accuracy: 0.8400 - val_loss: 0.4554 - val_accuracy: 0.8080\n",
      "Epoch 372/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3343 - accuracy: 0.8629 - val_loss: 0.4561 - val_accuracy: 0.8060\n",
      "Epoch 373/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3109 - accuracy: 0.8744 - val_loss: 0.4569 - val_accuracy: 0.8060\n",
      "Epoch 374/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3620 - accuracy: 0.8525 - val_loss: 0.4569 - val_accuracy: 0.8080\n",
      "Epoch 375/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3666 - accuracy: 0.8358 - val_loss: 0.4573 - val_accuracy: 0.8050\n",
      "Epoch 376/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3542 - accuracy: 0.8400 - val_loss: 0.4583 - val_accuracy: 0.8060\n",
      "Epoch 377/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3621 - accuracy: 0.8213 - val_loss: 0.4589 - val_accuracy: 0.8070\n",
      "Epoch 378/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3404 - accuracy: 0.8495 - val_loss: 0.4592 - val_accuracy: 0.8070\n",
      "Epoch 379/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3355 - accuracy: 0.8681 - val_loss: 0.4592 - val_accuracy: 0.8070\n",
      "Epoch 380/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3485 - accuracy: 0.8369 - val_loss: 0.4594 - val_accuracy: 0.8070\n",
      "Epoch 381/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3693 - accuracy: 0.8327 - val_loss: 0.4598 - val_accuracy: 0.8040\n",
      "Epoch 382/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3765 - accuracy: 0.8338 - val_loss: 0.4610 - val_accuracy: 0.8050\n",
      "Epoch 383/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3505 - accuracy: 0.8400 - val_loss: 0.4625 - val_accuracy: 0.8050\n",
      "Epoch 384/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3493 - accuracy: 0.8494 - val_loss: 0.4629 - val_accuracy: 0.8030\n",
      "Epoch 385/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3908 - accuracy: 0.8171 - val_loss: 0.4622 - val_accuracy: 0.8060\n",
      "Epoch 386/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3178 - accuracy: 0.8711 - val_loss: 0.4600 - val_accuracy: 0.8060\n",
      "Epoch 387/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3427 - accuracy: 0.8329 - val_loss: 0.4583 - val_accuracy: 0.8100\n",
      "Epoch 388/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3592 - accuracy: 0.8306 - val_loss: 0.4571 - val_accuracy: 0.8060\n",
      "Epoch 389/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3684 - accuracy: 0.8244 - val_loss: 0.4566 - val_accuracy: 0.8070\n",
      "Epoch 390/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3587 - accuracy: 0.8336 - val_loss: 0.4569 - val_accuracy: 0.8080\n",
      "Epoch 391/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3619 - accuracy: 0.8390 - val_loss: 0.4576 - val_accuracy: 0.8080\n",
      "Epoch 392/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3257 - accuracy: 0.8742 - val_loss: 0.4577 - val_accuracy: 0.8070\n",
      "Epoch 393/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3358 - accuracy: 0.8669 - val_loss: 0.4578 - val_accuracy: 0.8070\n",
      "Epoch 394/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3606 - accuracy: 0.8390 - val_loss: 0.4569 - val_accuracy: 0.8060\n",
      "Epoch 395/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3353 - accuracy: 0.8556 - val_loss: 0.4558 - val_accuracy: 0.8050\n",
      "Epoch 396/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3539 - accuracy: 0.8327 - val_loss: 0.4557 - val_accuracy: 0.8070\n",
      "Epoch 397/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3394 - accuracy: 0.8370 - val_loss: 0.4566 - val_accuracy: 0.8060\n",
      "Epoch 398/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3765 - accuracy: 0.8172 - val_loss: 0.4575 - val_accuracy: 0.8050\n",
      "Epoch 399/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3468 - accuracy: 0.8525 - val_loss: 0.4577 - val_accuracy: 0.8060\n",
      "Epoch 400/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3419 - accuracy: 0.8400 - val_loss: 0.4586 - val_accuracy: 0.8080\n",
      "Epoch 401/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3607 - accuracy: 0.8265 - val_loss: 0.4609 - val_accuracy: 0.8070\n",
      "Epoch 402/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3364 - accuracy: 0.8433 - val_loss: 0.4631 - val_accuracy: 0.8060\n",
      "Epoch 403/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3323 - accuracy: 0.8463 - val_loss: 0.4626 - val_accuracy: 0.8070\n",
      "Epoch 404/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3502 - accuracy: 0.8308 - val_loss: 0.4617 - val_accuracy: 0.8050\n",
      "Epoch 405/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3130 - accuracy: 0.8495 - val_loss: 0.4611 - val_accuracy: 0.8050\n",
      "Epoch 406/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3398 - accuracy: 0.8391 - val_loss: 0.4604 - val_accuracy: 0.8070\n",
      "Epoch 407/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3460 - accuracy: 0.8463 - val_loss: 0.4589 - val_accuracy: 0.8080\n",
      "Epoch 408/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3340 - accuracy: 0.8400 - val_loss: 0.4580 - val_accuracy: 0.8070\n",
      "Epoch 409/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3117 - accuracy: 0.8702 - val_loss: 0.4572 - val_accuracy: 0.8060\n",
      "Epoch 410/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3272 - accuracy: 0.8433 - val_loss: 0.4572 - val_accuracy: 0.8060\n",
      "Epoch 411/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3703 - accuracy: 0.8183 - val_loss: 0.4573 - val_accuracy: 0.8070\n",
      "Epoch 412/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.2969 - accuracy: 0.8765 - val_loss: 0.4579 - val_accuracy: 0.8080\n",
      "Epoch 413/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3717 - accuracy: 0.8306 - val_loss: 0.4593 - val_accuracy: 0.8060\n",
      "Epoch 414/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3326 - accuracy: 0.8442 - val_loss: 0.4598 - val_accuracy: 0.8080\n",
      "Epoch 415/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3575 - accuracy: 0.8308 - val_loss: 0.4584 - val_accuracy: 0.8050\n",
      "Epoch 416/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3405 - accuracy: 0.8122 - val_loss: 0.4579 - val_accuracy: 0.8070\n",
      "Epoch 417/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2801 - accuracy: 0.8716 - val_loss: 0.4580 - val_accuracy: 0.8060\n",
      "Epoch 418/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3286 - accuracy: 0.8424 - val_loss: 0.4586 - val_accuracy: 0.8070\n",
      "Epoch 419/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3285 - accuracy: 0.8372 - val_loss: 0.4595 - val_accuracy: 0.8060\n",
      "Epoch 420/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3297 - accuracy: 0.8516 - val_loss: 0.4598 - val_accuracy: 0.8050\n",
      "Epoch 421/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3412 - accuracy: 0.8422 - val_loss: 0.4598 - val_accuracy: 0.8060\n",
      "Epoch 422/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3247 - accuracy: 0.8608 - val_loss: 0.4603 - val_accuracy: 0.8060\n",
      "Epoch 423/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3111 - accuracy: 0.8775 - val_loss: 0.4611 - val_accuracy: 0.8060\n",
      "Epoch 424/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3466 - accuracy: 0.8306 - val_loss: 0.4621 - val_accuracy: 0.8060\n",
      "Epoch 425/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3300 - accuracy: 0.8464 - val_loss: 0.4628 - val_accuracy: 0.8060\n",
      "Epoch 426/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3043 - accuracy: 0.8495 - val_loss: 0.4634 - val_accuracy: 0.8060\n",
      "Epoch 427/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3271 - accuracy: 0.8619 - val_loss: 0.4642 - val_accuracy: 0.8070\n",
      "Epoch 428/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3457 - accuracy: 0.8318 - val_loss: 0.4663 - val_accuracy: 0.8060\n",
      "Epoch 429/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3387 - accuracy: 0.8588 - val_loss: 0.4684 - val_accuracy: 0.8040\n",
      "Epoch 430/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3569 - accuracy: 0.8277 - val_loss: 0.4704 - val_accuracy: 0.8010\n",
      "Epoch 431/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3390 - accuracy: 0.8339 - val_loss: 0.4715 - val_accuracy: 0.8020\n",
      "Epoch 432/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3515 - accuracy: 0.8306 - val_loss: 0.4725 - val_accuracy: 0.8010\n",
      "Epoch 433/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3522 - accuracy: 0.8588 - val_loss: 0.4720 - val_accuracy: 0.8020\n",
      "Epoch 434/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3620 - accuracy: 0.8306 - val_loss: 0.4696 - val_accuracy: 0.8020\n",
      "Epoch 435/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3416 - accuracy: 0.8400 - val_loss: 0.4674 - val_accuracy: 0.8040\n",
      "Epoch 436/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.2999 - accuracy: 0.8547 - val_loss: 0.4651 - val_accuracy: 0.8050\n",
      "Epoch 437/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3213 - accuracy: 0.8608 - val_loss: 0.4629 - val_accuracy: 0.8090\n",
      "Epoch 438/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3051 - accuracy: 0.8435 - val_loss: 0.4621 - val_accuracy: 0.8060\n",
      "Epoch 439/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3377 - accuracy: 0.8454 - val_loss: 0.4619 - val_accuracy: 0.8070\n",
      "Epoch 440/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3242 - accuracy: 0.8485 - val_loss: 0.4619 - val_accuracy: 0.8070\n",
      "Epoch 441/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3464 - accuracy: 0.8277 - val_loss: 0.4619 - val_accuracy: 0.8060\n",
      "Epoch 442/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3257 - accuracy: 0.8588 - val_loss: 0.4627 - val_accuracy: 0.8050\n",
      "Epoch 443/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3347 - accuracy: 0.8516 - val_loss: 0.4643 - val_accuracy: 0.8050\n",
      "Epoch 444/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3059 - accuracy: 0.8527 - val_loss: 0.4660 - val_accuracy: 0.8040\n",
      "Epoch 445/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3213 - accuracy: 0.8391 - val_loss: 0.4690 - val_accuracy: 0.8030\n",
      "Epoch 446/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3372 - accuracy: 0.8494 - val_loss: 0.4710 - val_accuracy: 0.8030\n",
      "Epoch 447/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3447 - accuracy: 0.8400 - val_loss: 0.4714 - val_accuracy: 0.8020\n",
      "Epoch 448/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.2965 - accuracy: 0.8765 - val_loss: 0.4709 - val_accuracy: 0.8030\n",
      "Epoch 449/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3030 - accuracy: 0.8588 - val_loss: 0.4703 - val_accuracy: 0.8060\n",
      "Epoch 450/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3263 - accuracy: 0.8329 - val_loss: 0.4689 - val_accuracy: 0.8050\n",
      "Epoch 451/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3156 - accuracy: 0.8391 - val_loss: 0.4687 - val_accuracy: 0.8060\n",
      "Epoch 452/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3181 - accuracy: 0.8547 - val_loss: 0.4675 - val_accuracy: 0.8060\n",
      "Epoch 453/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3391 - accuracy: 0.8381 - val_loss: 0.4669 - val_accuracy: 0.8050\n",
      "Epoch 454/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3000 - accuracy: 0.8589 - val_loss: 0.4663 - val_accuracy: 0.8060\n",
      "Epoch 455/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2980 - accuracy: 0.8589 - val_loss: 0.4664 - val_accuracy: 0.8070\n",
      "Epoch 456/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3112 - accuracy: 0.8507 - val_loss: 0.4676 - val_accuracy: 0.8050\n",
      "Epoch 457/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2905 - accuracy: 0.8683 - val_loss: 0.4678 - val_accuracy: 0.8060\n",
      "Epoch 458/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3243 - accuracy: 0.8350 - val_loss: 0.4663 - val_accuracy: 0.8080\n",
      "Epoch 459/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3435 - accuracy: 0.8143 - val_loss: 0.4640 - val_accuracy: 0.8060\n",
      "Epoch 460/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3123 - accuracy: 0.8455 - val_loss: 0.4622 - val_accuracy: 0.8070\n",
      "Epoch 461/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3277 - accuracy: 0.8329 - val_loss: 0.4609 - val_accuracy: 0.8060\n",
      "Epoch 462/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3236 - accuracy: 0.8568 - val_loss: 0.4602 - val_accuracy: 0.8070\n",
      "Epoch 463/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3004 - accuracy: 0.8652 - val_loss: 0.4604 - val_accuracy: 0.8070\n",
      "Epoch 464/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3266 - accuracy: 0.8112 - val_loss: 0.4615 - val_accuracy: 0.8060\n",
      "Epoch 465/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3499 - accuracy: 0.8091 - val_loss: 0.4634 - val_accuracy: 0.8070\n",
      "Epoch 466/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3382 - accuracy: 0.8341 - val_loss: 0.4646 - val_accuracy: 0.8070\n",
      "Epoch 467/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3086 - accuracy: 0.8528 - val_loss: 0.4653 - val_accuracy: 0.8070\n",
      "Epoch 468/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3241 - accuracy: 0.8330 - val_loss: 0.4657 - val_accuracy: 0.8060\n",
      "Epoch 469/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3334 - accuracy: 0.8268 - val_loss: 0.4655 - val_accuracy: 0.8060\n",
      "Epoch 470/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3444 - accuracy: 0.8245 - val_loss: 0.4656 - val_accuracy: 0.8050\n",
      "Epoch 471/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3187 - accuracy: 0.8485 - val_loss: 0.4667 - val_accuracy: 0.8060\n",
      "Epoch 472/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3289 - accuracy: 0.8308 - val_loss: 0.4688 - val_accuracy: 0.8030\n",
      "Epoch 473/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3545 - accuracy: 0.8338 - val_loss: 0.4701 - val_accuracy: 0.8020\n",
      "Epoch 474/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3344 - accuracy: 0.8431 - val_loss: 0.4723 - val_accuracy: 0.8020\n",
      "Epoch 475/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3007 - accuracy: 0.8702 - val_loss: 0.4724 - val_accuracy: 0.8020\n",
      "Epoch 476/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3113 - accuracy: 0.8452 - val_loss: 0.4704 - val_accuracy: 0.8050\n",
      "Epoch 477/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3307 - accuracy: 0.8556 - val_loss: 0.4680 - val_accuracy: 0.8070\n",
      "Epoch 478/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3287 - accuracy: 0.8617 - val_loss: 0.4669 - val_accuracy: 0.8060\n",
      "Epoch 479/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2948 - accuracy: 0.8836 - val_loss: 0.4668 - val_accuracy: 0.8050\n",
      "Epoch 480/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3063 - accuracy: 0.8713 - val_loss: 0.4656 - val_accuracy: 0.8060\n",
      "Epoch 481/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3247 - accuracy: 0.8370 - val_loss: 0.4646 - val_accuracy: 0.8070\n",
      "Epoch 482/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3121 - accuracy: 0.8577 - val_loss: 0.4638 - val_accuracy: 0.8060\n",
      "Epoch 483/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3629 - accuracy: 0.8358 - val_loss: 0.4632 - val_accuracy: 0.8070\n",
      "Epoch 484/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3382 - accuracy: 0.8327 - val_loss: 0.4638 - val_accuracy: 0.8070\n",
      "Epoch 485/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3233 - accuracy: 0.8495 - val_loss: 0.4653 - val_accuracy: 0.8090\n",
      "Epoch 486/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3274 - accuracy: 0.8237 - val_loss: 0.4670 - val_accuracy: 0.8070\n",
      "Epoch 487/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3302 - accuracy: 0.8308 - val_loss: 0.4681 - val_accuracy: 0.8050\n",
      "Epoch 488/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3387 - accuracy: 0.8214 - val_loss: 0.4683 - val_accuracy: 0.8050\n",
      "Epoch 489/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3562 - accuracy: 0.8204 - val_loss: 0.4684 - val_accuracy: 0.8050\n",
      "Epoch 490/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3499 - accuracy: 0.8308 - val_loss: 0.4682 - val_accuracy: 0.8050\n",
      "Epoch 491/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.3263 - accuracy: 0.8422 - val_loss: 0.4681 - val_accuracy: 0.8080\n",
      "Epoch 492/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3314 - accuracy: 0.8527 - val_loss: 0.4677 - val_accuracy: 0.8100\n",
      "Epoch 493/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3314 - accuracy: 0.8277 - val_loss: 0.4667 - val_accuracy: 0.8090\n",
      "Epoch 494/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3157 - accuracy: 0.8454 - val_loss: 0.4656 - val_accuracy: 0.8070\n",
      "Epoch 495/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3180 - accuracy: 0.8648 - val_loss: 0.4634 - val_accuracy: 0.8070\n",
      "Epoch 496/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3152 - accuracy: 0.8680 - val_loss: 0.4619 - val_accuracy: 0.8070\n",
      "Epoch 497/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3243 - accuracy: 0.8544 - val_loss: 0.4613 - val_accuracy: 0.8090\n",
      "Epoch 498/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3310 - accuracy: 0.8555 - val_loss: 0.4618 - val_accuracy: 0.8080\n",
      "Epoch 499/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3335 - accuracy: 0.8461 - val_loss: 0.4630 - val_accuracy: 0.8070\n",
      "Epoch 500/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3217 - accuracy: 0.8471 - val_loss: 0.4639 - val_accuracy: 0.8070\n",
      "Epoch 501/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2930 - accuracy: 0.8690 - val_loss: 0.4638 - val_accuracy: 0.8070\n",
      "Epoch 502/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3168 - accuracy: 0.8547 - val_loss: 0.4629 - val_accuracy: 0.8100\n",
      "Epoch 503/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3120 - accuracy: 0.8638 - val_loss: 0.4632 - val_accuracy: 0.8070\n",
      "Epoch 504/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2963 - accuracy: 0.8867 - val_loss: 0.4646 - val_accuracy: 0.8100\n",
      "Epoch 505/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2956 - accuracy: 0.8702 - val_loss: 0.4656 - val_accuracy: 0.8070\n",
      "Epoch 506/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3152 - accuracy: 0.8638 - val_loss: 0.4668 - val_accuracy: 0.8070\n",
      "Epoch 507/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3098 - accuracy: 0.8650 - val_loss: 0.4667 - val_accuracy: 0.8080\n",
      "Epoch 508/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3181 - accuracy: 0.8546 - val_loss: 0.4654 - val_accuracy: 0.8070\n",
      "Epoch 509/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3129 - accuracy: 0.8617 - val_loss: 0.4638 - val_accuracy: 0.8090\n",
      "Epoch 510/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3489 - accuracy: 0.8450 - val_loss: 0.4626 - val_accuracy: 0.8080\n",
      "Epoch 511/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3633 - accuracy: 0.8047 - val_loss: 0.4619 - val_accuracy: 0.8100\n",
      "Epoch 512/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3445 - accuracy: 0.8277 - val_loss: 0.4617 - val_accuracy: 0.8120\n",
      "Epoch 513/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3532 - accuracy: 0.8060 - val_loss: 0.4615 - val_accuracy: 0.8120\n",
      "Epoch 514/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3190 - accuracy: 0.8403 - val_loss: 0.4615 - val_accuracy: 0.8110\n",
      "Epoch 515/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3343 - accuracy: 0.8091 - val_loss: 0.4616 - val_accuracy: 0.8120\n",
      "Epoch 516/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3331 - accuracy: 0.8060 - val_loss: 0.4628 - val_accuracy: 0.8120\n",
      "Epoch 517/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3643 - accuracy: 0.7997 - val_loss: 0.4635 - val_accuracy: 0.8120\n",
      "Epoch 518/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3565 - accuracy: 0.8018 - val_loss: 0.4642 - val_accuracy: 0.8110\n",
      "Epoch 519/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3432 - accuracy: 0.8091 - val_loss: 0.4660 - val_accuracy: 0.8120\n",
      "Epoch 520/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3468 - accuracy: 0.8235 - val_loss: 0.4659 - val_accuracy: 0.8130\n",
      "Epoch 521/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3584 - accuracy: 0.8058 - val_loss: 0.4649 - val_accuracy: 0.8110\n",
      "Epoch 522/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3347 - accuracy: 0.8318 - val_loss: 0.4652 - val_accuracy: 0.8090\n",
      "Epoch 523/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3457 - accuracy: 0.8091 - val_loss: 0.4637 - val_accuracy: 0.8090\n",
      "Epoch 524/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2948 - accuracy: 0.8464 - val_loss: 0.4626 - val_accuracy: 0.8080\n",
      "Epoch 525/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3325 - accuracy: 0.8338 - val_loss: 0.4623 - val_accuracy: 0.8080\n",
      "Epoch 526/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3404 - accuracy: 0.8482 - val_loss: 0.4613 - val_accuracy: 0.8100\n",
      "Epoch 527/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3462 - accuracy: 0.8398 - val_loss: 0.4609 - val_accuracy: 0.8100\n",
      "Epoch 528/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3200 - accuracy: 0.8669 - val_loss: 0.4606 - val_accuracy: 0.8100\n",
      "Epoch 529/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3564 - accuracy: 0.8232 - val_loss: 0.4609 - val_accuracy: 0.8120\n",
      "Epoch 530/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2904 - accuracy: 0.8878 - val_loss: 0.4599 - val_accuracy: 0.8130\n",
      "Epoch 531/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3188 - accuracy: 0.8275 - val_loss: 0.4593 - val_accuracy: 0.8140\n",
      "Epoch 532/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3467 - accuracy: 0.8273 - val_loss: 0.4603 - val_accuracy: 0.8140\n",
      "Epoch 533/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3132 - accuracy: 0.8857 - val_loss: 0.4604 - val_accuracy: 0.8130\n",
      "Epoch 534/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3057 - accuracy: 0.8648 - val_loss: 0.4600 - val_accuracy: 0.8150\n",
      "Epoch 535/1000\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3417 - accuracy: 0.8492 - val_loss: 0.4590 - val_accuracy: 0.8140\n",
      "Epoch 536/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3284 - accuracy: 0.8523 - val_loss: 0.4585 - val_accuracy: 0.8140\n",
      "Epoch 537/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3567 - accuracy: 0.8430 - val_loss: 0.4592 - val_accuracy: 0.8140\n",
      "Epoch 538/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3098 - accuracy: 0.8555 - val_loss: 0.4608 - val_accuracy: 0.8120\n",
      "Epoch 539/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3056 - accuracy: 0.8763 - val_loss: 0.4627 - val_accuracy: 0.8100\n",
      "Epoch 540/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3187 - accuracy: 0.8463 - val_loss: 0.4614 - val_accuracy: 0.8120\n",
      "Epoch 541/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3244 - accuracy: 0.8483 - val_loss: 0.4608 - val_accuracy: 0.8120\n",
      "Epoch 542/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3242 - accuracy: 0.8525 - val_loss: 0.4605 - val_accuracy: 0.8120\n",
      "Epoch 543/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3262 - accuracy: 0.8525 - val_loss: 0.4605 - val_accuracy: 0.8110\n",
      "Epoch 544/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3252 - accuracy: 0.8494 - val_loss: 0.4605 - val_accuracy: 0.8110\n",
      "Epoch 545/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3021 - accuracy: 0.8640 - val_loss: 0.4600 - val_accuracy: 0.8100\n",
      "Epoch 546/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3016 - accuracy: 0.8711 - val_loss: 0.4596 - val_accuracy: 0.8120\n",
      "Epoch 547/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3117 - accuracy: 0.8648 - val_loss: 0.4594 - val_accuracy: 0.8130\n",
      "Epoch 548/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2996 - accuracy: 0.8794 - val_loss: 0.4592 - val_accuracy: 0.8120\n",
      "Epoch 549/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2736 - accuracy: 0.8909 - val_loss: 0.4592 - val_accuracy: 0.8120\n",
      "Epoch 550/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3208 - accuracy: 0.8638 - val_loss: 0.4593 - val_accuracy: 0.8100\n",
      "Epoch 551/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3094 - accuracy: 0.8700 - val_loss: 0.4590 - val_accuracy: 0.8110\n",
      "Epoch 552/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3206 - accuracy: 0.8669 - val_loss: 0.4592 - val_accuracy: 0.8110\n",
      "Epoch 553/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3501 - accuracy: 0.8388 - val_loss: 0.4607 - val_accuracy: 0.8110\n",
      "Epoch 554/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3055 - accuracy: 0.8650 - val_loss: 0.4625 - val_accuracy: 0.8110\n",
      "Epoch 555/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3007 - accuracy: 0.8857 - val_loss: 0.4642 - val_accuracy: 0.8100\n",
      "Epoch 556/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3380 - accuracy: 0.8523 - val_loss: 0.4632 - val_accuracy: 0.8110\n",
      "Epoch 557/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3212 - accuracy: 0.8598 - val_loss: 0.4628 - val_accuracy: 0.8120\n",
      "Epoch 558/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3245 - accuracy: 0.8640 - val_loss: 0.4638 - val_accuracy: 0.8110\n",
      "Epoch 559/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3137 - accuracy: 0.8544 - val_loss: 0.4639 - val_accuracy: 0.8120\n",
      "Epoch 560/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2993 - accuracy: 0.8671 - val_loss: 0.4625 - val_accuracy: 0.8130\n",
      "Epoch 561/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3072 - accuracy: 0.8577 - val_loss: 0.4605 - val_accuracy: 0.8190\n",
      "Epoch 562/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3038 - accuracy: 0.8669 - val_loss: 0.4607 - val_accuracy: 0.8150\n",
      "Epoch 563/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3177 - accuracy: 0.8607 - val_loss: 0.4609 - val_accuracy: 0.8130\n",
      "Epoch 564/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3350 - accuracy: 0.8513 - val_loss: 0.4613 - val_accuracy: 0.8130\n",
      "Epoch 565/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2930 - accuracy: 0.8732 - val_loss: 0.4617 - val_accuracy: 0.8140\n",
      "Epoch 566/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3136 - accuracy: 0.8588 - val_loss: 0.4624 - val_accuracy: 0.8130\n",
      "Epoch 567/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3144 - accuracy: 0.8680 - val_loss: 0.4628 - val_accuracy: 0.8140\n",
      "Epoch 568/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3086 - accuracy: 0.8669 - val_loss: 0.4635 - val_accuracy: 0.8140\n",
      "Epoch 569/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3220 - accuracy: 0.8586 - val_loss: 0.4641 - val_accuracy: 0.8160\n",
      "Epoch 570/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3051 - accuracy: 0.8700 - val_loss: 0.4652 - val_accuracy: 0.8140\n",
      "Epoch 571/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3094 - accuracy: 0.8711 - val_loss: 0.4660 - val_accuracy: 0.8120\n",
      "Epoch 572/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2937 - accuracy: 0.8692 - val_loss: 0.4663 - val_accuracy: 0.8140\n",
      "Epoch 573/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3176 - accuracy: 0.8463 - val_loss: 0.4660 - val_accuracy: 0.8120\n",
      "Epoch 574/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3477 - accuracy: 0.8402 - val_loss: 0.4658 - val_accuracy: 0.8120\n",
      "Epoch 575/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3264 - accuracy: 0.8360 - val_loss: 0.4654 - val_accuracy: 0.8140\n",
      "Epoch 576/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3502 - accuracy: 0.8153 - val_loss: 0.4639 - val_accuracy: 0.8150\n",
      "Epoch 577/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3102 - accuracy: 0.8494 - val_loss: 0.4631 - val_accuracy: 0.8150\n",
      "Epoch 578/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3271 - accuracy: 0.8430 - val_loss: 0.4630 - val_accuracy: 0.8150\n",
      "Epoch 579/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3004 - accuracy: 0.8711 - val_loss: 0.4640 - val_accuracy: 0.8130\n",
      "Epoch 580/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3272 - accuracy: 0.8492 - val_loss: 0.4651 - val_accuracy: 0.8130\n",
      "Epoch 581/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3230 - accuracy: 0.8650 - val_loss: 0.4657 - val_accuracy: 0.8110\n",
      "Epoch 582/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2928 - accuracy: 0.8671 - val_loss: 0.4668 - val_accuracy: 0.8100\n",
      "Epoch 583/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3553 - accuracy: 0.8242 - val_loss: 0.4675 - val_accuracy: 0.8090\n",
      "Epoch 584/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3094 - accuracy: 0.8617 - val_loss: 0.4693 - val_accuracy: 0.8060\n",
      "Epoch 585/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3282 - accuracy: 0.8515 - val_loss: 0.4703 - val_accuracy: 0.8060\n",
      "Epoch 586/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3056 - accuracy: 0.8785 - val_loss: 0.4700 - val_accuracy: 0.8060\n",
      "Epoch 587/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2951 - accuracy: 0.8765 - val_loss: 0.4694 - val_accuracy: 0.8060\n",
      "Epoch 588/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2903 - accuracy: 0.8650 - val_loss: 0.4689 - val_accuracy: 0.8060\n",
      "Epoch 589/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3296 - accuracy: 0.8494 - val_loss: 0.4692 - val_accuracy: 0.8090\n",
      "Epoch 590/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2905 - accuracy: 0.8733 - val_loss: 0.4696 - val_accuracy: 0.8120\n",
      "Epoch 591/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3062 - accuracy: 0.8589 - val_loss: 0.4704 - val_accuracy: 0.8120\n",
      "Epoch 592/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3304 - accuracy: 0.8308 - val_loss: 0.4713 - val_accuracy: 0.8080\n",
      "Epoch 593/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3473 - accuracy: 0.8079 - val_loss: 0.4721 - val_accuracy: 0.8080\n",
      "Epoch 594/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2734 - accuracy: 0.8683 - val_loss: 0.4721 - val_accuracy: 0.8080\n",
      "Epoch 595/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3230 - accuracy: 0.8454 - val_loss: 0.4714 - val_accuracy: 0.8080\n",
      "Epoch 596/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3281 - accuracy: 0.8339 - val_loss: 0.4705 - val_accuracy: 0.8080\n",
      "Epoch 597/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3181 - accuracy: 0.8268 - val_loss: 0.4691 - val_accuracy: 0.8090\n",
      "Epoch 598/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3045 - accuracy: 0.8422 - val_loss: 0.4679 - val_accuracy: 0.8110\n",
      "Epoch 599/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3081 - accuracy: 0.8403 - val_loss: 0.4677 - val_accuracy: 0.8130\n",
      "Epoch 600/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3178 - accuracy: 0.8391 - val_loss: 0.4672 - val_accuracy: 0.8090\n",
      "Epoch 601/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3202 - accuracy: 0.8506 - val_loss: 0.4669 - val_accuracy: 0.8100\n",
      "Epoch 602/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3320 - accuracy: 0.8329 - val_loss: 0.4677 - val_accuracy: 0.8050\n",
      "Epoch 603/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3290 - accuracy: 0.8390 - val_loss: 0.4688 - val_accuracy: 0.8060\n",
      "Epoch 604/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3131 - accuracy: 0.8523 - val_loss: 0.4694 - val_accuracy: 0.8090\n",
      "Epoch 605/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3149 - accuracy: 0.8607 - val_loss: 0.4688 - val_accuracy: 0.8100\n",
      "Epoch 606/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3084 - accuracy: 0.8369 - val_loss: 0.4669 - val_accuracy: 0.8080\n",
      "Epoch 607/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3045 - accuracy: 0.8588 - val_loss: 0.4656 - val_accuracy: 0.8080\n",
      "Epoch 608/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3237 - accuracy: 0.8483 - val_loss: 0.4647 - val_accuracy: 0.8110\n",
      "Epoch 609/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3466 - accuracy: 0.8306 - val_loss: 0.4639 - val_accuracy: 0.8110\n",
      "Epoch 610/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3090 - accuracy: 0.8619 - val_loss: 0.4646 - val_accuracy: 0.8100\n",
      "Epoch 611/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2980 - accuracy: 0.8640 - val_loss: 0.4646 - val_accuracy: 0.8110\n",
      "Epoch 612/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3261 - accuracy: 0.8430 - val_loss: 0.4634 - val_accuracy: 0.8120\n",
      "Epoch 613/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3469 - accuracy: 0.8430 - val_loss: 0.4631 - val_accuracy: 0.8140\n",
      "Epoch 614/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3051 - accuracy: 0.8638 - val_loss: 0.4637 - val_accuracy: 0.8140\n",
      "Epoch 615/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3166 - accuracy: 0.8638 - val_loss: 0.4641 - val_accuracy: 0.8130\n",
      "Epoch 616/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3037 - accuracy: 0.8638 - val_loss: 0.4642 - val_accuracy: 0.8120\n",
      "Epoch 617/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3050 - accuracy: 0.8648 - val_loss: 0.4642 - val_accuracy: 0.8130\n",
      "Epoch 618/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3374 - accuracy: 0.8242 - val_loss: 0.4645 - val_accuracy: 0.8120\n",
      "Epoch 619/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3211 - accuracy: 0.8523 - val_loss: 0.4648 - val_accuracy: 0.8140\n",
      "Epoch 620/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3046 - accuracy: 0.8400 - val_loss: 0.4652 - val_accuracy: 0.8140\n",
      "Epoch 621/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2980 - accuracy: 0.8617 - val_loss: 0.4657 - val_accuracy: 0.8140\n",
      "Epoch 622/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2932 - accuracy: 0.8825 - val_loss: 0.4667 - val_accuracy: 0.8110\n",
      "Epoch 623/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3057 - accuracy: 0.8680 - val_loss: 0.4681 - val_accuracy: 0.8090\n",
      "Epoch 624/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3459 - accuracy: 0.8119 - val_loss: 0.4688 - val_accuracy: 0.8080\n",
      "Epoch 625/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3026 - accuracy: 0.8494 - val_loss: 0.4696 - val_accuracy: 0.8080\n",
      "Epoch 626/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2918 - accuracy: 0.8588 - val_loss: 0.4696 - val_accuracy: 0.8080\n",
      "Epoch 627/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3235 - accuracy: 0.8546 - val_loss: 0.4698 - val_accuracy: 0.8090\n",
      "Epoch 628/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3561 - accuracy: 0.8119 - val_loss: 0.4697 - val_accuracy: 0.8100\n",
      "Epoch 629/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3069 - accuracy: 0.8577 - val_loss: 0.4701 - val_accuracy: 0.8100\n",
      "Epoch 630/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3288 - accuracy: 0.8254 - val_loss: 0.4704 - val_accuracy: 0.8110\n",
      "Epoch 631/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3262 - accuracy: 0.8339 - val_loss: 0.4714 - val_accuracy: 0.8090\n",
      "Epoch 632/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.2768 - accuracy: 0.8641 - val_loss: 0.4722 - val_accuracy: 0.8080\n",
      "Epoch 633/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3247 - accuracy: 0.8523 - val_loss: 0.4738 - val_accuracy: 0.8090\n",
      "Epoch 634/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3467 - accuracy: 0.8242 - val_loss: 0.4763 - val_accuracy: 0.8090\n",
      "Epoch 635/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3282 - accuracy: 0.8638 - val_loss: 0.4766 - val_accuracy: 0.8080\n",
      "Epoch 636/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3362 - accuracy: 0.8358 - val_loss: 0.4770 - val_accuracy: 0.8080\n",
      "Epoch 637/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2893 - accuracy: 0.8785 - val_loss: 0.4771 - val_accuracy: 0.8080\n",
      "Epoch 638/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3332 - accuracy: 0.8327 - val_loss: 0.4759 - val_accuracy: 0.8060\n",
      "Epoch 639/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3254 - accuracy: 0.8400 - val_loss: 0.4761 - val_accuracy: 0.8070\n",
      "Epoch 640/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3333 - accuracy: 0.8369 - val_loss: 0.4774 - val_accuracy: 0.8080\n",
      "Epoch 641/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.2871 - accuracy: 0.8516 - val_loss: 0.4797 - val_accuracy: 0.8070\n",
      "Epoch 642/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3030 - accuracy: 0.8422 - val_loss: 0.4800 - val_accuracy: 0.8050\n",
      "Epoch 643/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3275 - accuracy: 0.8329 - val_loss: 0.4789 - val_accuracy: 0.8070\n",
      "Epoch 644/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3172 - accuracy: 0.8390 - val_loss: 0.4768 - val_accuracy: 0.8060\n",
      "Epoch 645/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3065 - accuracy: 0.8544 - val_loss: 0.4746 - val_accuracy: 0.8120\n",
      "Epoch 646/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3325 - accuracy: 0.8305 - val_loss: 0.4727 - val_accuracy: 0.8110\n",
      "Epoch 647/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3275 - accuracy: 0.8358 - val_loss: 0.4718 - val_accuracy: 0.8120\n",
      "Epoch 648/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3093 - accuracy: 0.8485 - val_loss: 0.4716 - val_accuracy: 0.8120\n",
      "Epoch 649/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3338 - accuracy: 0.8204 - val_loss: 0.4712 - val_accuracy: 0.8100\n",
      "Epoch 650/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3080 - accuracy: 0.8525 - val_loss: 0.4701 - val_accuracy: 0.8100\n",
      "Epoch 651/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3501 - accuracy: 0.8306 - val_loss: 0.4694 - val_accuracy: 0.8090\n",
      "Epoch 652/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3139 - accuracy: 0.8277 - val_loss: 0.4694 - val_accuracy: 0.8090\n",
      "Epoch 653/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3457 - accuracy: 0.8140 - val_loss: 0.4697 - val_accuracy: 0.8100\n",
      "Epoch 654/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3079 - accuracy: 0.8525 - val_loss: 0.4697 - val_accuracy: 0.8130\n",
      "Epoch 655/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3384 - accuracy: 0.8544 - val_loss: 0.4698 - val_accuracy: 0.8130\n",
      "Epoch 656/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2992 - accuracy: 0.8669 - val_loss: 0.4699 - val_accuracy: 0.8130\n",
      "Epoch 657/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2959 - accuracy: 0.8607 - val_loss: 0.4703 - val_accuracy: 0.8080\n",
      "Epoch 658/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3061 - accuracy: 0.8555 - val_loss: 0.4732 - val_accuracy: 0.8100\n",
      "Epoch 659/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3091 - accuracy: 0.8678 - val_loss: 0.4760 - val_accuracy: 0.8110\n",
      "Epoch 660/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2990 - accuracy: 0.8699 - val_loss: 0.4784 - val_accuracy: 0.8100\n",
      "Epoch 661/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3368 - accuracy: 0.8607 - val_loss: 0.4796 - val_accuracy: 0.8080\n",
      "Epoch 662/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3055 - accuracy: 0.8546 - val_loss: 0.4774 - val_accuracy: 0.8080\n",
      "Epoch 663/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3164 - accuracy: 0.8565 - val_loss: 0.4750 - val_accuracy: 0.8080\n",
      "Epoch 664/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3144 - accuracy: 0.8421 - val_loss: 0.4726 - val_accuracy: 0.8080\n",
      "Epoch 665/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3250 - accuracy: 0.8360 - val_loss: 0.4734 - val_accuracy: 0.8080\n",
      "Epoch 666/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2839 - accuracy: 0.8796 - val_loss: 0.4740 - val_accuracy: 0.8080\n",
      "Epoch 667/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3435 - accuracy: 0.8171 - val_loss: 0.4715 - val_accuracy: 0.8090\n",
      "Epoch 668/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3156 - accuracy: 0.8185 - val_loss: 0.4699 - val_accuracy: 0.8110\n",
      "Epoch 669/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3244 - accuracy: 0.8183 - val_loss: 0.4696 - val_accuracy: 0.8110\n",
      "Epoch 670/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2872 - accuracy: 0.8672 - val_loss: 0.4698 - val_accuracy: 0.8130\n",
      "Epoch 671/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3509 - accuracy: 0.8213 - val_loss: 0.4699 - val_accuracy: 0.8110\n",
      "Epoch 672/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3148 - accuracy: 0.8327 - val_loss: 0.4704 - val_accuracy: 0.8100\n",
      "Epoch 673/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3126 - accuracy: 0.8442 - val_loss: 0.4702 - val_accuracy: 0.8100\n",
      "Epoch 674/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3428 - accuracy: 0.8181 - val_loss: 0.4695 - val_accuracy: 0.8090\n",
      "Epoch 675/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3273 - accuracy: 0.8358 - val_loss: 0.4693 - val_accuracy: 0.8120\n",
      "Epoch 676/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3078 - accuracy: 0.8555 - val_loss: 0.4697 - val_accuracy: 0.8160\n",
      "Epoch 677/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3167 - accuracy: 0.8544 - val_loss: 0.4705 - val_accuracy: 0.8150\n",
      "Epoch 678/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3168 - accuracy: 0.8586 - val_loss: 0.4713 - val_accuracy: 0.8170\n",
      "Epoch 679/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3058 - accuracy: 0.8523 - val_loss: 0.4719 - val_accuracy: 0.8160\n",
      "Epoch 680/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3294 - accuracy: 0.8398 - val_loss: 0.4719 - val_accuracy: 0.8160\n",
      "Epoch 681/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3206 - accuracy: 0.8523 - val_loss: 0.4722 - val_accuracy: 0.8140\n",
      "Epoch 682/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3164 - accuracy: 0.8452 - val_loss: 0.4725 - val_accuracy: 0.8140\n",
      "Epoch 683/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2897 - accuracy: 0.8608 - val_loss: 0.4730 - val_accuracy: 0.8110\n",
      "Epoch 684/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3590 - accuracy: 0.8296 - val_loss: 0.4743 - val_accuracy: 0.8090\n",
      "Epoch 685/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2887 - accuracy: 0.8773 - val_loss: 0.4768 - val_accuracy: 0.8060\n",
      "Epoch 686/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3300 - accuracy: 0.8296 - val_loss: 0.4790 - val_accuracy: 0.8090\n",
      "Epoch 687/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3339 - accuracy: 0.8575 - val_loss: 0.4797 - val_accuracy: 0.8100\n",
      "Epoch 688/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2937 - accuracy: 0.8794 - val_loss: 0.4769 - val_accuracy: 0.8090\n",
      "Epoch 689/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2975 - accuracy: 0.8619 - val_loss: 0.4727 - val_accuracy: 0.8120\n",
      "Epoch 690/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3063 - accuracy: 0.8515 - val_loss: 0.4711 - val_accuracy: 0.8100\n",
      "Epoch 691/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3274 - accuracy: 0.8245 - val_loss: 0.4711 - val_accuracy: 0.8110\n",
      "Epoch 692/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2852 - accuracy: 0.8485 - val_loss: 0.4715 - val_accuracy: 0.8100\n",
      "Epoch 693/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3168 - accuracy: 0.8433 - val_loss: 0.4723 - val_accuracy: 0.8120\n",
      "Epoch 694/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3274 - accuracy: 0.8277 - val_loss: 0.4728 - val_accuracy: 0.8120\n",
      "Epoch 695/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3298 - accuracy: 0.8204 - val_loss: 0.4734 - val_accuracy: 0.8110\n",
      "Epoch 696/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3293 - accuracy: 0.8235 - val_loss: 0.4731 - val_accuracy: 0.8140\n",
      "Epoch 697/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2863 - accuracy: 0.8662 - val_loss: 0.4720 - val_accuracy: 0.8120\n",
      "Epoch 698/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2895 - accuracy: 0.8485 - val_loss: 0.4715 - val_accuracy: 0.8090\n",
      "Epoch 699/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3055 - accuracy: 0.8485 - val_loss: 0.4709 - val_accuracy: 0.8110\n",
      "Epoch 700/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2492 - accuracy: 0.8994 - val_loss: 0.4713 - val_accuracy: 0.8100\n",
      "Epoch 701/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2960 - accuracy: 0.8608 - val_loss: 0.4716 - val_accuracy: 0.8090\n",
      "Epoch 702/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3193 - accuracy: 0.8367 - val_loss: 0.4734 - val_accuracy: 0.8100\n",
      "Epoch 703/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2862 - accuracy: 0.8464 - val_loss: 0.4764 - val_accuracy: 0.8080\n",
      "Epoch 704/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2878 - accuracy: 0.8690 - val_loss: 0.4795 - val_accuracy: 0.8090\n",
      "Epoch 705/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3281 - accuracy: 0.8617 - val_loss: 0.4802 - val_accuracy: 0.8060\n",
      "Epoch 706/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2939 - accuracy: 0.8740 - val_loss: 0.4805 - val_accuracy: 0.8060\n",
      "Epoch 707/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.2803 - accuracy: 0.8865 - val_loss: 0.4803 - val_accuracy: 0.8070\n",
      "Epoch 708/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2848 - accuracy: 0.8897 - val_loss: 0.4799 - val_accuracy: 0.8070\n",
      "Epoch 709/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3091 - accuracy: 0.8793 - val_loss: 0.4786 - val_accuracy: 0.8070\n",
      "Epoch 710/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3086 - accuracy: 0.8709 - val_loss: 0.4782 - val_accuracy: 0.8050\n",
      "Epoch 711/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3245 - accuracy: 0.8490 - val_loss: 0.4785 - val_accuracy: 0.8060\n",
      "Epoch 712/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3044 - accuracy: 0.8588 - val_loss: 0.4785 - val_accuracy: 0.8080\n",
      "Epoch 713/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3109 - accuracy: 0.8266 - val_loss: 0.4781 - val_accuracy: 0.8080\n",
      "Epoch 714/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3062 - accuracy: 0.8485 - val_loss: 0.4777 - val_accuracy: 0.8090\n",
      "Epoch 715/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3154 - accuracy: 0.8172 - val_loss: 0.4770 - val_accuracy: 0.8080\n",
      "Epoch 716/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3077 - accuracy: 0.8555 - val_loss: 0.4764 - val_accuracy: 0.8090\n",
      "Epoch 717/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3042 - accuracy: 0.8586 - val_loss: 0.4760 - val_accuracy: 0.8090\n",
      "Epoch 718/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2942 - accuracy: 0.8617 - val_loss: 0.4763 - val_accuracy: 0.8100\n",
      "Epoch 719/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2948 - accuracy: 0.8700 - val_loss: 0.4763 - val_accuracy: 0.8100\n",
      "Epoch 720/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2944 - accuracy: 0.8617 - val_loss: 0.4757 - val_accuracy: 0.8110\n",
      "Epoch 721/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3397 - accuracy: 0.8450 - val_loss: 0.4755 - val_accuracy: 0.8110\n",
      "Epoch 722/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3316 - accuracy: 0.8367 - val_loss: 0.4770 - val_accuracy: 0.8100\n",
      "Epoch 723/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3165 - accuracy: 0.8369 - val_loss: 0.4771 - val_accuracy: 0.8110\n",
      "Epoch 724/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3048 - accuracy: 0.8638 - val_loss: 0.4768 - val_accuracy: 0.8110\n",
      "Epoch 725/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3226 - accuracy: 0.8544 - val_loss: 0.4761 - val_accuracy: 0.8120\n",
      "Epoch 726/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2946 - accuracy: 0.8713 - val_loss: 0.4759 - val_accuracy: 0.8120\n",
      "Epoch 727/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3433 - accuracy: 0.8150 - val_loss: 0.4762 - val_accuracy: 0.8120\n",
      "Epoch 728/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3142 - accuracy: 0.8567 - val_loss: 0.4764 - val_accuracy: 0.8120\n",
      "Epoch 729/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3316 - accuracy: 0.8431 - val_loss: 0.4769 - val_accuracy: 0.8110\n",
      "Epoch 730/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3165 - accuracy: 0.8452 - val_loss: 0.4774 - val_accuracy: 0.8110\n",
      "Epoch 731/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3020 - accuracy: 0.8598 - val_loss: 0.4773 - val_accuracy: 0.8110\n",
      "Epoch 732/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2957 - accuracy: 0.8742 - val_loss: 0.4770 - val_accuracy: 0.8110\n",
      "Epoch 733/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3253 - accuracy: 0.8273 - val_loss: 0.4769 - val_accuracy: 0.8110\n",
      "Epoch 734/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3085 - accuracy: 0.8690 - val_loss: 0.4779 - val_accuracy: 0.8110\n",
      "Epoch 735/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2906 - accuracy: 0.8546 - val_loss: 0.4795 - val_accuracy: 0.8100\n",
      "Epoch 736/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3072 - accuracy: 0.8638 - val_loss: 0.4789 - val_accuracy: 0.8100\n",
      "Epoch 737/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3253 - accuracy: 0.8398 - val_loss: 0.4778 - val_accuracy: 0.8120\n",
      "Epoch 738/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3341 - accuracy: 0.8398 - val_loss: 0.4772 - val_accuracy: 0.8120\n",
      "Epoch 739/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3180 - accuracy: 0.8575 - val_loss: 0.4771 - val_accuracy: 0.8140\n",
      "Epoch 740/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2947 - accuracy: 0.8732 - val_loss: 0.4769 - val_accuracy: 0.8150\n",
      "Epoch 741/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.2888 - accuracy: 0.8648 - val_loss: 0.4769 - val_accuracy: 0.8170\n",
      "Epoch 742/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3111 - accuracy: 0.8586 - val_loss: 0.4772 - val_accuracy: 0.8160\n",
      "Epoch 743/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3085 - accuracy: 0.8690 - val_loss: 0.4772 - val_accuracy: 0.8160\n",
      "Epoch 744/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3000 - accuracy: 0.8669 - val_loss: 0.4768 - val_accuracy: 0.8160\n",
      "Epoch 745/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2943 - accuracy: 0.8765 - val_loss: 0.4772 - val_accuracy: 0.8160\n",
      "Epoch 746/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2768 - accuracy: 0.8680 - val_loss: 0.4788 - val_accuracy: 0.8120\n",
      "Epoch 747/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3036 - accuracy: 0.8824 - val_loss: 0.4787 - val_accuracy: 0.8120\n",
      "Epoch 748/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.2789 - accuracy: 0.8711 - val_loss: 0.4784 - val_accuracy: 0.8110\n",
      "Epoch 749/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2960 - accuracy: 0.8588 - val_loss: 0.4779 - val_accuracy: 0.8110\n",
      "Epoch 750/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3104 - accuracy: 0.8369 - val_loss: 0.4778 - val_accuracy: 0.8110\n",
      "Epoch 751/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2875 - accuracy: 0.8546 - val_loss: 0.4773 - val_accuracy: 0.8120\n",
      "Epoch 752/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3314 - accuracy: 0.8492 - val_loss: 0.4769 - val_accuracy: 0.8100\n",
      "Epoch 753/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2930 - accuracy: 0.8669 - val_loss: 0.4772 - val_accuracy: 0.8110\n",
      "Epoch 754/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3340 - accuracy: 0.8605 - val_loss: 0.4773 - val_accuracy: 0.8160\n",
      "Epoch 755/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2907 - accuracy: 0.8575 - val_loss: 0.4775 - val_accuracy: 0.8170\n",
      "Epoch 756/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3023 - accuracy: 0.8680 - val_loss: 0.4766 - val_accuracy: 0.8170\n",
      "Epoch 757/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3252 - accuracy: 0.8305 - val_loss: 0.4762 - val_accuracy: 0.8180\n",
      "Epoch 758/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3122 - accuracy: 0.8419 - val_loss: 0.4765 - val_accuracy: 0.8180\n",
      "Epoch 759/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.2835 - accuracy: 0.8638 - val_loss: 0.4783 - val_accuracy: 0.8150\n",
      "Epoch 760/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3254 - accuracy: 0.8523 - val_loss: 0.4800 - val_accuracy: 0.8130\n",
      "Epoch 761/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2840 - accuracy: 0.8817 - val_loss: 0.4811 - val_accuracy: 0.8130\n",
      "Epoch 762/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2933 - accuracy: 0.8586 - val_loss: 0.4838 - val_accuracy: 0.8100\n",
      "Epoch 763/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3369 - accuracy: 0.8522 - val_loss: 0.4846 - val_accuracy: 0.8100\n",
      "Epoch 764/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3088 - accuracy: 0.8553 - val_loss: 0.4857 - val_accuracy: 0.8100\n",
      "Epoch 765/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3057 - accuracy: 0.8897 - val_loss: 0.4853 - val_accuracy: 0.8120\n",
      "Epoch 766/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3095 - accuracy: 0.8772 - val_loss: 0.4842 - val_accuracy: 0.8110\n",
      "Epoch 767/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3076 - accuracy: 0.8793 - val_loss: 0.4822 - val_accuracy: 0.8120\n",
      "Epoch 768/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3316 - accuracy: 0.8398 - val_loss: 0.4792 - val_accuracy: 0.8130\n",
      "Epoch 769/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3203 - accuracy: 0.8398 - val_loss: 0.4776 - val_accuracy: 0.8130\n",
      "Epoch 770/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2922 - accuracy: 0.8803 - val_loss: 0.4774 - val_accuracy: 0.8130\n",
      "Epoch 771/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2881 - accuracy: 0.8732 - val_loss: 0.4783 - val_accuracy: 0.8140\n",
      "Epoch 772/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2962 - accuracy: 0.8700 - val_loss: 0.4800 - val_accuracy: 0.8140\n",
      "Epoch 773/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3045 - accuracy: 0.8461 - val_loss: 0.4816 - val_accuracy: 0.8150\n",
      "Epoch 774/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2951 - accuracy: 0.8555 - val_loss: 0.4827 - val_accuracy: 0.8130\n",
      "Epoch 775/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3067 - accuracy: 0.8732 - val_loss: 0.4829 - val_accuracy: 0.8120\n",
      "Epoch 776/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3034 - accuracy: 0.8742 - val_loss: 0.4828 - val_accuracy: 0.8130\n",
      "Epoch 777/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2877 - accuracy: 0.8680 - val_loss: 0.4834 - val_accuracy: 0.8120\n",
      "Epoch 778/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2867 - accuracy: 0.8794 - val_loss: 0.4844 - val_accuracy: 0.8110\n",
      "Epoch 779/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.2836 - accuracy: 0.8671 - val_loss: 0.4847 - val_accuracy: 0.8100\n",
      "Epoch 780/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3311 - accuracy: 0.8369 - val_loss: 0.4845 - val_accuracy: 0.8110\n",
      "Epoch 781/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3055 - accuracy: 0.8410 - val_loss: 0.4836 - val_accuracy: 0.8120\n",
      "Epoch 782/1000\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.3157 - accuracy: 0.8367 - val_loss: 0.4798 - val_accuracy: 0.8160\n",
      "Epoch 783/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3039 - accuracy: 0.8638 - val_loss: 0.4783 - val_accuracy: 0.8170\n",
      "Epoch 784/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3104 - accuracy: 0.8638 - val_loss: 0.4779 - val_accuracy: 0.8170\n",
      "Epoch 785/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3109 - accuracy: 0.8419 - val_loss: 0.4776 - val_accuracy: 0.8170\n",
      "Epoch 786/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2876 - accuracy: 0.8846 - val_loss: 0.4777 - val_accuracy: 0.8140\n",
      "Epoch 787/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2963 - accuracy: 0.8546 - val_loss: 0.4785 - val_accuracy: 0.8130\n",
      "Epoch 788/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.2892 - accuracy: 0.8492 - val_loss: 0.4803 - val_accuracy: 0.8100\n",
      "Epoch 789/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3352 - accuracy: 0.8419 - val_loss: 0.4814 - val_accuracy: 0.8120\n",
      "Epoch 790/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3385 - accuracy: 0.8273 - val_loss: 0.4819 - val_accuracy: 0.8120\n",
      "Epoch 791/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3137 - accuracy: 0.8461 - val_loss: 0.4818 - val_accuracy: 0.8130\n",
      "Epoch 792/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2950 - accuracy: 0.8523 - val_loss: 0.4812 - val_accuracy: 0.8130\n",
      "Epoch 793/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2985 - accuracy: 0.8680 - val_loss: 0.4807 - val_accuracy: 0.8130\n",
      "Epoch 794/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3029 - accuracy: 0.8482 - val_loss: 0.4811 - val_accuracy: 0.8130\n",
      "Epoch 795/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3040 - accuracy: 0.8452 - val_loss: 0.4807 - val_accuracy: 0.8140\n",
      "Epoch 796/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3074 - accuracy: 0.8586 - val_loss: 0.4792 - val_accuracy: 0.8140\n",
      "Epoch 797/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2909 - accuracy: 0.8431 - val_loss: 0.4798 - val_accuracy: 0.8110\n",
      "Epoch 798/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2976 - accuracy: 0.8515 - val_loss: 0.4808 - val_accuracy: 0.8120\n",
      "Epoch 799/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2948 - accuracy: 0.8527 - val_loss: 0.4823 - val_accuracy: 0.8110\n",
      "Epoch 800/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3079 - accuracy: 0.8318 - val_loss: 0.4837 - val_accuracy: 0.8070\n",
      "Epoch 801/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3071 - accuracy: 0.8330 - val_loss: 0.4835 - val_accuracy: 0.8100\n",
      "Epoch 802/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3087 - accuracy: 0.8247 - val_loss: 0.4831 - val_accuracy: 0.8120\n",
      "Epoch 803/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2962 - accuracy: 0.8464 - val_loss: 0.4831 - val_accuracy: 0.8120\n",
      "Epoch 804/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3064 - accuracy: 0.8506 - val_loss: 0.4835 - val_accuracy: 0.8120\n",
      "Epoch 805/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3130 - accuracy: 0.8329 - val_loss: 0.4850 - val_accuracy: 0.8090\n",
      "Epoch 806/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3274 - accuracy: 0.8297 - val_loss: 0.4873 - val_accuracy: 0.8060\n",
      "Epoch 807/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3239 - accuracy: 0.8393 - val_loss: 0.4885 - val_accuracy: 0.8050\n",
      "Epoch 808/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3109 - accuracy: 0.8445 - val_loss: 0.4895 - val_accuracy: 0.8050\n",
      "Epoch 809/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2788 - accuracy: 0.8433 - val_loss: 0.4899 - val_accuracy: 0.8050\n",
      "Epoch 810/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3019 - accuracy: 0.8310 - val_loss: 0.4894 - val_accuracy: 0.8070\n",
      "Epoch 811/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3252 - accuracy: 0.8112 - val_loss: 0.4883 - val_accuracy: 0.8090\n",
      "Epoch 812/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2908 - accuracy: 0.8558 - val_loss: 0.4877 - val_accuracy: 0.8120\n",
      "Epoch 813/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3097 - accuracy: 0.8452 - val_loss: 0.4878 - val_accuracy: 0.8120\n",
      "Epoch 814/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2978 - accuracy: 0.8464 - val_loss: 0.4897 - val_accuracy: 0.8100\n",
      "Epoch 815/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3260 - accuracy: 0.8523 - val_loss: 0.4902 - val_accuracy: 0.8100\n",
      "Epoch 816/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2797 - accuracy: 0.8680 - val_loss: 0.4908 - val_accuracy: 0.8100\n",
      "Epoch 817/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3107 - accuracy: 0.8492 - val_loss: 0.4907 - val_accuracy: 0.8100\n",
      "Epoch 818/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2903 - accuracy: 0.8617 - val_loss: 0.4899 - val_accuracy: 0.8110\n",
      "Epoch 819/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3329 - accuracy: 0.8461 - val_loss: 0.4891 - val_accuracy: 0.8110\n",
      "Epoch 820/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3225 - accuracy: 0.8141 - val_loss: 0.4893 - val_accuracy: 0.8110\n",
      "Epoch 821/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3197 - accuracy: 0.8214 - val_loss: 0.4885 - val_accuracy: 0.8110\n",
      "Epoch 822/1000\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3010 - accuracy: 0.8381 - val_loss: 0.4878 - val_accuracy: 0.8120\n",
      "Epoch 823/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2979 - accuracy: 0.8586 - val_loss: 0.4867 - val_accuracy: 0.8110\n",
      "Epoch 824/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2974 - accuracy: 0.8742 - val_loss: 0.4858 - val_accuracy: 0.8110\n",
      "Epoch 825/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2965 - accuracy: 0.8648 - val_loss: 0.4854 - val_accuracy: 0.8110\n",
      "Epoch 826/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2915 - accuracy: 0.8857 - val_loss: 0.4846 - val_accuracy: 0.8100\n",
      "Epoch 827/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3131 - accuracy: 0.8555 - val_loss: 0.4839 - val_accuracy: 0.8100\n",
      "Epoch 828/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3119 - accuracy: 0.8544 - val_loss: 0.4827 - val_accuracy: 0.8110\n",
      "Epoch 829/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3193 - accuracy: 0.8275 - val_loss: 0.4809 - val_accuracy: 0.8150\n",
      "Epoch 830/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2818 - accuracy: 0.8588 - val_loss: 0.4804 - val_accuracy: 0.8180\n",
      "Epoch 831/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3157 - accuracy: 0.8482 - val_loss: 0.4806 - val_accuracy: 0.8160\n",
      "Epoch 832/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3343 - accuracy: 0.8336 - val_loss: 0.4814 - val_accuracy: 0.8170\n",
      "Epoch 833/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.2841 - accuracy: 0.8723 - val_loss: 0.4823 - val_accuracy: 0.8130\n",
      "Epoch 834/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3050 - accuracy: 0.8421 - val_loss: 0.4829 - val_accuracy: 0.8120\n",
      "Epoch 835/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3121 - accuracy: 0.8494 - val_loss: 0.4837 - val_accuracy: 0.8130\n",
      "Epoch 836/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3067 - accuracy: 0.8463 - val_loss: 0.4837 - val_accuracy: 0.8120\n",
      "Epoch 837/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2791 - accuracy: 0.8681 - val_loss: 0.4838 - val_accuracy: 0.8110\n",
      "Epoch 838/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.2855 - accuracy: 0.8702 - val_loss: 0.4841 - val_accuracy: 0.8100\n",
      "Epoch 839/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3046 - accuracy: 0.8463 - val_loss: 0.4844 - val_accuracy: 0.8100\n",
      "Epoch 840/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3001 - accuracy: 0.8534 - val_loss: 0.4844 - val_accuracy: 0.8110\n",
      "Epoch 841/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3069 - accuracy: 0.8369 - val_loss: 0.4833 - val_accuracy: 0.8110\n",
      "Epoch 842/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3381 - accuracy: 0.8327 - val_loss: 0.4831 - val_accuracy: 0.8110\n",
      "Epoch 843/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2875 - accuracy: 0.8619 - val_loss: 0.4836 - val_accuracy: 0.8110\n",
      "Epoch 844/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2968 - accuracy: 0.8546 - val_loss: 0.4842 - val_accuracy: 0.8100\n",
      "Epoch 845/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3200 - accuracy: 0.8492 - val_loss: 0.4861 - val_accuracy: 0.8120\n",
      "Epoch 846/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3032 - accuracy: 0.8525 - val_loss: 0.4877 - val_accuracy: 0.8090\n",
      "Epoch 847/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2933 - accuracy: 0.8546 - val_loss: 0.4891 - val_accuracy: 0.8080\n",
      "Epoch 848/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2745 - accuracy: 0.8857 - val_loss: 0.4916 - val_accuracy: 0.8080\n",
      "Epoch 849/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2984 - accuracy: 0.8659 - val_loss: 0.4927 - val_accuracy: 0.8050\n",
      "Epoch 850/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3383 - accuracy: 0.8398 - val_loss: 0.4924 - val_accuracy: 0.8040\n",
      "Epoch 851/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3120 - accuracy: 0.8513 - val_loss: 0.4935 - val_accuracy: 0.8030\n",
      "Epoch 852/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3262 - accuracy: 0.8482 - val_loss: 0.4911 - val_accuracy: 0.8060\n",
      "Epoch 853/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2723 - accuracy: 0.8700 - val_loss: 0.4875 - val_accuracy: 0.8090\n",
      "Epoch 854/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3094 - accuracy: 0.8431 - val_loss: 0.4846 - val_accuracy: 0.8090\n",
      "Epoch 855/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2823 - accuracy: 0.8681 - val_loss: 0.4835 - val_accuracy: 0.8150\n",
      "Epoch 856/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3195 - accuracy: 0.8213 - val_loss: 0.4831 - val_accuracy: 0.8150\n",
      "Epoch 857/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2956 - accuracy: 0.8515 - val_loss: 0.4829 - val_accuracy: 0.8160\n",
      "Epoch 858/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3203 - accuracy: 0.8513 - val_loss: 0.4827 - val_accuracy: 0.8150\n",
      "Epoch 859/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3359 - accuracy: 0.8555 - val_loss: 0.4822 - val_accuracy: 0.8160\n",
      "Epoch 860/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3438 - accuracy: 0.8233 - val_loss: 0.4818 - val_accuracy: 0.8150\n",
      "Epoch 861/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3029 - accuracy: 0.8463 - val_loss: 0.4819 - val_accuracy: 0.8160\n",
      "Epoch 862/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3058 - accuracy: 0.8586 - val_loss: 0.4822 - val_accuracy: 0.8170\n",
      "Epoch 863/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3025 - accuracy: 0.8638 - val_loss: 0.4829 - val_accuracy: 0.8160\n",
      "Epoch 864/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2847 - accuracy: 0.8754 - val_loss: 0.4839 - val_accuracy: 0.8140\n",
      "Epoch 865/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3251 - accuracy: 0.8192 - val_loss: 0.4841 - val_accuracy: 0.8100\n",
      "Epoch 866/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.2568 - accuracy: 0.8827 - val_loss: 0.4845 - val_accuracy: 0.8100\n",
      "Epoch 867/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2919 - accuracy: 0.8546 - val_loss: 0.4837 - val_accuracy: 0.8110\n",
      "Epoch 868/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2851 - accuracy: 0.8546 - val_loss: 0.4829 - val_accuracy: 0.8110\n",
      "Epoch 869/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3120 - accuracy: 0.8400 - val_loss: 0.4822 - val_accuracy: 0.8140\n",
      "Epoch 870/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2977 - accuracy: 0.8494 - val_loss: 0.4822 - val_accuracy: 0.8200\n",
      "Epoch 871/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3236 - accuracy: 0.8555 - val_loss: 0.4820 - val_accuracy: 0.8190\n",
      "Epoch 872/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2863 - accuracy: 0.8680 - val_loss: 0.4818 - val_accuracy: 0.8170\n",
      "Epoch 873/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3054 - accuracy: 0.8617 - val_loss: 0.4819 - val_accuracy: 0.8130\n",
      "Epoch 874/1000\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.2884 - accuracy: 0.8640 - val_loss: 0.4820 - val_accuracy: 0.8120\n",
      "Epoch 875/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2987 - accuracy: 0.8577 - val_loss: 0.4822 - val_accuracy: 0.8110\n",
      "Epoch 876/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3216 - accuracy: 0.8181 - val_loss: 0.4829 - val_accuracy: 0.8120\n",
      "Epoch 877/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3041 - accuracy: 0.8431 - val_loss: 0.4840 - val_accuracy: 0.8120\n",
      "Epoch 878/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2823 - accuracy: 0.8751 - val_loss: 0.4845 - val_accuracy: 0.8150\n",
      "Epoch 879/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3142 - accuracy: 0.8648 - val_loss: 0.4833 - val_accuracy: 0.8150\n",
      "Epoch 880/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2945 - accuracy: 0.8607 - val_loss: 0.4830 - val_accuracy: 0.8170\n",
      "Epoch 881/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3301 - accuracy: 0.8294 - val_loss: 0.4833 - val_accuracy: 0.8160\n",
      "Epoch 882/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3197 - accuracy: 0.8461 - val_loss: 0.4837 - val_accuracy: 0.8160\n",
      "Epoch 883/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2898 - accuracy: 0.8773 - val_loss: 0.4843 - val_accuracy: 0.8160\n",
      "Epoch 884/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2865 - accuracy: 0.8588 - val_loss: 0.4855 - val_accuracy: 0.8120\n",
      "Epoch 885/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2871 - accuracy: 0.8577 - val_loss: 0.4867 - val_accuracy: 0.8140\n",
      "Epoch 886/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.2594 - accuracy: 0.8753 - val_loss: 0.4878 - val_accuracy: 0.8150\n",
      "Epoch 887/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3250 - accuracy: 0.8450 - val_loss: 0.4898 - val_accuracy: 0.8110\n",
      "Epoch 888/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3184 - accuracy: 0.8277 - val_loss: 0.4926 - val_accuracy: 0.8080\n",
      "Epoch 889/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3017 - accuracy: 0.8607 - val_loss: 0.4943 - val_accuracy: 0.8050\n",
      "Epoch 890/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3028 - accuracy: 0.8523 - val_loss: 0.4953 - val_accuracy: 0.8030\n",
      "Epoch 891/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3034 - accuracy: 0.8523 - val_loss: 0.4950 - val_accuracy: 0.8050\n",
      "Epoch 892/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3002 - accuracy: 0.8277 - val_loss: 0.4953 - val_accuracy: 0.8070\n",
      "Epoch 893/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3006 - accuracy: 0.8433 - val_loss: 0.4955 - val_accuracy: 0.8060\n",
      "Epoch 894/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2907 - accuracy: 0.8237 - val_loss: 0.4954 - val_accuracy: 0.8070\n",
      "Epoch 895/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2915 - accuracy: 0.8341 - val_loss: 0.4946 - val_accuracy: 0.8070\n",
      "Epoch 896/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3176 - accuracy: 0.8225 - val_loss: 0.4936 - val_accuracy: 0.8090\n",
      "Epoch 897/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2894 - accuracy: 0.8443 - val_loss: 0.4927 - val_accuracy: 0.8100\n",
      "Epoch 898/1000\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.3124 - accuracy: 0.8523 - val_loss: 0.4913 - val_accuracy: 0.8110\n",
      "Epoch 899/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2769 - accuracy: 0.8773 - val_loss: 0.4904 - val_accuracy: 0.8140\n",
      "Epoch 900/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3007 - accuracy: 0.8607 - val_loss: 0.4898 - val_accuracy: 0.8150\n",
      "Epoch 901/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3210 - accuracy: 0.8430 - val_loss: 0.4892 - val_accuracy: 0.8140\n",
      "Epoch 902/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3059 - accuracy: 0.8523 - val_loss: 0.4897 - val_accuracy: 0.8130\n",
      "Epoch 903/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2817 - accuracy: 0.8846 - val_loss: 0.4900 - val_accuracy: 0.8140\n",
      "Epoch 904/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2912 - accuracy: 0.8668 - val_loss: 0.4892 - val_accuracy: 0.8150\n",
      "Epoch 905/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2782 - accuracy: 0.8555 - val_loss: 0.4882 - val_accuracy: 0.8130\n",
      "Epoch 906/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3032 - accuracy: 0.8575 - val_loss: 0.4877 - val_accuracy: 0.8110\n",
      "Epoch 907/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3351 - accuracy: 0.8367 - val_loss: 0.4874 - val_accuracy: 0.8110\n",
      "Epoch 908/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3035 - accuracy: 0.8492 - val_loss: 0.4875 - val_accuracy: 0.8120\n",
      "Epoch 909/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.2891 - accuracy: 0.8836 - val_loss: 0.4871 - val_accuracy: 0.8140\n",
      "Epoch 910/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2983 - accuracy: 0.8421 - val_loss: 0.4870 - val_accuracy: 0.8130\n",
      "Epoch 911/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2986 - accuracy: 0.8452 - val_loss: 0.4869 - val_accuracy: 0.8130\n",
      "Epoch 912/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3322 - accuracy: 0.8265 - val_loss: 0.4873 - val_accuracy: 0.8130\n",
      "Epoch 913/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3044 - accuracy: 0.8266 - val_loss: 0.4885 - val_accuracy: 0.8120\n",
      "Epoch 914/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3323 - accuracy: 0.8336 - val_loss: 0.4900 - val_accuracy: 0.8120\n",
      "Epoch 915/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2780 - accuracy: 0.8794 - val_loss: 0.4915 - val_accuracy: 0.8180\n",
      "Epoch 916/1000\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.2907 - accuracy: 0.8669 - val_loss: 0.4924 - val_accuracy: 0.8160\n",
      "Epoch 917/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2951 - accuracy: 0.8544 - val_loss: 0.4940 - val_accuracy: 0.8120\n",
      "Epoch 918/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3160 - accuracy: 0.8306 - val_loss: 0.4949 - val_accuracy: 0.8150\n",
      "Epoch 919/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3102 - accuracy: 0.8483 - val_loss: 0.4954 - val_accuracy: 0.8150\n",
      "Epoch 920/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2972 - accuracy: 0.8680 - val_loss: 0.4957 - val_accuracy: 0.8130\n",
      "Epoch 921/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2911 - accuracy: 0.8659 - val_loss: 0.4959 - val_accuracy: 0.8100\n",
      "Epoch 922/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2806 - accuracy: 0.8721 - val_loss: 0.4947 - val_accuracy: 0.8080\n",
      "Epoch 923/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3138 - accuracy: 0.8390 - val_loss: 0.4930 - val_accuracy: 0.8090\n",
      "Epoch 924/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2910 - accuracy: 0.8525 - val_loss: 0.4932 - val_accuracy: 0.8090\n",
      "Epoch 925/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2958 - accuracy: 0.8515 - val_loss: 0.4929 - val_accuracy: 0.8100\n",
      "Epoch 926/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2831 - accuracy: 0.8577 - val_loss: 0.4911 - val_accuracy: 0.8120\n",
      "Epoch 927/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3157 - accuracy: 0.8244 - val_loss: 0.4896 - val_accuracy: 0.8100\n",
      "Epoch 928/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3189 - accuracy: 0.8534 - val_loss: 0.4884 - val_accuracy: 0.8110\n",
      "Epoch 929/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3271 - accuracy: 0.8450 - val_loss: 0.4876 - val_accuracy: 0.8100\n",
      "Epoch 930/1000\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.2929 - accuracy: 0.8740 - val_loss: 0.4868 - val_accuracy: 0.8120\n",
      "Epoch 931/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2670 - accuracy: 0.8940 - val_loss: 0.4864 - val_accuracy: 0.8120\n",
      "Epoch 932/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3127 - accuracy: 0.8523 - val_loss: 0.4871 - val_accuracy: 0.8130\n",
      "Epoch 933/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2951 - accuracy: 0.8742 - val_loss: 0.4879 - val_accuracy: 0.8150\n",
      "Epoch 934/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2759 - accuracy: 0.8659 - val_loss: 0.4881 - val_accuracy: 0.8140\n",
      "Epoch 935/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2934 - accuracy: 0.8732 - val_loss: 0.4889 - val_accuracy: 0.8110\n",
      "Epoch 936/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3194 - accuracy: 0.8398 - val_loss: 0.4876 - val_accuracy: 0.8150\n",
      "Epoch 937/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3041 - accuracy: 0.8617 - val_loss: 0.4870 - val_accuracy: 0.8140\n",
      "Epoch 938/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3109 - accuracy: 0.8523 - val_loss: 0.4867 - val_accuracy: 0.8130\n",
      "Epoch 939/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3033 - accuracy: 0.8555 - val_loss: 0.4868 - val_accuracy: 0.8120\n",
      "Epoch 940/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3193 - accuracy: 0.8369 - val_loss: 0.4878 - val_accuracy: 0.8100\n",
      "Epoch 941/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2973 - accuracy: 0.8577 - val_loss: 0.4891 - val_accuracy: 0.8100\n",
      "Epoch 942/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3068 - accuracy: 0.8431 - val_loss: 0.4904 - val_accuracy: 0.8110\n",
      "Epoch 943/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2984 - accuracy: 0.8367 - val_loss: 0.4919 - val_accuracy: 0.8140\n",
      "Epoch 944/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.2877 - accuracy: 0.8680 - val_loss: 0.4932 - val_accuracy: 0.8150\n",
      "Epoch 945/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.2891 - accuracy: 0.8492 - val_loss: 0.4946 - val_accuracy: 0.8150\n",
      "Epoch 946/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3020 - accuracy: 0.8463 - val_loss: 0.4956 - val_accuracy: 0.8120\n",
      "Epoch 947/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.2992 - accuracy: 0.8494 - val_loss: 0.4958 - val_accuracy: 0.8120\n",
      "Epoch 948/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3175 - accuracy: 0.8398 - val_loss: 0.4958 - val_accuracy: 0.8110\n",
      "Epoch 949/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2871 - accuracy: 0.8721 - val_loss: 0.4960 - val_accuracy: 0.8100\n",
      "Epoch 950/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3145 - accuracy: 0.8327 - val_loss: 0.4957 - val_accuracy: 0.8080\n",
      "Epoch 951/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3183 - accuracy: 0.8285 - val_loss: 0.4954 - val_accuracy: 0.8100\n",
      "Epoch 952/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2981 - accuracy: 0.8360 - val_loss: 0.4962 - val_accuracy: 0.8070\n",
      "Epoch 953/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2948 - accuracy: 0.8421 - val_loss: 0.4970 - val_accuracy: 0.8070\n",
      "Epoch 954/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2732 - accuracy: 0.8742 - val_loss: 0.4973 - val_accuracy: 0.8070\n",
      "Epoch 955/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3024 - accuracy: 0.8638 - val_loss: 0.4970 - val_accuracy: 0.8080\n",
      "Epoch 956/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2802 - accuracy: 0.8680 - val_loss: 0.4986 - val_accuracy: 0.8070\n",
      "Epoch 957/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2774 - accuracy: 0.8690 - val_loss: 0.4998 - val_accuracy: 0.8070\n",
      "Epoch 958/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2828 - accuracy: 0.8648 - val_loss: 0.4968 - val_accuracy: 0.8100\n",
      "Epoch 959/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3094 - accuracy: 0.8430 - val_loss: 0.4960 - val_accuracy: 0.8100\n",
      "Epoch 960/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2984 - accuracy: 0.8607 - val_loss: 0.4970 - val_accuracy: 0.8100\n",
      "Epoch 961/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.2808 - accuracy: 0.8794 - val_loss: 0.4969 - val_accuracy: 0.8090\n",
      "Epoch 962/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2849 - accuracy: 0.8680 - val_loss: 0.4945 - val_accuracy: 0.8110\n",
      "Epoch 963/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3057 - accuracy: 0.8556 - val_loss: 0.4932 - val_accuracy: 0.8090\n",
      "Epoch 964/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2856 - accuracy: 0.8454 - val_loss: 0.4927 - val_accuracy: 0.8080\n",
      "Epoch 965/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3024 - accuracy: 0.8433 - val_loss: 0.4927 - val_accuracy: 0.8130\n",
      "Epoch 966/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2802 - accuracy: 0.8660 - val_loss: 0.4930 - val_accuracy: 0.8150\n",
      "Epoch 967/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3078 - accuracy: 0.8535 - val_loss: 0.4925 - val_accuracy: 0.8130\n",
      "Epoch 968/1000\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.2953 - accuracy: 0.8619 - val_loss: 0.4917 - val_accuracy: 0.8150\n",
      "Epoch 969/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.2934 - accuracy: 0.8482 - val_loss: 0.4907 - val_accuracy: 0.8170\n",
      "Epoch 970/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2721 - accuracy: 0.8846 - val_loss: 0.4902 - val_accuracy: 0.8140\n",
      "Epoch 971/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2920 - accuracy: 0.8711 - val_loss: 0.4884 - val_accuracy: 0.8170\n",
      "Epoch 972/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2942 - accuracy: 0.8640 - val_loss: 0.4875 - val_accuracy: 0.8130\n",
      "Epoch 973/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2697 - accuracy: 0.8640 - val_loss: 0.4874 - val_accuracy: 0.8100\n",
      "Epoch 974/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.2948 - accuracy: 0.8452 - val_loss: 0.4884 - val_accuracy: 0.8090\n",
      "Epoch 975/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2892 - accuracy: 0.8711 - val_loss: 0.4896 - val_accuracy: 0.8100\n",
      "Epoch 976/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2952 - accuracy: 0.8523 - val_loss: 0.4904 - val_accuracy: 0.8100\n",
      "Epoch 977/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.2876 - accuracy: 0.8711 - val_loss: 0.4900 - val_accuracy: 0.8100\n",
      "Epoch 978/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2935 - accuracy: 0.8546 - val_loss: 0.4904 - val_accuracy: 0.8090\n",
      "Epoch 979/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3001 - accuracy: 0.8588 - val_loss: 0.4915 - val_accuracy: 0.8090\n",
      "Epoch 980/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2907 - accuracy: 0.8690 - val_loss: 0.4923 - val_accuracy: 0.8090\n",
      "Epoch 981/1000\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3196 - accuracy: 0.8369 - val_loss: 0.4919 - val_accuracy: 0.8120\n",
      "Epoch 982/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2951 - accuracy: 0.8607 - val_loss: 0.4919 - val_accuracy: 0.8140\n",
      "Epoch 983/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2879 - accuracy: 0.8586 - val_loss: 0.4921 - val_accuracy: 0.8130\n",
      "Epoch 984/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3118 - accuracy: 0.8338 - val_loss: 0.4924 - val_accuracy: 0.8100\n",
      "Epoch 985/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2877 - accuracy: 0.8608 - val_loss: 0.4930 - val_accuracy: 0.8120\n",
      "Epoch 986/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2942 - accuracy: 0.8546 - val_loss: 0.4942 - val_accuracy: 0.8140\n",
      "Epoch 987/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3174 - accuracy: 0.8482 - val_loss: 0.4959 - val_accuracy: 0.8150\n",
      "Epoch 988/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2926 - accuracy: 0.8742 - val_loss: 0.4972 - val_accuracy: 0.8120\n",
      "Epoch 989/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2789 - accuracy: 0.8669 - val_loss: 0.4981 - val_accuracy: 0.8100\n",
      "Epoch 990/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2805 - accuracy: 0.8391 - val_loss: 0.4990 - val_accuracy: 0.8080\n",
      "Epoch 991/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2900 - accuracy: 0.8433 - val_loss: 0.5003 - val_accuracy: 0.8060\n",
      "Epoch 992/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.2869 - accuracy: 0.8579 - val_loss: 0.5005 - val_accuracy: 0.8080\n",
      "Epoch 993/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.2848 - accuracy: 0.8495 - val_loss: 0.5006 - val_accuracy: 0.8080\n",
      "Epoch 994/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3135 - accuracy: 0.8329 - val_loss: 0.5005 - val_accuracy: 0.8080\n",
      "Epoch 995/1000\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.2852 - accuracy: 0.8443 - val_loss: 0.5014 - val_accuracy: 0.8080\n",
      "Epoch 996/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2822 - accuracy: 0.8452 - val_loss: 0.5022 - val_accuracy: 0.8090\n",
      "Epoch 997/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3058 - accuracy: 0.8431 - val_loss: 0.5015 - val_accuracy: 0.8100\n",
      "Epoch 998/1000\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3345 - accuracy: 0.8273 - val_loss: 0.5010 - val_accuracy: 0.8100\n",
      "Epoch 999/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3090 - accuracy: 0.8544 - val_loss: 0.5008 - val_accuracy: 0.8090\n",
      "Epoch 1000/1000\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2793 - accuracy: 0.8598 - val_loss: 0.5006 - val_accuracy: 0.8080\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=1000, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the run, we will evaluate the model's performance on both the train and the test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.840, Test: 0.808\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then finally, we will plot model loss and accuracy learning curves over each training epoch on both the training and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEFCAYAAADzHRw3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAABPfklEQVR4nO2deXxVxfn/38/dc7NvhCxAwiK7sokgLrhVwN2qFWu11ta239qqba36ba3Vbn67WPVna13q0lq11n1BxQUFEcSAyL4ECGQhIfue3G1+f8xJCBAggSSXezPv1+u+kjMz55xnzpzzOTPPzJkRpRQGg8FgiHxs4TbAYDAYDL2DEXSDwWCIEoygGwwGQ5RgBN1gMBiiBCPoBoPBECUYQTcYDIYowQi6wWAwRAlG0A1HhIhcJSL5ItIoIrtF5G0ROSWM9nxTRIKWPZ1/Wd3Yd7aIFPeHnd1BRApF5Oxw22GIPIygG3qMiPwYuB/4HZABDAX+Blx0kPSOfjJtmVIqbr9faW8cuB/zYDAcMUbQDT1CRBKBe4AfKKVeVko1KaX8Sqk3lFK3Wml+JSIvisgzIlIPfFNEskTkdRGpFpECEflOp2NOt2r79SJSLiL3WeEe6xhVIlIrIp+LSMYR2l0oIj8VkTUiUici/7GOHwu8DWR1rtUfQR7a0/9HRBpEZJWInGDF3SoiL+1nz4Mi8kAP8+AWkftFpNT63S8ibisuTUTetK5TtYgsERGbFXebiJRYdm0WkbOO5Boajn2MoBt6ykzAA7xymHQXAS8CScC/geeBYiALuAz4nYicaaV9AHhAKZUAjABesMKvBRKBIUAq8D2g5ShsvwKYA+QBxwPfVEo1AXOB0i5q9T3JQ3v6/wIpwLPAqyLiBJ4B5ohIEnTU9q8E/tlD+38OzAAmAScA04FfWHE/sWxLR7ea/hdQIjIauBE4USkVD5wLFPbwvIYIwQi6oaekApVKqcBh0i1TSr2qlAoBacAs4DalVKtSajXwOHCNldYPjBSRNKVUo1JqeafwVGCkUiqolFqplKo/xDlnWDXU9t+2/eIfVEqVKqWqgTfQwthbeQBYqZR6USnlB+5Dv/hmKKV2A4uBy610c9DXcOVhzr8/XwfuUUrtUUpVAHcD37Di/EAmMMxqMS1ReqKmIOAGxomIUylVqJTa/7oYogQj6IaeUgWkdcOnXNTp/yygWinV0ClsJ5Bt/X89cBywyXKrnG+F/wt4F3jecjH8QUScInJqJ/fI+k7HXK6USur0G7GfTWWd/m8G4noxD/ukt14C7bV5gKeBq63/r7by1lOyrHN2Pn/78f8IFAALRWS7iNxu2VEA3Az8CtgjIs93p6PYEJkYQTf0lGVAG3DxYdJ1nsazFEgRkfhOYUOBEgCl1Fal1HxgEPB/wIsiEmvVNO9WSo0DTgbOB66xap/t7pHxvZCng0052u08WAxp/8fyX+dY+wG8ChwvIhPQ+fj3EdhZCgzb7/ylAEqpBqXUT5RSw4ELgR+3+8qVUs8qpU6x9lXoa2yIQoygG3qEUqoO+CXwVxG5WES8Vq15roj84SD7FAGfAr+3OiKPR9fKnwEQkatFJN2q1dZau4VE5AwRmSgidqAe7VYI9UG2yoFUq8O3Sw6XB4upInKp1Xq5Gf3iW27t34r2xz8LrFBK7TqMTU7rPO0/B/Ac8AsRSReRNHQ5tF/D80VkpIgIUId2tYREZLSInGl1nrai+yD64hoajgGMoBt6jFLqz8CP0R1yFWhXw43oWujBmA/komuUrwB3KaXet+LmAOtFpBHdQXqlUqoFGIwWwXpgI/Axh3ZVzJQDx6Gf2I38bEKL5XbL934wl8Sh8gDwGvA1oAbt277U8qe38zQw8TB5aGcBWnzbf78CfgPkA2uAtcAqKwxgFPA+0IhuRf1NKbUI7T+/F6hEu5wGAXd04/yGCETMAhcGw9EjIr9Cd95efYg0Q4FNwODDdO4aDEeEqaEbDP2A5VP/MfC8EXNDXxG2GnpaWprKzc0Ny7kNht6mtLSUtrY28vLyDogLBoOsWbMGl8vFqFGjcLlcYbDQEC2sXLmyUimV3lVc2D5nzs3NJT8/P1ynNxgMhohERHYeLM64XAwGgyFKMIJuMBgMUULECfo/lxUy9dfv4QuYobQGg8HQmYicErSqyUddi5/0eHe4TTEYDP2M3++nuLiY1tbWcJvSp3g8HnJycnA6nd3eJ+IEPTFGZ84IusEwMCkuLiY+Pp7c3Fz0h7HRh1KKqqoqiouLuxw5dTAizuWS6HEQTzN1Lb5wm2IwGMJAa2srqampUSvmACJCampqj1shESfox219jLWeb9PQ2BhuUwwGQ5iIZjFv50jyGHGC7koYBEBLbXmYLTEYDIZji4gTdHeiXoHMX78nzJYYDIaBSG1tLX/72996vN+8efOora3tfYM6EXGCHpOsBT3YYATdYDD0PwcT9EDg0It4LViwgKSkpD6yShNxo1wc8drlQlNleA0xGAwDkttvv51t27YxadIknE4nHo+H5ORkNm3axJYtW7j44ospKiqitbWVm266iRtuuAHYO91JY2Mjc+fO5ZRTTuHTTz8lOzub1157jZiYmKO2LeIEnVg9J429xQi6wTDQufuN9Wwo7d3JK8dlJXDXBQdfCOvee+9l3bp1rF69mo8++ojzzjuPdevWdQwvfOKJJ0hJSaGlpYUTTzyRr371q6Smpu5zjK1bt/Lcc8/x2GOPccUVV/DSSy9x9dUHnXm520SeoLviaMOFo6Uq3JYYDAYD06dP32es+IMPPsgrr7wCQFFREVu3bj1A0PPy8pg0aRIAU6dOpbCwsFdsiTxBF6HBnoTbVx1uSwwGQ5g5VE26v4iNje34/6OPPuL9999n2bJleL1eZs+e3eVYcrd770eRdrudlpaWXrEl4jpFAZqdKXiNoBsMhjAQHx9PQ0NDl3F1dXUkJyfj9XrZtGkTy5cv71fbIq+GDvjcKcS37EYpNSA+MDAYDMcOqampzJo1iwkTJhATE0NGRkZH3Jw5c/j73//O2LFjGT16NDNmzOhX2yJS0AMxqaTUbqLZFyTWHZFZMBgMEcyzzz7bZbjb7ebtt9/uMq7dT56Wlsa6des6wn/605/2ml0R6XIhNp1U6qlqaAu3JQaDwXDMEJGCbo9Pxy0BamrN0EWDwWBoJyIF3ZUwGICm6rIwW2IwGAzHDhEp6F7r8//WGiPoBoPB0E5ECnpc+lAAgrUlYbbEYDAYjh0iUtBj0vVXWfb6nWG2xGAwGI4dIlLQccdRSwKuRlNDNxgM/cuRTp8LcP/999Pc3NzLFu0lMgUdqHQOJr7FCLrBYOhfjmVBj9ivcho82aQ1bgq3GQaDYYDRefrcc845h0GDBvHCCy/Q1tbGJZdcwt13301TUxNXXHEFxcXFBINB7rzzTsrLyyktLeWMM84gLS2NRYsW9bptESvorXHZZNQvQYWCiM0ebnMMBkM4ePt2KFvbu8ccPBHm3nvQ6M7T5y5cuJAXX3yRFStWoJTiwgsvZPHixVRUVJCVlcVbb70F6DleEhMTue+++1i0aBFpaWm9a7NFxLpcSByGSwLUlu8KtyUGg2GAsnDhQhYuXMjkyZOZMmUKmzZtYuvWrUycOJH33nuP2267jSVLlpCYmNgv9kRsDd0xaCRsgppd60nOzDv8DgaDIfo4RE26P1BKcccdd/Dd7373gLhVq1axYMECfvGLX3DWWWfxy1/+ss/tOWwNXUSeEJE9IrLuIPEiIg+KSIGIrBGRKb1v5oGk5E0CoKmol5tbBoPBcAg6T5977rnn8sQTT9DY2AhASUkJe/bsobS0FK/Xy9VXX82tt97KqlWrDti3L+hODf0p4CHgnweJnwuMsn4nAQ9bf/uUnCHDqFQJyJ4NfX0qg8Fg6KDz9Llz587lqquuYubMmQDExcXxzDPPUFBQwK233orNZsPpdPLwww8DcMMNNzBnzhyysrL6pFNUlFKHTySSC7yplJrQRdwjwEdKqees7c3AbKXU7kMdc9q0aSo/P/+IjG5n5T2nkObyM+z2z47qOAaDIXLYuHEjY8eODbcZ/UJXeRWRlUqpaV2l741O0WygqNN2sRV2ACJyg4jki0h+RUXFUZ+4MnYkGa07IBQ66mMZDAZDpNOvo1yUUo8qpaYppaalp6cf9fFaUsbhoQ1VYcajGwwGQ28IegkwpNN2jhXW56ih2m/VsGVxf5zOYDAcI3THVRzpHEkee0PQXweusUa7zADqDuc/7y3Sh4xht0rBV2AE3WAYKHg8HqqqqqJa1JVSVFVV4fF4erTfYUe5iMhzwGwgTUSKgbsAp3XSvwMLgHlAAdAMXNcjC46CERlxfBYaw7m7l4NSYBaMNhiinpycHIqLi+mNfrhjGY/HQ05OTo/2OaygK6XmHyZeAT/o0Vl7icEJHtY5JnKx71OoKoC0UeEww2Aw9CNOp5O8PPMxYVdE7qf/gIhQP9ga8l74SXiNMRgMhjAT0YIOMGjYeMpVEsFtvT9I32AwGCKJiBf0CTlJfBCcDAUfQKAt3OYYDAZD2Ih4QZ88NImFoWnY/Y2wY0m4zTEYDIawEfGCnpHgoSTpRFrFA5veDLc5BoPBEDYiXtABJg8fzBI1CbV5gZkGwGAwDFiiQtCn56Xypm8K0lgOpavCbY7BYDCEhagQ9JPyUlgUmkRIHMbtYjAYBixRIeg5yTEkpqSzwXMCrH3JuF0MBsOAJCoEXUQ4Y/QgnmyaBXW7YLsZk24wGAYeUSHoAGeMGcQbvqn43cmQ/0S4zTEYDIZ+J2oEfebwVGxON8uSLoBNb0HVtnCbZDAYDP1K1Ai6x2nn5BFp3Fd/BsruhGUPhdskg8Fg6FeiRtABzh6bweoaN9UjL4XVz0JDWbhNMhgMhn4jqgT9vOMzcTtsPG27BIJ+WPpAuE0yGAyGfiOqBD0xxsm8iZk8uUHwjb9cd442lIfbLIPBYOgXokrQAW44bTiNvgBPOy6DUAAW/iLcJhkMBkO/EHWCPjYzgfOPz+K+lUGaZvwY1r4Anz0abrMMBoOhz4k6QQe45exR+IIh/tR6IYw+D965Dda+GG6zDAaDoU+JSkEfnh7HZVNy+PdnxZSe9SBkT4WXrofXf2imBTAYDFFLVAo6wI/O1gtGP7ikFK57G2bdDKv+CW/dYkTdYDBEJY5wG9BXZCfF8PUZQ3nq00JOHZXOeWf/CsQGn9ynR76c92eIzwRb1L7TDAbDACNqBR3gZ+eOYV1JHTf/5wviPSdy2tl3QfxgWHgn/GUcOL2QNgqCAYhNheOvhOOvALsz3KYbDAZDjxGlVFhOPG3aNJWfn9/n56lr8XPlo8vZUdnIY9dM49RR6VBTCOtfhZodUF+qa+6VW6B6OwydCZc/pYXfYDAYjjFEZKVSalqXcdEu6ABVjW18/fHPKNjTyD0XTeCqk4YemEgpWPtfeONmSMiCq/4DqSP6xT6DwWDoLocS9AHhQE6Nc/PC92Yya2Qa//vKWu54eS21zb59E4lod8vVL0FzJTw6G/KfhEBbWGw2GAyGnjIgaujtBEOK/3tnE48v2U6c28EPzhjJtSfn4nHa901Yuwte+jYUfQbeVDj+a3DSdyE5t1/tNRgMhv0Z8C6X/dlc1sC9b29k0eYKshI9XH/qcC6flkOCp1NnqFJ65aOVT+t1SpXSLphBY2HyN2DUOWGx3WAwDGyMoB+ET7dV8qd3N7NqVy2xLjtfnZrDtSfnMiI9bt+EdSV6fvWytbrztLEchp4M7jhIHQln3gkub3gyYTAYBhRG0A+BUopVu2r492e7eOPLUkSEiydlce3JuYzPSjxwh4APVjwCXzwD/hao3QmeJP01avpoGD4bhpwEnkTtlzcYDIZexAh6N9lT38oDH2zl5VUltPiDTM9L4bqTczlnXAYO+0H6j3cu01+g7tkAFZsg0KrD046Dc38Po87uvwwYDIaoxwh6D6lr9vNCfhH/XF5IUXULWYkeLpuawyVTcshLiz34jv5W3ZG6azmsfxkqt8Kce2Hy13VHa8VmXYOPSeqvrBgMhijDCPoREgwpPty0h38uK+STgkqUgilDk7h0Sg7nH59Jktd18J3bGuCFa2HbB/uGe9Pg7Ltg0tVm2gGDwdBjjKD3AmV1rby2uoSXVhWzpbwRp12YPXoQ5x+fydljM4h1dzGLQigI2z+CkpWQkA2x6XoumV3LIPMEmP2/MOorRtgNBkO3OWpBF5E5wAOAHXhcKXXvfvHfBP4IlFhBDymlHj/UMSNN0NtRSrG+tJ5XvyjhzTW7KatvxeO0ce74wVx54lBOzE0+uL9dH0DPzf7hPdoNkzICJs2Hqm26Vj/+EhhzHjhj+i9TBoMhYjgqQRcRO7AFOAcoBj4H5iulNnRK801gmlLqxu4aFamC3plQSJG/s4Y3vizllS9KaGwLkOx1cs64DOZOyOTkkam4Hfaudw769Xwynz2sa/CeJHB4oLEM3Akw9kLImgTpY2DQOD15mMFgGPAcraDPBH6llDrX2r4DQCn1+05pvskAFPTONPsCfLy5gnfWl/Hhxj00tAWIdzuYlpvMpVNyOG9iJjbbQYYxNu7RwxxtTti5FFY/CxvfAF+Djre79Fj32DTtrkkdpacpiB8MpauhfB1MvAIc+/n0QyHjzjFEL83VemhwTHL30lcW6NFoABnjITlv3+cj4IOdn0BDma5IDZ544MyrVdv0wIfWOmhr1BP8pY7Q36VkHg+uToMmWmrB16gHSWx9T5+7oUx/fT77Nt0aPwKOVtAvA+Yopb5tbX8DOKmzeFuC/nugAl2bv0UpVdTFsW4AbgAYOnTo1J07dx5Rho512gJBlhZU8vbaMhZvraC8vo0xg+O5ZHI2l0zOZlCC5/AHUUrPBFm5BT75C+z4WIc7veBv1jNEJuRAw24I+fXY95Fn6+kJ4jNh3Yuw7mWY8X2YfoN+GRiij958aQcDEPQd3UdyDeV6kru4QbqVWb0N6or1cN6c6ZCQqSswQZ/uUypcooVv13LYsRjc8XrWU28qZE3e+/Om6FFiviZ9v29bBBteBbHDmT/XfVJ2F9QWaVGNz9TnrN2lRXnLu3rkmeq0uI07EZKH6ufI36w/HGyp3hsfnwm5p+gPCT2JULUd9qw/dP5jB0HKcP3cdj5WbDoMPl5P/NdSA9Ou08/rEdAfgp4KNCql2kTku8DXlFJnHuq40VZDPxjBkOKVL0p44IMtFFW34HbY+Mr4wVw6JZvTRqVjP1itvTNK6RskFNQfL1Vvh3UvQfUO/fAlZOuJxOqL990vbrB24dhd+uY5+YcwZIaOqy/RD53Drbd9zVBXpB+0QeMjo2bfWgcFH0D2lL3z7NTs1C+5tOO0COxPSy0UvK8f1PYpktufgf78EMzXpMvP7tLX3R2vhcvmgFDgwJphKKQ/Ymur12K27QPd51K2FkbP1S00b6rue/nsEV0BGH8J5J4KKXm69ddcqc/55XP6nKfcou8pV6wW3Y//AIEWLWTueC1CnkRdE3a4dNraIl3rTMzR162tTh87Jkm7DIs/1/dQV9jdEJcBdbsOjHN4tK2+Rv31dXM1lH4BDaVdH8sVD5OugtJV+pyHw5OoR5YdfwWg9HUrXa2vQ32pvgZJQ2HCV3W/1u4v9QugbK2uDLXW6esx/lIYcaYOc3p1eTXtgZ2f6vIpX69fIoPG6nvSkwQZEyDnxF57pvrc5bJfejtQrZTq4jPLvQwUQe9MYWUT//hkB2+sKaW22U9WoofTR6czc0Qa547POLi/vbv4W/VNVV8KycN0TaFis56PZs3z0FylayWeRP1QiU0/SCq094Mo0CNvTv2JvoFdsdaNa9fNRVecrnU1lOlaUcrwwwthndVXnpi9Nyzo1+JbX6JdRmLTDxPoh2D3at1MTRqi56gPBbQIqpCez75kJXz5PLTW6jxMvU4Lyap/6hoc6Ka4MxZO+Jp+Ye3+Up+rrV7vk3OiTrdzqf6bMUHP0VO7C5oq9XXMORHSj4PifP3wp4/Ri6LEpGgBDfm1GCZkaxF0erVtVQX6ZZk2WotS9XZ93NhBep/CpRDcbyZPu0vXOFUI8k7T5aRCWojLN+i/7Qwap8smOQ82L9BC2I7YtDjVFHZdHt40UEFdU+xM3mmQe5q21d+kr0FztRavoF/fA14r3/W79bY7QeenpUaXT8YEmPE/+lqtfRGyJ0PmJF1Om9/WL9SsSdr2hjIdl5Knr58n4UBbG8q08LbU6MpMTLIWUW+qrswopa91U6W+hxOy9bVortL2xWcBSt+n7ZWXCOdoBd2BdqOchR7F8jlwlVJqfac0mUqp3db/lwC3KaVmHOq4A1HQ2/EFQry/sZyXVxXz2Y5qGlp1Z+q54wczdVgyp45KZ3BiN9wyPTpps26iFn+uBXb46brW4W8GRItHcq5+AXxynxbQ7uBJ1A/loHG6Vik2XUvZs0E/UK44y28p+qFqqdbiEGg9xDkEOMzoK0cMjDgDpl0Pq57SYiE2XYMad5F+yKu26hrl9kW6FpkzTdf+xpwP2z6E4hX6JTjyLG371vehfC0kDtEiFpu+118aNxgyxula4/5C2BXuRC10/mZ97uRhlsju1MIy/AxtTyioWwrNlVq4QgH9winOh9Z6XTP2pll+2hn6RTJorH6ptNPWoH27rXX6lzpCl0dLjbbf36Jfdt5UXcZJw7SgV2zSeWyt07Xn1BFmuooIoDeGLc4D7kcPW3xCKfVbEbkHyFdKvS4ivwcuBAJANfB9pdSmQx1zIAt6Z4IhxafbKnlyaSGrdtVQ26xrl8PTYjllVBpnjc1gxvAUnDbbwTtVe5umKl1r9TfrWle7IMQP1mHOWBg0Bnav0U3e0i90h1PIr2uU7SNzQn5dIxt+uu5w2rNevwBccbqGnDZKi2dKnq5R7Visa6mN5Xr/EWfq8J2fahF0eLRLIWmoFubObon2hb+7atY2lOv9uqoBdkYpPf+9s9PLNBTU18AVt/fY/hZdc22u0nGpI7R4tjXoX9JQHRb06zSx6frlZjD0AubDogghFFJsKmtgydYKlm+vYtn2Klr9IbwuO4GQYlJOEjedPYoZw1O753vvb5TSgm7Ey2DoM4ygRyit/iDLtlWxaPMeAiHFG1+W0tAaYFC8m6+Mz2BcZiKjB8czIj320NMQGAyGqMEIepRQ1djGJwWVvLlmN0sLKmn2BTvipg5LZmJ2IhOyE5mQncDI9DhCClyOCBitYjAYuo0R9CgkFFKU1LawqayBL4tq+aSgki3lDR0ib7cJIaU4cVgKl07J5ivjB5MSa2rxBkOkYwR9gBAMKXZUNrG+tI4NpfW0BUJ8tHkPhVXNOGzClKHJJHmd5CR7OWNMOuMyE0iJdVFU3YLHaeveB08GgyGsGEEfwLRPJvbmmt18XlhNY2uAwqom2gJ6VIjXZe+o1c8YnsKJuSkclxHP9oomkrxO5k3MJD0+OsbvGgzRgBF0wz40tQXI31nD1vIGiqqbGTEojpLaFj7eXMGmsoZ90tptQl5aLF6XncEJHgYneohzO4h1O4hzO1BKkRDjZPTgeMZlJiBmHLPB0KcYQTd0m1Z/kPWldcQ4HTjtwitflLCjsokmX5DdtS3saWijsS1AMHTgfeOy20j0Okn2OknyukiKcZLkdeK024j3OMlIcONx2rHbhNRYF+nxbhJjnMS6HThtel+DwXBojKAbehWlFG2BEI1tAQJBRV2Ln/yd1RRVt1Db7KOm2Udts5/aZj/VzT7a/EFa/EH8wUPfa0leJ3FuB16XHa9L/411O6hu8hHrdpCV6MHtsBHncZAY46Sm2c+SrRV4XQ4uOCGLnKQYGtsCVDf5SItzkx6/9+cPhCipbWHkoDjK61uJcdpJj3ebFoUh4jiUoHexzI7BcGhEBI/TjsepPyAanOhh9OD4Q+7jD4ZobA3QGgjS5g9R1eSjuslHbbP+KwI7q5pp8QVp9gVp8gVo8QWpbGzCH1T4AiE2lNbr43RqIZyYm0xJTQt3vrquW7bbBNobFwkeByMHxXWsNpXgcRIMKew2IRAKUdXoI8nrYlRGHElWS0K/bOyU17exaPMeZo1IY/TgeGLdDtwOGy6HjWBI0eIPkhjjpLCyiWGpXmLdDlp8QRpaA4zNTOhyOGkopHr8NXBds594j6P/viI2HNMYQTf0C067jeROwyZzD7XY9mFQStHYFsDtsONy2PAFQmzcXU99q58Uy5VT3eSjoqGt42e3CYMSPGwtbyDB48RpFwoqGinY00hjWwCloKS2BZSezsRptxHndrC9spEPN5XThYeJxBgnH22u6LH97drrsNlw2gWnw4bDJlQ3+Rg5KA6P045NhLoWP5UNbYzMiCPe48Tr1C+T6mYf2yuaqG7y0dgWID3ezfTcFDITPbidNsrr21hXUseIQXFkJ8WQFufC47RTWttKktdJZqKHnVXNDEmJIRBUOO02HHahqLqFhlY/IwfFMSJdv+hsol/gNtH9KXFuB75ACI/LjsMm2G2C02bDH9Iv7IrGNkpqWvT6u8OSDxgqGwiGaPEH8TjtOA+1spfhiDAuF4PhMCilaGgL0OoP0tQWpMUXxO20kZsay67qZiobdb+CLxAiEFQEQiHcDhv1LQEykzxUN/lo9gVxO2y4HXY27q4HIBBS+IOhjlZHbbOftkAQh81GSOm4lFgXNU1+mv1BWnwBmtqCJMQ4GZ4eS6zLTlqcm4276ymsamZ3XQv+oEKACdmJFFY10ewL4rNGNDntcli3l8MmBLp6ex0hyV4nzb4gNhFcDht1Lf6OuEHxbpraAviCIfLSYlHWy9Qm+kWRZPXFBIIhtlc0kR7v1pPWKWgNBCmuaWF3XSvpcW5EoMUXJDPJg02EHZVNDEn2MiYzHq/LjtuhR3O1+AK4nXbdmrLbcDpstPj0NXXZBZtNrBeVDbsNCiubeW11CQ67je+eNpxhqbH4gyGcdhsVDW3sqGwkNc5NvMfByp017KxqZt7ETIameIlx6gqH0y4dL82Q0i2xlFhX1+sQdwPjQzcYBihKKepb9csoLc5NXYufkpoWMhLc1LcGcDtsHS+WxBgnqbEutlc2UVjZRItfD2cNKYVS2m3WYO3TFggRDCkCIUUgqFAokr0uUuNcZCbGEAiGWLmrhpKaFmKtWn1di5/0eDdOu+Cy2ymqacblsGETKK5pweuyo5T+nsIf1Olrm/00+4KMz0qgptlHWV0rNpt2+aXHubHbhKKaZoameIn3ONhd10owpBiS4mVnVRM7KvQQ3UBI4bQLXpeDtkCQtkCI7krfzOGplDe0sr2i6ZDpXHYbaXEuSutaD5kO4NcXT+AbM4Z1z4D9MD50g2GAIiIkxjhJjNEjiFJiXR1ukEEHmXzyuIx4jss4dJ9Idzhp+LGzDm4gGEJBh5tHKf0y8gV0a6qhNUBQqY6XVMj663XZyUjwEArp7znqWvw47NLRespNjWVHZRMltS1MGpJEepzbmlQvaL2YdIvNZ71U7CIgMGVoN5fN6yGmhm4wGAwRxKFq6KZXwmAwGKIEI+gGg8EQJYTN5SIiFcDOI9w9Dag8bKrowuR5YGDyPDA4mjwPU0qldxURNkE/GkQk/2A+pGjF5HlgYPI8MOirPBuXi8FgMEQJRtANBoMhSohUQX803AaEAZPngYHJ88CgT/IckT50g8FgMBxIpNbQDQaDwbAfRtANBoMhSog4QReROSKyWUQKROT2cNvTW4jIEBFZJCIbRGS9iNxkhaeIyHsistX6m2yFi4g8aF2HNSIyJbw5ODJExC4iX4jIm9Z2noh8ZuXrPyLissLd1naBFZ8bVsOPEBFJEpEXRWSTiGwUkZkDoIxvse7pdSLynIh4orGcReQJEdkjIus6hfW4bEXkWiv9VhG5tic2RJSgi4gd+CswFxgHzBeRceG1qtcIAD9RSo0DZgA/sPJ2O/CBUmoU8IG1DfoajLJ+NwAP97/JvcJNwMZO2/8H/EUpNRKoAa63wq8HaqzweGCLiETi6tUPAO8opcYAJ6DzHrVlLCLZwI+AaUqpCYAduJLulfNfrHSRwlPAnP3CelS2IpIC3AWcBEwH7mp/CXQLpVTE/ICZwLudtu8A7gi3XX2U19eAc4DNQKYVlglstv5/BJjfKX1Hukj5ATnWTX4m8CYg6K/nHPuXN/CutZ0LBIEQcHk/2urohWMkAjuwBiN0VXZRWMbZQBGQgp7d9U3g3MOVc/s1t9JJOGw/wvzmAuuOtGyB+cAjncL3SXe4X0TV0Nl7c7RTbIVFFVYzczLwGZChlNptRZUBGdb/0XAt7gd+hhZngFSgVikVsLY756k9v9cAy4F64NvtB7JcVi+LSIWIVInIQ53ivmO5Nxosl9YUK1yJyMhO6Z4Skd9Y/88WkWIRuU1EyoAnRSRZRN60zlFj/Z/Taf8UEXlSREqt+Fet8HUicgGQB1QAT4lIQEReEZFYoriMlVIlwJ+AXcBuoA5YyeHLGSu+Dn1fRCo9LdujKvNIE/SoR0TigJeAm5VS9Z3jlH5lR8U4UxE5H9ijlFrZw12vAf4NNABnikiG5Yp7Ez03UC76AXjeOs/lwK+s/RKAC4Gqbp5rMLpmOQzdLLYBT1rbQ4EW4KFO6f8FeIHxwCC0ywDgn8DV6BrnFGAd2tWyi71NcCC6yhjAchdchH6ZZQGxHOiWGBD0R9lGmqCXAEM6bedYYVGBiDjRYv5vpdTLVnC5iGRa8ZnAHis80q/FLOBCESlEi++ZaP9ykoi0L7zSOU8lwHloMX0JiAG2AVehfY1ZwK1KqSalVKtS6hNrv28Df1BKfa40BUqp7k4KFwLuUkq1KaValFJVSqmXlFLNSqkG4LfA6dBRNnOB7ymlapRSfqXUx9ZxngHmAbXoGtdJaPF/ES3w0VrGAGcDO5RSFUopP/AyuuwPVc5DAKz4RLr/Aj4W6WnZHlWZR5qgfw6MsnrIXejOldfDbFOvICIC/APYqJS6r1PU60B7T/e1aN96e/g1Vm/5DKCuU9PumEcpdYdSKkcplYsuxw+VUl8HFgGXWcn2z+9NwELgDOBD4FkrzRBgZ6cmfGeGoIX/SKhQSnWsJyYiXhF5RER2ikg9sBgtTHbrPNVKqZou8loKLAVOBUrR4v5v4CxgA1Faxha7gBnWtRP25vlQ5dx+LS5D3xeR3GLpadm+C3zFcu8lA1+xwrpHuDsRjqDTYR6wBf2Q/jzc9vRivk5BN8fWAKut3zy0//ADYCvwPpBipRf0iJ9twFr0KIKw5+MI8z4beNP6fziwAigA/gu4rfAkwI+uNfvQvuga65qdjq75HNBxaT0MNx3kvE3A8Z223wF+08mm4v3S3wl8BAy2tidZ53egO7RCQNJBzjXfKsd70P7/NcCrQHK0lzFwN7AJ7Wr6F+A+RDl7rO0CK354uO3vQT6fQ/cT+NEtseuPpGyBb1n5LwCu65EN4b4I5md+3flZgliN9l0P7vRbjPZVf4nufIu1RGGWtd/l6E6mqdZDNBI9nzToWvO96KF0c9A+8UMJ+h+At63jpwCvtAu6Ff8WutWQDDiB0zrtG4N+Aa0Drgn39TS/6PxFmsvFMHC5FnhSKbVLKVXW/kN3Ss4HLkCL9S507ehrAEqp/6J93c+iO1JfRYsxaBfOBWjf9tetuENxP1qYK9Ejbd7ZL/4b6NrZJnSL4eb2CKVUC9r3n4f2IxsMvY6ZnMtg6CdE5JfAcUqpq8NtiyE6cRw+icFgOFqsLwCvR9fiDYY+IWw19LS0NJWbmxuWcxsM/UlFRQXFxcWkpKQwbNiwcJtjiHBWrlxZqQ6ypmjYaui5ubnk5+eH6/QGg8EQkYjIQb+jMJ2iBoPBECUYQTf0Ko1tAXZVNdPqD7JsWxX+YIgdlU2sKa7FHwwd/gCGo2ZTWT1F1c093q+qsY0PN5XzydZKAt0sq42761FKUbCnkbZAkFBIsbSgko+3VLCmuJZQKHyDLpRSbNxdf9D4UEixqezg8f5giK3lDX1hWp9hBN3Qq3zziRWc9sdFPPzRNuY/tpwX8os4408fceFDS/ndgo2HP4DhqGjxBZlz/xIu+dunPd73jpfX8q2n8rn6H5/x9rqyw6bPL6xm7gNLuO+9LZx938f84pV1LNxQztcf/4xrn1jBhQ8t5YmlO44kG73Cu+vLmfvAEt5cU9pl/GNLtjPn/iV8WVTbZfxv39rIOX9ZTGltSx9a2buErVN02rRpyvjQo4NgSHHhQ5+QnRTDwg3lB02XkxzDJ7edeUD4nvpW5j24hMpGHwB/+/oU5k3M7DN7a5t9nPfgJ3x1SjY//srojvArHlnGupI6mn3BjrAhKTG8fdNpxLl1d9OrX5Twi1fX0dimZxnwuuyMGRzPTWcfxw+fXUUgpPbZXwSU0umumZmLx2nj/ve3AnDz2aO4+ezj9rHtV6+vZ/HWCv50+Qlc/9TntAVCHce799KJXDl9KNc+sYL1pfW8fuMs2gIhLv/7Mn58znHMHp3Oyfd+2HGsiyZl8cCVkwH48QureWddGXab0NCqbX/p+yczdZieanvab96nsrGNM8cM4sNNe0j2Ovc59+1zx7C7toWnl+3syHfnfB4Kr8sOgE2k47q1s+ins8lLi+3Yfn9DObe8sBqHTWgL6FZCsy9IWpyLV/5nFve+vQlfMMR71n3mddk5ZWQaH2zaQzCkuOmsUUzPS+H7z6yksS3A/g2E9Hg3/7p+Otc9+Tm76/SsDk67cOaYQXxeWEN1k6+jbNrLye2wYbcJIaVw2mw0tAWYMTyF0tpWKhvbADhrbAanjkzj129uIKj2vQdcdhvDUr289aNT+fN7m3l7bRkLbjq1457qKSKyUik1rcs4I+gDj2BI8eeFm4n3OPn+7BEd4c+v2MWE7EQmZCcesE9+YTUv5OtZPedOyOSMMYMA2FzWwFOfFvLcil2HPa/dJlwzcxguh42aJh91LX5KalsIhWDDfk3jeRMHs21PE+cfn8mOyibagiFOG5XGRZOy+eO7mymra2VHZRN5abEU1TRz+dQc6lsD+IMhyupauXrGsC7z8em2Sh5bvJ1FmysA+NasPH42ZzRKwdhfvsP03BRWFFbvs8/rN87i+JwkAL7zz3yWbavqEKZ2ARyeHkt5XSuXTMnmmeUHvxbJXic1zf6O7StPHMK3TsnjuIx4lFLk3bEAgBHpsWyvbOJbs/L4xye6ljskJYbpuam8tKoYgFkjUxGETwoq9znHkJQYiqp1rfLGM0ZS2+LjhfxiRqbH7XOdT8hJ5LUbT2Hlzhq++rCu0f/j2mnc+uIaqpt8DIp3s6ehrSO9y2HDZ4nsd07N47El+9a+v3lyLk99WgjAU9edyCMfb2dCdgIrdlTzZXEdsS47Tfu9BI7PSeT1G08B2Cf/7eSlxbKjsgmAmcNTWbZ933m6clO9FFbt614al5mwTz5HDoqjYE9jx/Z3Tx/OIx9v58ITsqhoaKOqqY0t5Y37HMPjtNHq35vXL3bVkr/zgGl6uHRyNkU1zXxeeGDc/rz8PydzqdVyevpb0zn9uC4HqhyWQwm6GYd+jKCUYtWuWlx2G4MTPaTHH9liPGuKaxmWGktijBPQfsIvimoYl5lIjFVTWltSx98+0vNVDUv1khLrorbZx+0vrwWg8N7z9jlmSW0LP3h2FVWNPhx24d315Xx862zWltTxjX+s2Cdt55rNN2YM4/2N5dx89ig+KajijS9LeXJpIbCvsA3qIq8L1uom/+b39vow31qzG19QdQgc7H0RrCmu22f/svpWrj05F6/TbuUzljiPgxuf/YKmTrXEJ5buwG6D0YMTAJh/0pADBH1tSR3jsxLJL6zmvQ3lXDolm/qWAJdNzWFCdgKFVU20+IJcOX0ovzhvLMU1LXxkvTDmTx/ClvJGzpuYyT1vbthHzNPj3fx3ZTE1zT7mTx/Kzk7C1OwLcsHxWdx5/jiafUFW7Kii2RfsEHOAVTtrafEfWEt+75bTGXOn/oj1oUUFgBb5ey4azwMfbGXJVv0C+LK4jvc3lHe8qAFOHpFGbqqX6iYfx+ck8fWThnLdU58zOMGDwy64HDa+ftIwrj8lD7vNxnsbymj1h/jZnNFcNCmb+hY/M0ekMnv0IGaP1i/9T7dVcttLa5g/fSirdtZyxbQc2gIhfvjcF6wpruPzwmqUgj0NrezP92eP4PEl29lS3khhVdM+cdfNymVcZgJ/eW8LpVZtOz3eTU2zj6EpXoIhxcTsRL4xcxhff/yzjv3a78Efn3McuWmxvL12N9//9yoAxmcl0NgWwB8IkRDj5LnvzCA51sUXu2q46fnViLBPOf3mkgksLaji80JdOR2W6iUQVNhsdLxU2/nLe1s6/q/s9KLsTUwN/Rghv7Cay/6+rGN7f1HtDpvLGjj3/sXMGT+Yv39jKgAfbirnW0/l861ZefzyAr1a3z8+2cGv39xw0OPs+P089MR4mhm/+4Cyev3A/PDMkfy/Dwu63G/DPefidTk47Q+L2FXdvE8evvbIMj7bsVco37n5VH71+nqWb69mx+/nkXfHAvLSYln009mc8aePOmplR0KM036A0I1Ij+WUkWk8vWwn3zt9BKcdl8ZVj312wL6v/mAWF/91KZmJHt656TROuGchYzMT+J/ZI/jhc18A8LtLJnLVSUN7ZFPn2me76wTg209/zvsb9+yTdn83RGfufmM9Ty4t5NnvnMSa4jrufXvTPvm9bc4Yvj97BBt31zP3gSUd++1/P+1/v3VO93/vbOLhj7bx3dOGc8e8sT3KZ0/oLKT747Lb8AVDh7wWPWH59iqufHT5PmGbfj0Hj/XCv+D/fcLakrpuP3fTfvM+VU1t7Pj9eVQ3+Zjy6/c4YUgSr/1g1gFpS2pbmGW5wnKSY1jyszP2eb56inG5RABz7l/MprK9tdE1v/oK/1q2k0cXbycrKYbXfjALl0P3Ye+obGL+o8vxWW6Ij7dUEFK6V35/v2bnpiNAvNtBQ1uA7KQYSg7R2ZMY46Suxb9PWLzbwYqfn83YX+4/hYmm/WFoagvgC4RIjnV1xF3xyDJWdBL07b+bR1sgRJMvQFqcm5omHy6HjVi3g8rGNlp8QZp8AVp8QQYneiiuaeFyS4CuOmkoXz9pKG2BECleF48s3s5zK3Zx5YlD+PpJwxiW5mVLWcMBguWwCZlJHt675XQ8TjtF1c2c+odFALz4vZl4XQ7GZsZT1+LHYbcR53bwP/9eydvrynDZbR0+3QU/OpVxWQkHvXYH48cvrOblVSXc/7VJXDxZL0JT1+JnS3lDR96umTmMey6acNBjtAWCbK9oYszgePxBxfrSOkYOimNXdTOxLgfDUr0dYnHfe1t48IOtzJ8+lN9fOvGAY20orafZp1srQ1O92ERIi3PT6g+yYXc94zITOgSvLwiFFMP/d8EB4Wt+9RUaWwM0tgU4LiO+185XUtvCQx8W8NyKXVw6JZv7rpjUEdfiC9LsC5Aa172WcX2rHxWCRK9uCe+obCLZ6yTJ6+oyfcGeBmqb/QxJ8ZKR4DmqfBhBP8Zp9QeZcNe7DE31Eud2sKa4jnPHZ/Du+vKOTrVFP51NbqqXBz8oYPn2KpZtr+rwSXpddq6YpufEb/dhtvPNk3Np9gV4Ib94n/D/N38yI9Lj+NUb67nu5FxGZcSxeEslf164mSZfkInZiawt0W6MG04bTlaih5NHpnFcRjyvflHC4q0VlNe3MiErkSSvi8xET4dIdUVxTTOfbK1kTGYC5fWtnDt+cI+v0z8+2cHu2ha+c9rwfR6KxrYAz322i+tm5eKw7x249d6Gcj7ctIdhqV4qG9oIhBQXTcpi8tC9a+5+XlhNY2ugo09gf3ZWNfH0pzsJKUVOcgxOu41rZg7rXg1r5zKISYZBYyAUoiWg+NfyQq6blYfTvu8As1e+KOaDjXv48xUn4Hb0joi2BYI8tbSQa0/OxeOwQX0JxA0Gey94WlvrQIXAk6R7fltqoTgfMo+HuC6upVJQVQCxafqaWHyytZI1JbVkxHv45/KdfO+04czdv0O8sgACrTD44C86AOp363MXLoGKLXDi9RBo07bGDwYRyutbeXJpIdfMHEZWUsy++1ds0ek8PX9ZAxAMQF0RJA0FW9+9CI9a0EVkDno1GTvwuFLq3v3ihwJPo+estgO3K6UOfPV2wgj6Xt5ZV8b3nlnJ49dM4+SRqYz75d757L82bQj/yS/i398+iYZWP997ZhUJHgeThybjcth4b0M5HqeNTb+eC8DLq4q5580N1Db7efK6EznD8mN+sLGcO//5LqUqhQtPyObB+ZO7tOXTbZX87MU13HfpGDYX7mJTg4ffjiqA7Ysg73RIyIKsyeCKhZpC8KbCruXw2d/B4YGcE/Vv6AxoqoCSlZA+BlJHdHm+Dpqq9DGdR1B7UQqCPnAcQb+Dv0Xb3bBb/7U7IeCDpj2QOhK2vAtFnwFKh594PVTvgPwndL6Th+kHOGuyzq/dCamjoHEPLPqNPkf6GKjYBGmjwdeor+Hpt8PIs+DL5yBpGCRmw5I/w7TrIeiHPeuh4ANoa4DmKhg8Ec79LZR+oa95yghwx4G/FZY9BGLT13jTAkjJgxPmQ3wmrHwSGsth1b90nmJSYM7vobkaandC2Tpwxuh8ZU3WggbQWg/V26FsLbTUQFu9ttvmgD0bYcWjEArofUaeA4v/oPfzJMH072iBr90Jtbu07aEArH9Fpxl6MsSl6/sn91Rwx+tjDJ4IX/wT4jL0/q114PLCm7fo/cZeCGMv0LYH2+Ck72lbnrlUXyOAtOOgZqeOz5oC1dv0cQBm3Qzn3K3L/K2fws6lurwGjdV53LFYp5t4BZxwJWx9T4eveR7ciTDpKtj8lj7u1Gsh9zRoroTCT2DbIlj9jN4/91Q44+dQugo+f1zfN6f/TO+zZyO44iCp86JEPeOoBN1ajWULegX6YvSqQfOVUhs6pXkU+EIp9bCIjAMWKL0SzUExgr6Xi/+6lNVFtXxx5zkkx7q4b+FmHvywgHPGZXDneeM47Y+LmDosmZVWL/uSn53BkBQvu6qaOe2PizhjdDpPXjf90CfZsQSePl8/kDHJcMot+gF59fsw8TL9EA07GbZ/rMN8jQc/ls0Bo+fBxkMsFuWK08IUtDp/Bo2DUBAqN+ttu0uLcMoIaK3d+0DGDtLCqoJapBweXQOstDqUBk/UaUJ+cHphxv/Ax3+AnZ/AyLO1MJSthbpdUF+qa4UJ2dpmb4r+P3sKbHxDP2gVG7XAlK3RotVdYlK0EPlbtFAeFNGCscd6XLKnQl2xFtnUkdq+g2F3abEN+CBwJGOhhX2WsEwfo6959faD75KQrQXzcOfLOw28afoeCAXA5oTTbtVC32yNukkdpc/fnsdhs7TA1xXpl1htpy/YXXH6xbjn4H07B+CMBb/V13LCVfpcXz4HYtcvt8otkDhU/7990d5r4o7XZZ06Ur+cVRA8iXuF/1DYHPo+Run7Ux3Bx3Jig3l/0i/RI+BoR7lMBwqUUtutgz2PXvS185VX6AV4Qa8B2PVIfgOw17cJule8uKaFacOSO3zOxw3WfsPspBiGpMQwJaaM2l0luEmnDRc5yTFQtIKhRStYfOkwkvKy4aN79cMYP1jXpJqrtRi2N29fvkH/HTIDSvLh3f/VP9BN1M6kj4VR50BMknYbpI2CM+/UNZo9G2Hp/XvF/MTv6Acx71T9ALc16BrLmud1TXLkOfDuHVpcO1cegj4tCCq0V8xTRmjx27ZIi2/Qp+PjMvY+vL4mCJVpt8GOxbDF8uc7PFCxGQre19vuRC36AIlDdG22pRY2val/AIPGa2HZs15fp2EnAwI1O3SNevQ8XeM75WZwxOjwTW/pfY77ihYBX7Ml6KJFqbEM7G7d5E7IBkcXPlV/K3xwt67hT7hMn0OFtMhvfAOGz4ZxF2rRbC+/re/plkLqKH296ktg95danHOm6Rp/2RpIzNGuD3+zFtrMSVrQGsp0i6ClBt76iRaz7Kk6viQfytfrMnXG6pbSiLP0y3PQWF0W8VnavrYG3RJqb3G1NerWjd2lWyuzfqRbEelj9H6gr1Fdkc5nW73Of3yGLsulD0LGeFjyJ2iugfP+rNPFZ+naef1unaf29GVr9X2zeQF8+qDO41cf19cL4OQfabdO3CCd3mV1qPpb4OkLoXiFLueTvgcjztCVjraGvbYCbHhNX6f4LMg8QT8HYtf5yhivhf/l7+jy9qbA4ON1+mnf0uct+EC/EANt+vp6U+DD31jbJ+h7JPeUA++LXqA7NfTLgDlKqW9b298ATlJK3dgpTSZ6rcdk9IoxZ6suVnMXkRvQq6czdOjQqTt3dnet3sinqLqZpz8txB8MdXyckUodP3a8iBAiY8QJnPXNX8HOpQR3LGVhRRJnpFThqS2AdS91HKc6dSopXoflBughlzwKJ3xN39z5T2hBaKqA4+bom3r5w/omP+1nkH7cwY8TCuoHobvukYBPC1y7XzEUApvlQ1ZKtwbEtvfh6y5FK7R4DTsZsibpY+3ZqF8EWZO63qemEMo3aNF0eXt2PsOxRbt/vCuffZfpffpe6yzeEcjRuly6I+g/to71ZxGZiV7seIJSB2+PDBSXS2ltC1lJMfz2rQ2dPsRQ3OF4lmvdi/EEuzdXRLEMJkdZn2Onj4Ux5+ka1ju36Wbl9O9o32m7z7O5Wvtx49IhNh1Gz4WU4X2TSYPB0G8crculBL2ieTs5VlhnrkevyYhSapmIeIA09DJcA5a1xXVc8NAn3HvpxH0+fLnR/irfdbwFQWDmjTDlGu37q98NQ6bDcefqJnB8pq65ZownR0Q3b4O+fWsYY+bte9LME/oncwaD4ZijO4L+OTBKRPLQQn4lcNV+aXYBZwFPichY9CK6Fb1paCRSWNXENNmE542HCAXO5uQRs/j50PWMX/ZfXbv+9ge6ww/g7F/tu3NizoEHdMf1uc0GgyFyOaygK6UCInIj8C56SOITSqn1InIPkK+Ueh34CfCYiNyC7iD9phrAi5WuK6njrS92cP66W3jRrb+EO9m2gX/UtDC+5I860dl37xVzg8Fg6AXMh0V9wA+f+4LYdc9wr/NxANoufhzHqzdgJ6RHVlz31j4fVxgMBkN3OZQP3cyH3gdUN7Ux2/YlZSqZH2T9B/eky7Ff9xaMOR+++pgRc4PB0CeY2RaPlm2LYNXT2oXSVAnv3MYDJdtJs1exe/jl3HXJbJ1u2MnWOGeDwWDoG4ygHwkttfDO7fqrt51LdVj7Z83o4T0AmbOugvijm4jHYDAYuosR9E5UN/lIiXURDCm2VTQS3G+5E0fTbjwVaxm89Jc4G/XIzQqS+Yv/UmbYNlJKGs8Fz2akrYRvTM9i9ogDV+cxGAyGvsIIukX7/NCPXTONwsomfttp/cscqeAuxz85y7YKmyhCSvh9YD7vh6ZQq+K45eJZXDhjGADfC1cGDAbDgCeqBf3TbZX85/MivT7j0/lUNLQhgG+/Fc07L0/1nX/mk+CxcUv8h3xtUBHepl14G3ehEHaM/C5KbOweMo/JCSOZDDjtNk4ZlYbBYDCEm6gV9PWldR0r0lw8OZvFW/Z+5zQxO5GhKXvn8WgXdLfDxtljM5jc8DHfLnscKhP15ERD5sGZv2BEci4AI/svGwaDwdBtolbQz3vwk47/X7QWdxibmcC2ikb+76vH77PijDy7ikBQ6WXbti2CV/6u5z25Mb9PJ6o3GAyG3iRqBb0zb63dTU5yDAt+dApUbkU+/zW89imcdReMOoeHLsjWU6iufg5e/R4gcM2rRswNBkNEEZWCXrjfAsMPzp/M2MHxSNkaeOxMPeFV0AfPX6VdKmVr9ia2u+Ca12HYzH622mAwGI6OqBT05dv1ggnD02N56pvTGZrkgmX/Dz59SC8+8MN8LdyLfqcXYwA9J3jaKD1BfnfnVzYYDIZjiKgU9DXW4sbv3nwaztZqeHwu7F6tIy/+u17RB+D8+8JjoMFgMPQBUSfo60vrePazXaTFuXHWbIOnL9BLZJ36E5h63VEtzmowGAzHMlEn6J8WaHfLny4/Hv5zgV6156oX9KIRBoPBEMVEnaDvqm4myetkdnqTXuj33N8ZMTcYDAOCqJs+t7rZR4rXtXf191FGzA0Gw8Ag6gS9ttlHktepBT05F1JHhNskg8Fg6BeiStCbfQGWFlQxKAbYsRhGnmOWeTMYDAOGbgm6iMwRkc0iUiAitx8kzRUiskFE1ovIs71r5uFpagtw64v6A6E58dvB3wwjz+5vMwwGgyFsHLZTVETswF+Bc4Bi4HMReV0ptaFTmlHAHcAspVSNiPT7lznvrCvjrTW7sQmc592gPxzKO7W/zTAYDIaw0Z0a+nSgQCm1XSnlA54HLtovzXeAvyqlagCUUnt618zDU9nYBkD+L87Buf0DGDYLXLH9bYbBYDCEje4IejZQ1Gm72ArrzHHAcSKyVESWi8icrg4kIjeISL6I5FdUVHSV5IipafbjsttI9pVC5WbjbjEYDAOO3uoUdQCjgNnAfOAxEUnaP5FS6lGl1DSl1LT09PReOrWmpkmPbpE1L+iAMef16vENBoPhWKc7gl4CdP5ePscK60wx8LpSyq+U2gFsQQt8v1HV1Eaa1w4rn4bhZ0BKXn+e3mAwGMJOdwT9c2CUiOSJiAu4Enh9vzSvomvniEga2gWzvffMPDRf7Krh/Y17GNK4BuqLYco3+uvUBoPBcMxwWEFXSgWAG4F3gY3AC0qp9SJyj4hcaCV7F6gSkQ3AIuBWpVRVXxm9P4u3VAIwte0zPbpl1Ff669QGg8FwzNCtuVyUUguABfuF/bLT/wr4sfXrV2qbfSxYuxtQXJ20DjJOBXd8f5thMBgMYSfivxT966ICNpc3MEJK8TbuhNFzw22SwWAwhIWIF/Q9DXr8+SzHJh1ghisaDIYBSsQLenl9KwC3TgGcsZA0LLwGGQwGQ5iIaEEvqm5m+fZqLjghi/jGHZA2EmwRnSWDwWA4YiJa/a59cgUAoZCCii2QNjrMFhkMBkP4iFhBD4YU2yuaAEhz+fX48/TjwmyVwWAwhI+IFfTaZh8AJ+Ymc+uJVjbSjKAbDIaBS8QKek2zH4CrZwwjrmiJDsyaHEaLDAaDIbxEsKDrGnpKrAvWvww5J0LS0DBbZTAYDOEjYgX9uRW7ABgcKIWytTD+0jBbZDAYDOElYgW9LRACYES9HunCceeG0RqDwWAIPxEr6K2+IOOzErAVfgyJQyBleLhNMhgMhrASuYIeCJIn5bDxDcg7HUTCbZLBYDCElYgV9DENn/FQ1fV6Y4LxnxsMBkO3ps89Frmzzpq994T5MPKs8BpjMBj6Db/fT3FxMa2treE2pU/xeDzk5OTgdDq7vU/ECnojscTRBGfdFW5TDAZDP1JcXEx8fDy5ublIlLpalVJUVVVRXFxMXl73l9OMTJeLUnho5cO0qyEhM9zWGAyGfqS1tZXU1NSoFXMAESE1NbXHrZBuCbqIzBGRzSJSICK3HyLdV0VEici0HlnRU/zNOAgScJmViQyGgUg0i3k7R5LHwwq6iNiBvwJzgXHAfBEZ10W6eOAm4LMeW9FTWusB8Dvi+vxUBoPBECl0p4Y+HShQSm1XSvmA54GLukj3a+D/gL7vqWjTgh5wmhq6wWDoX2pra/nb3/7W4/3mzZtHbW1t7xvUie4IejZQ1Gm72ArrQESmAEOUUm8d6kAicoOI5ItIfkVFRY+N7aC9hu40NXSDwdC/HEzQA4HAIfdbsGABSUlJfWSV5qhHuYiIDbgP+Obh0iqlHgUeBZg2bZo60nMqfzMCKIf3SA9hMBiigLvfWM+G0vpePea4rATuumD8QeNvv/12tm3bxqRJk3A6nXg8HpKTk9m0aRNbtmzh4osvpqioiNbWVm666SZuuOEGAHJzc8nPz6exsZG5c+dyyimn8Omnn5Kdnc1rr71GTEzMUdvenRp6CTCk03aOFdZOPDAB+EhECoEZwOt92TEaDOipc8UWsaMuDQZDhHLvvfcyYsQIVq9ezR//+EdWrVrFAw88wJYtWwB44oknWLlyJfn5+Tz44INUVVUdcIytW7fygx/8gPXr15OUlMRLL73UK7Z1RxE/B0aJSB5ayK8ErmqPVErVAWnt2yLyEfBTpVR+r1jYBaF2QXe4+uoUBoMhAjhUTbq/mD59+j5jxR988EFeeeUVAIqKiti6dSupqan77JOXl8ekSZMAmDp1KoWFhb1iy2EFXSkVEJEbgXcBO/CEUmq9iNwD5CulXu8VS3pA0N8GgNi7/wWVwWAw9AWxsbEd/3/00Ue8//77LFu2DK/Xy+zZs7scS+52uzv+t9vttLS09Iot3fJZKKUWAAv2C/vlQdLOPnqzDk0oqBe3EIcRdIPB0L/Ex8fT0NDQZVxdXR3Jycl4vV42bdrE8uXL+9W2iHRCd/jQTQ3dYDD0M6mpqcyaNYsJEyYQExNDRkZGR9ycOXP4+9//ztixYxk9ejQzZszoV9siUtCVJeg240M3GAxh4Nlnn+0y3O128/bbb3cZ1+4nT0tLY926dR3hP/3pT3vNroicyyUYsFwupoZuMBgMHUSkoKugrqHbjQ/dYDAYOohIQQ+119CNy8VgMBg6iEhBb6+h24zLxWAwGDqISEEPdbhcTA3dYDAY2olIQVdBPQmOzfjQDQaDoYOIFPS6hkZCSnAYQTcYDP3MkU6fC3D//ffT3NzcyxbtJSIFvbKuCT92cpKPfnYyg8Fg6AnHsqBH5IdFhPz4xcnwdDMfusEwoHn7dihb27vHHDwR5t570OjO0+eec845DBo0iBdeeIG2tjYuueQS7r77bpqamrjiiisoLi4mGAxy5513Ul5eTmlpKWeccQZpaWksWrSod+0mQgXdHmimFQ9Gzg0GQ39z7733sm7dOlavXs3ChQt58cUXWbFiBUopLrzwQhYvXkxFRQVZWVm89ZZe86euro7ExETuu+8+Fi1aRFpa2mHOcmREpqAHW/GJ+/AJDQZDdHOImnR/sHDhQhYuXMjkyZMBaGxsZOvWrZx66qn85Cc/4bbbbuP888/n1FNP7Rd7IlPQAy202TzhNsNgMAxwlFLccccdfPe73z0gbtWqVSxYsIBf/OIXnHXWWfzyl11OUNurRGSnqCPUgt8IusFgCAOdp88999xzeeKJJ2hsbASgpKSEPXv2UFpaitfr5eqrr+bWW29l1apVB+zbF0RkDd0ZbMVvN4JuMBj6n87T586dO5errrqKmTNnAhAXF8czzzxDQUEBt956KzabDafTycMPPwzADTfcwJw5c8jKyuqTTlFR6ojXaj4qpk2bpvLze75K3ZdFtdgfO51gXCYn/OydPrDMYDAcy2zcuJGxY8eG24x+oau8ishKpVSXazZHnMtl5c4a4mihGdMpajAYDJ3plqCLyBwR2SwiBSJyexfxPxaRDSKyRkQ+EJFhvW+q5vjseDKlGkdSTl+dwmAwGCKSwwq6iNiBvwJzgXHAfBEZt1+yL4BpSqnjgReBP/S2oe1MS/XjFj9TTjihr05hMBiOccLlKu5PjiSP3amhTwcKlFLblVI+4Hngov1OvEgp1f4963Kg76rPu78EwJ6x/zvFYDAMBDweD1VVVVEt6kopqqqq8Hh6NvijO6NcsoGiTtvFwEmHSH890OWieiJyA3ADwNChQ7tp4n6UrQWbA7KmHNn+BoMhosnJyaG4uJiKiopwm9KneDwecnJ6Vjfu1WGLInI1MA04vat4pdSjwKOgR7kc0UlO+ylM+Qa4vEdqpsFgiGCcTid5eXnhNuOYpDuCXgIM6bSdY4Xtg4icDfwcOF0p1dY75nWBCMQP7rPDGwwGQ6TSHR/658AoEckTERdwJfB65wQiMhl4BLhQKbWn9800GAwGw+E4rKArpQLAjcC7wEbgBaXUehG5R0QutJL9EYgD/isiq0Xk9YMczmAwGAx9RNi+FBWRCmDnEe6eBlT2ojmRgMnzwMDkeWBwNHkeppRK7yoibIJ+NIhI/sE+fY1WTJ4HBibPA4O+ynPEffpvMBgMhq4xgm4wGAxRQqQK+qPhNiAMmDwPDEyeBwZ9kueI9KEbDAaD4UAitYZuMBgMhv0wgm4wGAxRQsQJ+uHmZo9URGSIiCyy5pVfLyI3WeEpIvKeiGy1/iZb4SIiD1rXYY2IRORsZSJiF5EvRORNaztPRD6z8vUf6+tkRMRtbRdY8blhNfwIEZEkEXlRRDaJyEYRmTkAyvgW655eJyLPiYgnGstZRJ4QkT0isq5TWI/LVkSutdJvFZFre2JDRAl6N+dmj1QCwE+UUuOAGcAPrLzdDnyglBoFfGBtg74Go6zfDcDD/W9yr3AT+gvkdv4P+ItSaiRQg569E+tvjRX+FytdJPIA8I5SagxwAjrvUVvGIpIN/Ai9XsIEwI6ePiQay/kpYM5+YT0qWxFJAe5Cz2g7Hbir/SXQLZRSEfMDZgLvdtq+A7gj3Hb1UV5fA84BNgOZVlgmsNn6/xFgfqf0Heki5Yee6O0D4EzgTUDQX8859i9v9NQTM63/HVY6CXceepjfRGDH/nZHeRm3T7+dYpXbm8C50VrOQC6w7kjLFpgPPNIpfJ90h/tFVA2drudmzw6TLX2G1cycDHwGZCildltRZUCG9X80XIv7gZ8BIWs7FahVev4g2DdPHfm14uus9JFEHlABPGm5mR4XkViiuIyVUiXAn4BdwG50ua0kusu5Mz0t26Mq80gT9KhHROKAl4CblVL1neOUfmVHxThTETkf2KOUWhluW/oRBzAFeFgpNRloYm8THIiuMgaw3AUXoV9mWUAsB7olBgT9UbaRJujdmps9UhERJ1rM/62UetkKLheRTCs+E2ifnjjSr8Us4EIRKUQva3gm2r+cJCLt8/R3zlNHfq34RKCqPw3uBYqBYqXUZ9b2i2iBj9YyBjgb2KGUqlBK+YGX0WUfzeXcmZ6W7VGVeaQJ+mHnZo9URESAfwAblVL3dYp6HWjv6b4W7VtvD7/G6i2fAdR1atod8yil7lBK5SilctHl+KFS6uvAIuAyK9n++W2/DpdZ6SOqJquUKgOKRGS0FXQWsIEoLWOLXcAMEfFa93h7nqO2nPejp2X7LvAVEUm2WjdfscK6R7g7EY6g02EesAXYBvw83Pb0Yr5OQTfH1gCrrd88tP/wA2Ar8D6QYqUX9IifbcBa9CiCsOfjCPM+G3jT+n84sAIoAP4LuK1wj7VdYMUPD7fdR5jXSUC+Vc6vAsnRXsbA3cAmYB3wL8AdjeUMPIfuJ/CjW2PXH0nZAt+y8l8AXNcTG8yn/waDwRAlRJrLxWAwGAwHwQi6wWAwRAlG0A0GgyFKMIJuMBgMUYIRdIPBYIgSjKAbDAZDlGAE3WAwGKKE/w9ul1tjsjfa9AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss learning curves\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Cross-Entropy Loss', pad=-40)\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "\n",
    "# plot accuracy learning curves\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy', pad=-40)\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.860, Test: 0.809\n"
     ]
    }
   ],
   "source": [
    "# develop an mlp for blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "\n",
    "# one hot encode output variable\n",
    "y = to_categorical(y)\n",
    "\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=2, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=1000, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example, it first prints the performance of the final model on the train and test datasets.\n",
    "\n",
    "**Note:** Your specific results may vary, given the stochastic nature of the learning algorithm. Consider running the example a few times and compare the average performance.\n",
    "\n",
    "In this case, we can see that the model achieved about 85% accuracy on the training dataset, which we know is optimistic, and about 81% on the test dataset, which we would expect to be more realistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A line plot also shows the learning curves for the model accuracy on the train and test sets over each training epoch. We can see that training accuracy is more optimistic over the whole run, as we noted with the final scores. We can see that the model's accuracy has high variance on the training dataset compared to the test set, as we would expect. The variance in the model highlights the fact that choosing the model at the end of the run or any model from about epoch 400 is challenging as the accuracy on the training dataset has a high variance. We also see a muted version of the variance on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEFCAYAAADzHRw3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAABQ6klEQVR4nO2dd5wV1fn/38/t2/vSFtilg4A0EQQVBBSwG2OMGmuC+WmMRkPUbyzRxMQU6ze2WPO1GzRWUCwgWECKgPRelmXZ3tst5/fHmb1bWGCB3b3cu+f9et3XnTlzZuY5c2Y+55lnzpwRpRQGg8FgCH9soTbAYDAYDG2DEXSDwWCIEIygGwwGQ4RgBN1gMBgiBCPoBoPBECEYQTcYDIYIwQi6wWAwRAhG0A1HhYhcJiLLRaRCRPaJyDwRmRhCe64WEb9lT+Nf91asO0lEsjvCztYgIjtFZGqo7TCEH0bQDUeMiNwKPAr8GegC9AKeBM4/SH5HB5n2rVIqttkvpy023IFlMBiOGiPohiNCRBKA+4EblVLvKKUqlVJepdQHSqnZVp4/iMgcEXlFRMqAq0Wku4i8LyJFIrJVRH7RaJtjLW+/TET2i8jDVrrH2kahiJSIyDIR6XKUdu8Ukd+KyBoRKRWRN63txwDzgO6NvfqjKEN9/jdFpFxEVorIiday2SLydjN7HheRx46wDG4ReVREcqzfoyLitpalisiH1nEqEpHFImKzlt0uInstuzaJyJSjOYaG4x8j6IYjZTzgAf57mHznA3OAROBV4A0gG+gOXAz8WUTOsPI+BjymlIoH+gJvWelXAQlATyAF+CVQfQy2XwJMB7KA4cDVSqlKYAaQ04JXfyRlqM//HyAZeA14V0ScwCvAdBFJhKC3fynwf0do/++BccAI4ERgLHCXtew2y7Y09F3T/wBKRAYCvwJOUkrFAWcBO49wv4YwwQi64UhJAQqUUr7D5PtWKfWuUioApAITgNuVUjVKqVXAc8CVVl4v0E9EUpVSFUqpJY3SU4B+Sim/UmqFUqrsEPscZ3mo9b9tzZY/rpTKUUoVAR+ghbGtygCwQik1RynlBR5GN3zjlFL7gEXAj61809HHcMVh9t+cy4H7lVJ5Sql84D7gZ9YyL9AN6G3dMS1WeqAmP+AGhoiIUym1UynV/LgYIgQj6IYjpRBIbUVMeU+j6e5AkVKqvFHaLqCHNX0dMADYaIVVzrHSXwY+Ad6wQgx/ExGniJzaKDyyrtE2lyilEhv9+jazKbfRdBUQ24ZlaJLfagTqvXmAfwNXWNNXWGU7Urpb+2y8//rt/x3YCswXke0icodlx1bgFuAPQJ6IvNGaB8WG8MQIuuFI+RaoBS44TL7Gw3jmAMkiEtcorRewF0AptUUp9VMgHfgrMEdEYixP8z6l1BDgFOAc4ErL+6wPj5zQBmU62JCjrS6DRc/6CSt+nWGtB/AuMFxEhqLL8epR2JkD9G62/xwApVS5Uuo2pVQf4Dzg1vpYuVLqNaXURGtdhT7GhgjECLrhiFBKlQL3AE+IyAUiEm15zTNE5G8HWWcP8A3wF+tB5HC0V/4KgIhcISJplldbYq0WEJHJIjJMROxAGTqsEGiHYu0HUqwHvi1yuDJYjBaRi6y7l1vQDd8Sa/0adDz+NeA7pdTuw9jktPZT/3MArwN3iUiaiKSi66H+GJ4jIv1ERIBSdKglICIDReQM6+FpDfoZRHscQ8NxgBF0wxGjlHoIuBX9QC4fHWr4FdoLPRg/BTLRHuV/gXuVUp9Zy6YD60SkAv2A9FKlVDXQFS2CZcAG4EsOHaoYLwf2Qz+pFeXZiBbL7Vbs/WAhiUOVAeA94CdAMTq2fZEVT6/n38Cww5Shnrlo8a3//QH4E7AcWAP8AKy00gD6A58BFei7qCeVUgvQ8fMHgQJ0yCkduLMV+zeEIWI+cGEwHDsi8gf0w9srDpGnF7AR6HqYh7sGw1FhPHSDoQOwYuq3Am8YMTe0FyHz0FNTU1VmZmZI9m0wtDU5OTnU1taSlZV1wDK/38+aNWtwuVz0798fl8sVAgsNkcKKFSsKlFJpLS0L2evMmZmZLF++PFS7NxgMhrBERHYdbJkJuRgMBkOEYATdYDAYIoSwE/SXv93JqD9+Sp3PdKU1GAyGxoTlkKBFlXWUVNeRHucJtSkGg6GD8Xq9ZGdnU1NTE2pT2hWPx0NGRgZOp7PV64SdoCdG6x4CJVVeI+gGQyckOzubuLg4MjMz0S/GRh5KKQoLC8nOzm6x59TBCLuQS5Il6MWVdSG2xGAwhIKamhpSUlIiVswBRISUlJQjvgsJO0FPjNa3H8VV3sPkNBgMkUoki3k9R1PGsBP0pJj6kIvx0A0Gg6ExbSLoItJTRBaIyHoRWSciN7fFdlsibeOrfOe+gdKKqvbahcFgMByUkpISnnzyySNeb+bMmZSUlLS9QY1oKw/dB9xmjVs9DrhRRIa00bab4LJDupRQU1bQHps3GAyGQ3IwQff5Dv0Rr7lz55KYmNhOVmnapJeL9YmtfdZ0uYhsQH/JZX1bbL8J0SkAeMvz23zTBoPBcDjuuOMOtm3bxogRI3A6nXg8HpKSkti4cSObN2/mggsuYM+ePdTU1HDzzTcza9YsoGG4k4qKCmbMmMHEiRP55ptv6NGjB++99x5RUVHHbFubd1sUkUxgJLC0hWWzgFkAvXr1OrodWIKuKguP0kKDwRAp3PfBOtbntO3glUO6x3PvuQf/ENaDDz7I2rVrWbVqFQsXLuTss89m7dq1we6FL7zwAsnJyVRXV3PSSSfxox/9iJSUlCbb2LJlC6+//jrPPvssl1xyCW+//TZXXHHQkZdbTZs+FBWRWOBt4JaWhghVSv1LKTVGKTUmLa3FwcIOjyXoUm0E3WAwhJ6xY8c26Sv++OOPc+KJJzJu3Dj27NnDli1bDlgnKyuLESNGADB69Gh27tzZJra0mYcuIk60mL+qlHqnrbZ7ANGpANhritptFwaDITw4lCfdUcTExASnFy5cyGeffca3335LdHQ0kyZNarEvudvtDk7b7Xaqq6vbxJa26uUiwPPABqXUw22xzYMSnQyAu66kXXdjMBgMLREXF0d5eXmLy0pLS0lKSiI6OpqNGzeyZMmSDrWtrTz0CehvKP4gIqustP9RSs1to+03YHdSY48lqq4EpVSneMHAYDAcP6SkpDBhwgSGDh1KVFQUXbp0CS6bPn06Tz/9NIMHD2bgwIGMGzeuQ21rq14uXwEdpqy1riQS68opq/GRENX6gWsMBoOhLXjttddaTHe73cybN6/FZfVx8tTUVNauXRtM/+1vf9tmdoXdm6IAPncSKZSZ8VwMBoOhEWEp6Co6mSQpp9i8/m8wGAxBwlLQJTqFJKmgxAzQZTAYDEHCUtAdsakkYzx0g8FgaEzYfeACwJXQhSippay8bd8QMxgMhnAmLD10d7x+uajODNBlMBgMQcJS0G0xWtBry8wAXQaDoWM52uFzAR599FGqqtpv6O+wFPT68Vz85XkhNsRgMHQ2jmdBD8sYOjF6YC9bpfHQDQZDx9J4+Nxp06aRnp7OW2+9RW1tLRdeeCH33XcflZWVXHLJJWRnZ+P3+7n77rvZv38/OTk5TJ48mdTUVBYsWNDmtoWnoMemA+CoMTF0g6FTM+8OyP2hbbfZdRjMePCgixsPnzt//nzmzJnDd999h1KK8847j0WLFpGfn0/37t356KOPAD3GS0JCAg8//DALFiwgNTW1bW22CM+QiysWr81NVJ0ZQtdgMISO+fPnM3/+fEaOHMmoUaPYuHEjW7ZsYdiwYXz66afcfvvtLF68mISEhA6xJzw9dBGqXSkkVJZQVecj2hWexTAYDMfIITzpjkApxZ133sn1119/wLKVK1cyd+5c7rrrLqZMmcI999zT7vaEp4cOeD2ppFJKYYV5uchgMHQcjYfPPeuss3jhhReoqKgAYO/eveTl5ZGTk0N0dDRXXHEFs2fPZuXKlQes2x6ErWsbiEkjrWgL+RW19EyODrU5BoOhk9B4+NwZM2Zw2WWXMX78eABiY2N55ZVX2Lp1K7Nnz8Zms+F0OnnqqacAmDVrFtOnT6d79+7t8lBUlFJtvtHWMGbMGLV8+fKjXr/o9V/i3ziX7y/5jjNP6NqGlhkMhuOZDRs2MHjw4FCb0SG0VFYRWaGUGtNS/rD10J2JXUmgjMLytvl0k8FgMIQ7YRtDj0rsil0UFcX7Q22KwWAwHBeEraA74vVnn+pKc0NsicFg6GhCFSruSI6mjGEr6MRqQfeXGQ/dYOhMeDweCgsLI1rUlVIUFhbi8XiOaL2wjaETo98WFfP6v8HQqcjIyCA7O5v8/Mi+9j0eDxkZGUe0TvgKeqwez8VZHdmVajAYmuJ0OsnKygq1Gccl4RtyccdTZ4sips6MuGgwGAwQzoIuQoW7Kyn+Amq8/lBbYzAYDCEnfAUdqI3tTncpIL+8NtSmGAwGQ8gJa0GX+B70kEKyi83LRQaDwRDWgu5O7U2alLKvsCTUphgMBkPICWtBj0nPBKBs/67QGmIwGAzHAWEt6K7kXgDUFu4MrSEGg8FwHBDWgk6C7nSvSveG2BCDwWAIPeEt6PE9AHBWGEE3GAyG8BZ0h5sKZwpxtfsJBCJ3XAeDwWBoDeEt6EBNVFe6qnzyK0xfdIPB0LkJe0EPxGfQ3fRFNxgMhvAXdEdyL7pLIXuLq0JtisFgMISUsBf0mC59iZZaivZnh9oUg8FgCClhL+juLgMA8OZtCrElBoPBEFrCXtBJ7Q+Ao3hbiA0xGAyG0BL+gh6fQZ24iK3YGWpLDAaDIaSEv6DbbBR7epJSsxu/6YtuMBg6MW0m6CLygojkicjattpma/Em9iWTHHYUVHb0rg0Gg+G4oS099JeA6W24vVbj7HYCmbKfLXv2hWL3BoPBcFzQZoKulFoEFLXV9o6E5AHjsImidPvyUOzeYDAYjgs6NIYuIrNEZLmILM/Pz2+z7Tp7jgHAlrOyzbZpMBgM4UaHCrpS6l9KqTFKqTFpaWltt+GYVPa7etGzZClKmQejBoOhcxL+vVwsSjImMyqwjk17ckNtisFgMISEiBH0lJHn4hYfe5bPDbUpBoPBEBLastvi68C3wEARyRaR69pq260hdfDplEg8CVve6cjdGgwGw3GDo602pJT6aVtt66hwuNjc5RxG7nuT4rxsktIzQmqOwWAwdDQRE3IBSDl9FnYC5H7051CbYjAYDB1ORAl6n0EjmOuezoBdrxPIWR1qcwwGg6FDiShBFxECk++mSMVS8+oVUJEXapMMBoOhw4goQQeYOXYwd3t+j61yP+rFmVBqPnxhMBg6BxEn6A67jXPPPp+f1d5OXUkOPH8WZK8ItVkGg8HQ7kScoAPMHNaV9GGTuaj699R4ffD8VPj0HqitCLVpBoPB0G5EpKCLCH+/eDiO7icyseLP7OtzMXz9GPxzDKx4CXy1oTbRYDAY2pyIFHSAaJeD568+idSUNCasv4C3TnyeQHwP+OBmeGSo9tgLtobaTIPBYGgzJFSDWY0ZM0YtX97+w91W1fm4850feG9VDn1To3lsbClD974Fmz8G5Ydep8Dgc6DXOOh6Itjb7F0rg8FgaHNEZIVSakyLyyJd0OtZsDGPP3ywjl2FVYzrk8wtJ8dzctl8ZNWrULhFZ3LGQMYY6DUeMifqfyPwBoPhOMIIukWN188rS3bx7OLt7C+rZWiPeH4+sQ9n9vQTnbscdn+rf7lrAQVRSTBwJgw+F/pMBqenQ+01GAyG5hhBb0atz8+73+/l6S+3s6OgErfDxqSBaZw9vDunD0gjQapgxyLY8CFsmge1pdp77z8VBp0D/aZCdHJIbDcYDJ0bI+gHwR9QLN9ZxLy1ucz9YR955bXYbcKoXolMGpjO6QPSGJLuwbbrK9jwAWyaCxX7Qew6HDPgLIhJhapCqC6GnuOg3xSw2UNaLoPBELkYQW8F/oDi+93FLNyUz8LNeazdWwZAUrSTU/qmcv6I7kwakIpr/yrttW+aB3nrGm1BAAWJvWDMtTDqKuPFGwyGNscI+lGQV17Dos0FLN1eyBcb8yisrCPe42DyoHTOGJTOhH6ppAaKwFcN0SngiNIe/LLnYOdicHggfQgk9AB3PLhiwRUD7ljoMRp6ngzOqFAX02AwhBlG0I8Rrz/AV1sL+HD1PhZsyqOosg6AgV3iOKVfCpMHpjOiVyLxHqdeYf86+P4V/V+xX7+hWlcOdZUQ8Ok8djf0GAWJvSEhQwt/fAbEd9ei7/eC3QXxPUxPG4PheCLg19eyt0r/11WCv05fs/5afb0nZEBUItisa7emFLKXw9bPIK4rnHipduyOAiPobYg/oFiTXcI32wr5dlsh3+0sos4XAKBPWgwnZyVzSt9URvRMJC3OjcfZLJ5eU6Z70uxYBHtX6sHDyvbqPvEHI6EXdBsOmadqr76mRMfs3fH6IW3agPYrsMHQnEAAasu0SNWU6H9frXZW6n9K6fPTGaV7h/nqGoQv4NXiF5WknzdVFULJHn0dgL6btTtBBbR4qoC+Pvw+qMjV6zk8UFUEVQWQlKlF0hGl911Xoa+zkl2WAyXaDocHYtPBk6jt8NVou5v817Sc7ogCsen9V+Yf/bGLz9DX7jkPa1E/CoygtyNVdT6W7yxmTXYJq/aUsHRHEeU12gu3CZzQPYGxWcmMzUpmTO8kUmLdB26k/kQt2wdl2VBXBQ63PjHL9kHhVshepk/QemxOfWEAxKRBbBfwJOiLyBMP7jgd4gn49YPbmHR9Utudetru0BemCuiQkc0OZTlQshuSekPX4fpOIRLx1UJ1CThc+uIGEGlYrpS+aPM36rqI6wpx3fRxtNn1+jWWoNWV63WcMdoj8yTqY+yt1h5cveiJTXtrrhhd38oPOd/rBj2xt67vqkL9K8/VQqUC2halrHpKtuy10rSx1rTSIlW/X2+1tt1bpYUvKgG6jdBCXFVk/Qq1bYi2ze7Q/zaHPr9sdmu7XsuWAHhr9DZoB93wJGhb6iq0EIvN+tmt42fXglzfgEQl63UKNmvPOIjo8z+hpz6uKqCPh69G16u3SmdzROnlDk/Dv9PTdN7h1vlq9TM1Yrvo88EdB85oXZ/OaJ3X7tR31a5oKN6l91PfuHnidT13H2k1Un697aPACHoH4vMHWJtTxubccnYXVbFsZxHf7ykJevHJMS76pcXSNz2Gvmmx9E2PpV9aLD0So7DZ5NAbL91LsH+8MxrK98HGjyB3DVQW6hO9thRqy61fhb4QfNVHURLRF4szWp+YteXac3LH6RO2/oJXSp+c3mq9f2e0dftZq4XA4YbYrlo8o5J1SAl0/vJc7V15EvQ2VECLUFUBVBZoT8ZXo5c73A3CRf2f0he28uvQlNigugiqivW/zalF2F/XIAKuGL2sHptDX3SeRH2h+mr1RV/XwkBuYtf5/YcbC0g4JsGLStKNtM2htyXWCB1VBboeEKsBkobdIVpQgiITpaed0fr4leyC4p16OjpF10V0st4XqqGRCfh0vQX8DeFBh6vBDodHi5MnsaEB8yTo/dnsDccIpc8/X7Wua4e7wfO2u3Seek83OkWHKNxxR3e8lNXo+Kr1dp3RTRvo5vi9Ot+h8hzHGEEPMbU+Pz9kl7JqTwnb8ivYmlfBtvzKYCwewOO0kZUaS7/0WPqmxVj/sWSlxhwYtjliAyqgMk/f9vpqtFgGfNYFKHpeKX2BJ/eBwm2wb7UWEG+VdREn6Fvm2ooGD6rei7LZ9UXqjtPCUH/R2p06b1WRFsryfVqkQV9QsV2geIe+4Ou9MWc0xKRAdKq2x+HWYuy37kaCYmZN14tNWU6DFxtlCZW/Tou33dXgOddVauGOTtbeZlWBFv7qYm1f/W15Yi9IG6TLVJ6rl5Xv03Z4EhruhtxxgNINUU2J9vz9dbrRc8XqX1SiNtdvhR3sLm1r12Haayvbq49PdIr1gN11bPVtiGiMoB+nFFXWNQh8XgVb8yvYll9BdnF18I5aBHomRdMvPZY+qTH0SokmLdZNapybtFg3GUlROOwRO8aawWBoxqEE3XSfCCHJMS6SY5I5KbNpf/XqOj87CiobefP6/+utBdRaoZt6XA4babFuUmJdJEW7SIlxkRzjIilGT9f/J1u/eI/z8KEdg8EQlhhBPw6JctkZ0j2eId3jm6QHAoqCyloKyusoqKglt6yGbXkV5JfXUlhZR3FVHVvzKiiqrKPa23KvGbtNSIp2BgU++Iu2/mPdDdMxLpJinLgd5s1XgyEcMIIeRthsQnqch/S4ww8SVl3np6iqjuLKOgor6yiqrKWo0tvsv45NueUUVdZRUu3lYNG3OLdDe/qxLuwi2GxCWpybOLc+fdLjPXSJd+N22PE4bcS4HMG4v8MuJEY5SYx2ER/lwGGzYTd3CAZDu2AEPUKJctnp4YqiR2Lr3kb1BxQlVXUUVTb8CisbNwj65/UH8AcUG3LKKK/VvSAKKmoP2hi0hMthI97jJMZtx2ETRASHTfA4dYPgcdrxWI2Dy2Ej0QoleZx2XA4bLrtOdzlsuBv/2+0HpDttNhx2IcZtTnVD5GPOcgOgQzEpse6W+8kfBq8/QGFFHXW+ALU+PxW1Pqq9fgTBFwhQXOWlpKqO8hofPr+iyuujrNpHVZ2eB6jzB6jx+qn1BiiqrNPTvoCer6oLdvs8WtwOG9EuO1FOOwEFfqVQChKiHES5dAMS7Xbgsgu1vgDVdX6qvX5cDhu9kqOJdTtw2vXdhcMuOK07DaddsNtsOO26UbLbbThtYi3TjYnDJthECChFUaWXvSVVDOmWQK/kaGw2feztohu2hmkr3Sb1nROx2fS2HHYbDmv7rbnbCQQUCsydUSfACLrhmHHabXRNaL+x4pVSVNb5qfX6qfNrka/zB6wGRP/Xz+tpfzCP16+o8wUoqaqjyhLp+rARQFm1lxqvn6o6P6XVXry+AC5L/OOjnNT6/KzYVUyN14/Xr/AHFF5/AF9AT4caEbTIW3ciTXrAWxPVXj++gCLGZSfWo8NhPr/C47RhE93YiKCnbVjzels1Xj9J0S7iPA4U9e8z6cZQz6tgev18cPdK7zve46B3SgyVtT6cDhvpcW7rfSm9bkAp/AG9L4B4jwOPy47LariqvQGinDb8Clx2XXeCYLdBYWUdcR4nNoGAgminHbfThlJ6u/X/+eW1VNb56ZUcTYzLHmw8baIbSpvohrTJvE1/n9huHaP6YxNcT4RAo+MRUPpO124TXA5tu83K63bYKayoZfmuYromeBjWI4HUo3CeDocRdMNxj4gQ63YQe5yFTZRS+AIKn1/hCwTw+RXegA5J+fwqGJ7y+hUBpS90j9NO90QP63PKKKioI6AUgYDCr3QDoSxR8DdK1/vSwlS/L6+/6bTf+q+nvqu+ILidNpx2G5W1PipqfFTW+XDZbdT6AyilCAT0tgOWyAaCQgupsS6KKusoLqpDrI2KtX0RvX39rxODyyw73A47+0prWLazmFi3g2qvbjjrqRdGmwhRLjsBpaio9R1RCC8cuf/8E7hyfGabb/f4ukIMhjBCRIdc9PPfI+sJNLJXUrvYdLyjLK9ZNwgth4ACAaXvuPwBvL4AHqedaq8fh02o8weCjZs/oEiMdlFe4w3eZVTW+vH5A8Ht26xGxuWwkRLrIqekhhqv32pIre0ohbLuEuob2IA69LL6dNANEhD03P0BqPP78fmthjGgqPH6iXY5GNU7kaJKLz2T22ekVSPoBoOhwxBLeA+FzSZ4bPYmb0gf6qF2kzu3w4wekJUa0xozwxbziqHBYDBECEbQDQaDIUII2VguIpIP7DpsxpZJBQra0JxwwJS5c2DK3Dk4ljL3VkqltbQgZIJ+LIjI8oMNThOpmDJ3DkyZOwftVWYTcjEYDIYIwQi6wWAwRAjhKuj/CrUBIcCUuXNgytw5aJcyh2UM3WAwGAwHEq4eusFgMBiaYQTdYDAYIoSwE3QRmS4im0Rkq4jcEWp72goR6SkiC0RkvYisE5GbrfRkEflURLZY/0lWuojI49ZxWCMio0JbgqNDROwi8r2IfGjNZ4nIUqtcb4qIy0p3W/NbreWZITX8KBGRRBGZIyIbRWSDiIzvBHX8G+ucXisir4uIJxLrWUReEJE8EVnbKO2I61ZErrLybxGRq47EhrASdBGxA08AM4AhwE9FZEhorWozfMBtSqkhwDjgRqtsdwCfK6X6A59b86CPQX/rNwt4quNNbhNuBjY0mv8r8IhSqh9QDFxnpV8HFFvpccBmEWn78Ufbn8eAj5VSg4AT0WWP2DoWkR7Ar4ExSqmh6FHMLqV19fyIlS9ceAmY3iztiOpWRJKBe4GTgbHAvfWNQKtQ1qhh4fADxgOfNJq/E7gz1Ha1U1nfA6YBm4BuVlo3YJM1/Qzw00b5g/nC5QdkWCf5GcCH6FFXCwBH8/oGPrHmMwE/EAB+3IG2OtpgGwnADqzOCC3VXQTWcQ9gD5CMHgzwQ+Csw9Vz/TG38kkobD/K8mYCa4+2boGfAs80Sm+S73C/sPLQaTg56sm20iIK6zZzJLAU6KKU2mctygW6WNORcCweBX6HFmeAFKBEKeWz5huXqb68VwJLgDLg5/UbskJW74hIvogUisg/Gy37hRXeKLdCWqOsdCUi/Rrle0lE/mRNTxKRbBG5XURygRdFJElEPrT2UWxNZzRaP1lEXhSRHGv5u1b6WhE5F8gC8oGXRMQnIv8VkRgiuI6VUnuBfwC7gX1AKbCCw9cz1vJS9HkRrhxp3R5TnYeboEc8IhILvA3copQqa7xM6SY7IvqZisg5QJ5SasURrnol8CpQDpwhIl2sUNyH6LGBMtEXwBvWfn4M/MFaLx44Dyhs5b66oj3L3ujbYhvwojXfC6gG/tko/8tANHACkI4OGQD8H3AF2uMcBaxFh1p203ALDkRWHQNY4YLz0Y1ZdyCGA8MSnYKOqNtwE/S9QM9G8xlWWkQgIk60mL+qlHrHSt4vIt2s5d2APCs93I/FBOA8EdmJFt8z0PHlRBGpH+C6cZn2AmejxfRtIArYBlyGjjV2B2YrpSqVUjVKqa+s9X4O/E0ptUxptiqlWjsoXAC4VylVq5SqVkoVKqXeVkpVKaXKgQeA0yFYNzOAXyqlipVSXqXUl9Z2XgFmAiVoj+tktPjPQQt8pNYxwFRgh1IqXynlBd5B1/2h6rkngLU8gdY3wMcjR1q3x1Tn4Sboy4D+1hNyF/rhyvshtqlNEBEBngc2KKUebrTofaD+SfdV6Nh6ffqV1tPycUBpo1u74x6l1J1KqQylVCa6Hr9QSl0OLAAutrI1L+/NwHxgMvAF8JqVpyewq9EtfGN6ooX/aMhXStXUz4hItIg8IyK7RKQMWIQWJru1nyKlVHELZc0BvgZOBXLQ4v4qMAVYT4TWscVuYJx17ISGMh+qnuuPxcXo8yKc71iOtG4/Ac60wntJwJlWWusI9UOEo3joMBPYjL5Ifx9qe9qwXBPRt2NrgFXWbyY6fvg5sAX4DEi28gu6x8824Ad0L4KQl+Moyz4J+NCa7gN8B2wF/gO4rfREwIv2muvQsehi65idjvZ8DnhwaV0MNx9kv5XA8EbzHwN/amRTdrP8dwMLga7W/Ahr/w70A60AkHiQff3Uqsf70fH/NcC7QFKk1zFwH7ARHWp6GXAfop491vxWa3mfUNt/BOV8Hf2cwIu+E7vuaOoWuNYq/1bgmiOyIdQHwfzMrzU/SxCL0LHrro1+i9Cx6tXoh28xlihMsNb7Mfoh02jrIuqHHk8atNf8ILor3XR0TPxQgv43YJ61/WTgv/WCbi3/CH3XkAQ4gdMarRuFboDWAleG+niaX2T+wi3kYui8XAW8qJTarZTKrf+hH0r+FDgXLda70d7RTwCUUv9Bx7pfQz9IfRctxqBDOOeiY9uXW8sOxaNoYS5A97T5uNnyn6G9s43oO4Zb6hcoparRsf8sdBzZYGhzzOBcBkMHISL3AAOUUleE2hZDZHLwT2kbDIY2w3oD8Dq0F28wtAsh89BTU1NVZmZmSPZtMHQk+fn5ZGdnk5ycTO/evUNtjiHMWbFiRYE6yDdFQ+ahZ2Zmsnz58lDt3mAwGMISETnoexTmoajBYDBECEbQDQYD5TVe9hRVtSrvhn1lRFpniu35FWzPr6DG62d7fkUwfdWeEjbmlh1izdazp6iK8hpvm2zrYBhBNxgMXPXCd5z6twWHFeoFG/OY8dhi/vt9uI1AcGimP7aYMx76knveW8sZD31JabWX7OIqLnjia6Y/upgar/+Ytq+U4tS/LeCK579rI4tbxvRyMRyUF7/ewfb8Sn5xah96pUR36L5X7ynhzeV7yEiKIspp5+pTMtFvjreO3YVV/PvbnVx/eh/e/X4vFbV+fjO1/xFtQynFC1/v5Oxh3eia4Dls/u35FXyzrZArxrX84PO9VXvJTInhxJ6JzPthH2lxbsZkJvPx2ly+3JxPRa2PKYPSuWBky4PrLdiYxw97S7npjH5NyqGU4qkvt+H1KbrEu9leUMmt0wbgcdp5ecku9hZXc9MZ/Xj8iy3klNRQWu3FJtAtIYr4KAe3TRvIyt0lAHy5OZ8u8R5eWbKLgILT+qfiDSjyy2v57/fZwX0+tXAbF43KaG5iEz5ck0P3xCjW7Cnh1AFp/GXuBmLcDnolR3PL1AHYbcLTX27j660FjOyZSEK0i7Q4N+ed2B2Ab7YWUO31M2Vwl0PuJ6+8hicXbKPWF2iS3jslml+e3jc4X1rl5d731+Kw2zg5K5mVu0tIjHZyy9T+1FnrvrVcl/GOt9c02d5v/7OaOI+TjKQoxvVJZs6KvZw+II3pQ7sCkF1cxWfr93OVdZ5+vHYfT325nSHd4gHYsr8c0Od1e2IE3dAi5TVe7vtgPQBxHge/mz6oQ/f/zKJtzP0hNzg/bUgXMpJa36g8s2gbry7dTWKUk4c+3QzAj0b1oHdKTKu3sWFfOX/8cD2Lt+Tz0jVjD5v/iueWklNaw8xh3UiOcTVZFggobn5jFQA7/jKT//fqSgB2Png2j3y6mU3WBf/B6hzOH9G9xYbn2n8vQyk4Z3g3+qTFBtNzy2r428ebmuQd3zeF8X1SuPtd/fGcOI+DZ77c3iRPUrST4iovpw9IIzXWRUFFHR+vzUUpmLMyG6ddWLQ5n70l1cF14j1aMnYXVREIKGy2lhtIpRS/eu37gx6rM4d0pVdKNA/O2wjA4i0FwWX1gn7Zc0uDx+hQfLh6Hy99s5PUWDf1h63G66e8xsdFI3uQHq8b40/W5/LuqhwA5qzIxmETfAHF2KzkA7a5fJcekqdrvAe7TVi6oyi4zRE9E1m1p4TvdxcHBf2WN1axfFcxkwel0zslhl++out3d2El/oCirKZhmKFDHbdjxQh6GFHj9XPx09+wv6yWGyb15ZoJWQfNW1Hr4/LnlvKHc4cwsteBHzwpqKjlkme+pbLWx93nDOGc4d3xBxQ/eeZbqr1+JvZPDeZ9cuE2zjqhKyf2TDxgOx+szuFPH63ntjMHcsmYngcsb86zi7bzwNwNvHTNSUwamH7QfBtzy5vMT/zrAmLdDipqfaTFNf1Q0aCucYzuncSn6/cz55enEOWys9uKB3+1tUEoCipqDynop/zlc3JKa3DaBa9fkRDlBPRxv+6lZcRHOXnkJyP41WsrWbqjiPzy2uC6MS47lXX6tnzKQwtx2HU0UwCXw9bkln3Mnz5rst+Citom8/nltUERqsfnD1AfDdmUWx4U9F+9trKJWNTz69e+x+VoiKgu3pJ/QJ5PfnMaYx/4nOtfXkGVZfs7K3Uo5eSsZCb0S+XvnzRtKJ64fBQ5JdXc/vYPnPq3BXx662lEu5rKSK3Pz/RHFx+wv8ZszC0jymVvcdmMxxbz3xtOCc6f8pfPef7qk/jdnDXcMWMQ6XFubnlzFf937VhSYt1szC0jNdbF8rumBtf5ZlsBlz27lJ/8awkJUU7yy2ubNEygG453vt/LNS8uA+DJy0dxg9XQLvv9VJqzfGcRFz/9LassL3tjbjmn/u0LvD5Fbpkew+3y55by1e1nBNf59NbT2V1UxUVPfhNMG/vnz7hzxmB+NPrQdzhHgxH045jc0hoKKmoZ1DUOh93G3pJq1u7VD2he/Honkwamk5XaVKCKKusoqqxlY245q/eU8KvXvufpK0YzqFscZdVefAFFl3gPm3LL2Z5fCcA/v9jKaQPS2FNUFfRM9pXqE/RvFw/nd3PW8OXm/BYF/eN1uewvq+XrrQWHFXSlFA/M1V+bu/rFZXx26+nERzlIj/MEl2/MLaeqzsfOgkp+OrYXdhu8smQ3oBspgBMzEkiz1tmWX8HiLQVBD++t5Xv48ZgMlm4vAmDpjqLg/vPL6w5q2/KdReRYZfb6tXKeNiCND1bnsL+slh0Fejv3njuEzzbsx+NsKkb1Yg4wfWi34PSn63PJLq4mKzWGzJQYlu8qprCywY7vdhRRWFnHlEHp7CysZFt+JTmlNcRHOdmaV8HArnE47bYm62zILefUAWnkltbw4ZqGwRd1WAriPM5gYxPt0mGX+uNw4cgeTB/alWiXnfQ4D3fMGMSuQt349UqODjaE54/oTo/EqKCgT+yXSkqsi1P6plJe4+XlJbtYu7eMdTllnJSpPdw6X4BNueVsy69gR4E+t2YM7crOwiqGdo9n0/5y/nj+UH7yr2/56Id9wQZn5rCuwbsxt8PGhn1lfNSoXDmlNdz5zg/8sLeUe99fR3KMi3U5ZfxnRTbj+6Swek8pg7rGN6mPMb2T6ZEYFbSjHpvAkO7xjOmdzPkjtKADXDshiwn9UnnxmpNw21t+tDiga9wBaXuKmjYS2cXVrN1bCkBanJvUWDeJUU5unNyXYT0SWLK9iFpfgB5JUS3u41gxgn6c4g8oznp0EaXVXv5w7hCunpBFgXWR1l94k/+xkC9nT2ridZ77v1+xt6Q6eLHsLanm3H9+xe+mD+TVJbvJL69l4x+nB73CAV1i2Zhbzk2vfU90I4+pqLKOH4/O4JIxPXliwdaDPunful/3CGjuZbbEyt1NR5ad+vCXJEY7WXXPmQB8tiGPX/xfw7sJUwalM3VIF7bnV/LNtkKuPiWTl77ZyX3nD6VHor4glu0s4sdPfxtc59731/HIZ5up8zeNpwLsL6s5IA107PviRttIjXVTUFHLP348nLyymiaNwoj7PwXgtP5pzF+/P5g+uFs8G/aV8c0dZ9A9seFirfMFeHtlNheN7MFNU/rz/15Zwby1ufRPj2VLXgWXPKP3O2lQOsN7JHD+E19TUF7LAyuyeXnJLmafNZAbJ/drcjewcV8ZN766ki83N3jdPRKj+MN5J7RYvs37y1m8pQCP08bfLx4evHsAmsSYD4bLYeOVn58cnE+MdvGvn43hlAe/YOO+BkF/auE2HvlsczDfv68dy+kDDnz/pV96LAs35bNwk7Z/9lmDmL9uP76A4qNfn8rUh7/ktv+sbrJOYaUuf2qsiyVWY10frgGYNKjpflwOG3/90XCueH5pk/Tnrz6JydadYVWddhB6JEZxz7n608STD3HXGO9xkpUaw46CSqJd9uBdTT1RTjvVXj/n/K8eiv93Zw0EwGG3MfssHbJs3Ni3B0bQjzOUUry8ZBc7CioprdZdnP4ybyPTTujKny3vtndKgyf16zdW8dLVJ5FkxWzrbyvrfAGinHaeuHwkt761mg37yoPLsourgwLx9BWj+cX/LQ+KQ++UaB6/dCSFlbWM7qUv1EFd45j7Qy57S6p5b9VefjKmJymxOuyRV65FclNuBY99toXLx/UiNdbNsp1FPLd4OwO6xKEUbMkrD3q+00/oysfrtEdWUuXlL/M24LLbWL6zGLtNeOaK0US77Izro7889uyVYyisqKNboocrxvUKijnAmN5JvDFrHNVef/DWuaRKH7e3rh8fFMxYt4N731/XpOHpkxbDhSMzWJOtParbpg3gR6Mz8AcUlXU+3A47j146gvF/+eKAehrYNS4o6G/MGseoXknsLqpsIuYAd58zmPNGdGesJXoPXDiMS8f2orLWF7y9B+ifHkuqFUp6Y9keNuzTDej7q3Ko8fqDXQq7JXhY0czLr6+jg/HQJSeydm8p3ROjmoh5a/juf6ZgbyHe2y3BQ7zHwdsr95JnnUvz1uaSmRLN3pJqvH5FohWyas6Tl43mtL8vAOCSMRlkpcYw/zenoYC+abHBxk4E3pyl67DeE27uEZ89vBsXj84INiqNmdAvhdd/MQ6Xw0Z6nJs9xVWMy2r4ml20y8FHv554wPOOQ/Hva8ayLb+CE3smsnpPCf6Aok9aDDsLKzmhewIzH1tMYWUd/dJjOX9Ex38t0Aj6ccb2gkrueW+dvnV2OxjYNY7lu4q54ZUVrM4uxeO08ZtpA4IhhtV7Svjv93u5dmLWAV3OLhjZnTMGdaFrvIdvGsWStxdUkF1cTZTTTlZqDFeOz+Te99cBcM0pmQeEVoZnJPLJuv1c8+J3bN5fgctu4+en9sHrD1Bc5SXGZaeospZHPtuM0yHcMKkfV73wHVV1fj5Z1+DFiuhwyV9/NJzlu4ooqKgjymnn2UUND+tOG5DG1CFNezXEuB3EuPWp2i+9qXCJSFD4pw5O57MN+oMw04Z0YWxWMgO7xHHO8G5Bz/GfC7YiQEBpe84c0pUNuWU47cIvJ/XF2UzwuiVE8Ydzh/Dgxxup8WqvP8Zl54xB6Tz/1Q7+ctGw4P6b2wbam23spSbH6Pk9RVXEeRyU1/hIj3NzQvd4PE47vVOi+WLjfkQEu03YklfOljz9PCElxsU1EzIPeAAKMGlgi2+CA5Ae5+GMQYfvpdPiuvEtryciTBnchfdW7WVNdkkw/frT+zKqVxI3vLqCrLSWn1f0Solm0sA0lu0o4nrrDqHxQ96rJ2Ry17trObV/GmOzkjm1fypfby0goGBfaTUxLjujeiexdHsRl4zp2eJdQL2N4/s2CHjP5AMfqp/QPeGwx6C57fU9viYPavDm6+2/ZkImj3y2hZvO6NfkGUZHEbKxXMaMGaM646v/SilueHUlA7vGccvUAQcsn/2f1fxnRTYf3jSRoT0SUEqRdedcAJx2YcP904NellKKMX/6jCmD0/nTBcO44vmlfNcoPHDTGf247cyB/Oz5pU16Efzjxyfy9opsqr1+3r1xAku3F/KTfy0hJcbFirunHWBTjdfPkHs+JmCdKn1SY3A77fgDATbvr+BPFwzlinG9mfDgF1TW+ega7wk+1Jw6uAu1Pj+LtxTwn1+Ob9GT6ghO/vNn7C+r5ds7z6BbQhTz1+Uy6+UV9EmLIb+8loykaObdfGpIbDMYjgQRWaGUGtPSMuOhdzAlVV7mrc1l3trcJoKulGJfaQ2r9pRgk4Zb6Mbd1y4endHklllEGNQtjq+3FrJ8V1FQzLvEuxndO4mZw3S8LtUKj4iAUrCzoJIlOwq59CT9EHNEr0R+MqYn04a03N/X47Rz67QBrMkuZf76/WwvqCQhysnJWckM6BIX9A5/PaUfn1secs/kaGq8fu4//wQCSvF/3+5iZM9EKNsHGz+EpCzoOxkq8mDbF7BvtU5P6Amn3ASZEyEq8dgPePEuiEnj39eOZd4PuXS1PM5T+qVy0cgeVNT66JcWyzlWV7mjRilY/Tqseg1G/gwGzQR3M4+9cBu89hOoLoaLnoF+B/akCBm+Wlj7Dgw4C6JD0+geM/mbIDoVYlIOnqe6GL57DtbOgTMfgC4nQMAL0SngOkyXVm8NOI/uTqejMB56B7NlfznTHlkEwPK7pgbF9okFW4M9Cm6c3Df4EAXgwie/5vvdJWz84/QDelf8ee4G/rWoaf/iJy8fFRRzgDvfWcPr3+3hxIwENuaWB1+Y+OP5J/Cz8ZlHZP+fHnqIISVfsGTgnfzt/H6w62tIPwHSW9FPvWAL/PMkgh8+T+wNJc3GGRI7KL8W/MvnQGq/A7exfSGoAJTnQnKWFtCqIi2gjkbx0B/mwNvXQa9T4Kr3wd5yTBeAXd/Ap/dClyF638N/AvHdoHw/vHAmRCXBFe+0LHa+Olj8D/jyr03Tp/4BEC0Yrli9fb8Vw3dEwS+/0uXbtxp2LAabHU64COIO/SLNQakpg+zvwFsNPcdBbKNQhFKwf50+3n3PALtL72/vCtg0Dxb9g2C9dB8Fqf1h3A1a8MQO3zwGOavgxEuhaAeMuKx1Da63Wu+7NBueOAlGXQVnP9RQF36ry6X9GHzL2nJ49wbY8D7EdoEfvwQx6QeeO8W74LHhLW/D7oZ+U6Bivy6fMxrOewziM6B0Dyx7HrZ8AtPu1w7H4di3Glb8G/Yshf1rwR0PY2fBlLuPvpwWh/LQWyXoIjId/UV2O/CcUurBZst7Af9Gf/fRDtyhlJp7qG12NkFfn1NGdnEVxVV13P72DwC8+vOTmdBP9/e+9qVlbN5fzm+mDmDq4C4kRDeIT2mVl11FlQzPSDxgu8WVdYz8o+55MWVQOheNymDqkHTcjgbhv+6lZXy+MY/ZZw1kXU4pc3/IJc7tYOnvp+g+xJWF2jsednHLXopS4K2CtW/D+y2czM5ouOUHiEk9cFk93mr4xwCoLYOz/qJP8lWv6mVjZ0HGWOh1sr44v/wrbJ4PSb3h6rlaeGrLYd1/4Ys/NYhiSyT3gfQh+rfob02XdR8JfafAST/XYl22D3Yu1sK/pdl3eKNTIeMk2DyvafqoK6G2ArJO1SKw8ytY/ZpeNnAmXPgMzLkGtjbtaw7oBuysP0O34fD0ROgyDMZcoxudoI2j4NqPwdG0rz3bv4QPf6OP9bCLYch54PdC0XbdGGxfCAWbIGAJpCsWeoyCQefqO6F178KCPzVsz+aALkNh3yo9H9sFEjK0wLeGYZfA6b+DlH4Q8DcVZL8P1rwBXzwA5TkHrjvhFjjjLi3AH/1Wr3/StbqxztugRXPwOVBTqoUQdHpyFjijtDA7o3WDFfDDizNhz5ID9zN2lm6UPAn6GG+zHm7P/Idu1P53FGSeqoV89ZuQv1E3YK4YLcSNsTl1wwww5lp9NzDpDt3Qx/fQ+9ixSJ+rXYbCo8OgrqLpNlxxMGuhbjSSsyD+6O4Kj0nQrS+abwamoT/ttQz4qVJqfaM8/wK+V0o9JSJDgLlKf839oHQ2QR/7wGfkldcGn44D3HX2YH5+ah8AJv9jIUO6x/PEZaOOeNvvvPo0J256lK79RxOTkAwzH4L9P8DmT2DUVSxevYmvP3mTWT8+l5wqO0vn/puzulWScd7dENcNHh2qN3TChTD9r/DxHbB7ScOJV7i16Q6TssATDz1P1rfq378C3UfA1R/pC645mz+Bt64CXzVM/r0WAqVgzZvQdZi+iJqz+k3476wD0xN7wUXPQlQyVBXCSzO1t57SX4ug3QU5Db1H+PX38PkfYV2zr75d+R68eSXU6h4u9J0C5/9Ti8imubDxowZxm3grpA6Ad3956Iq4YQmkD9ZlU0pf0CW7tJ2eBH2h13umK1+G939llak3XPE27PkO3rtBl3HE5ZC9TJdnyPnw6T1arAM+bWNzMk7SjUHmRCjfB8tf0HcwNSUNeVL66zup/M3a6/RWgTMGLn0V+kzSMbn966EsB1L6woYP4Is/gr8Opt6nvfMNH8D8u3VdNmbg2fr86TEK1r8Hn993oI1DLYdh5b8PcgCF4F1CPe54fQyqCnQjFJUMlXkgNt1AqwDkfK89/3Me1edUwKfDX7u+brqt1AEw5R4YfK6eryzUdxk2y/nx1TY0pNUlsPghWP8unDZbr7N3JbxyUQtm27QdzbngKd3wiQ0Kt8BTpzQ0uGc/pB2Lo+BYBX088Ael1FnW/J0ASqm/NMrzDLBdKfVXK/9DSqlTWtygRWcS9Hk/7Au+6g1wxqB01mSXUFBRF+y3PPTeT/jxmAzuPbflvsTs+hby1muRGHSOvuD2r9MnSXPs7gYv1hV7oKfQmPqTsflJ2XMcVOTq9ffr18dxx2uP5oIntGdSz8K/wsI/N8wn99EXrtj1f/2FdeJlcMGT0JrxVJSCD2/R4YzEnuDwQP9pkDa4qTdYkQfFO6Fno1fz8zfBV4/ASb+AjNEQCMBmy+td9A/Ybb21Z3dpsXZFw7gbD7ztDwSgaJsOP4D2BnN/gKRMLXrFO7SIeuIPH39tqXyf3g1bP4dJd2qPG+DNn2nPtSUuelbH3efO1vWV2l/HhJWC6X858Lj6anWj8MUDuszXL4JuJ+pltRX6bulwXmJ1sa5HT6MXd/Ysa7ircEZpz7Y5vcbD+U/oc6FsL3z3LJx8vT5OL0zXzxMGzYRJ/6Pt/uoRGH01ILDo77quEnvpxqssB2LTtYOx7Qsd4us6TN8FAPSeoJ2JxuVXSoc9XrtErzftfphw86HLejj8PljwgL5OopP1nRHoa7F4pz7WVYU6bdSVcO7jTW36+E5Y8iRknQYX/kvfJR4FxyroFwPTlVI/t+Z/BpyslPpVozzdgPnor53HAFOVUgfcu4nILGAWQK9evUbv2nXQcdojisw7Pmoyf8vU/tTU1fHh4uX8aUwNY4adwO0vzWfkpAv5+QlKe1KeeB0TfeMyfUEUNY2T03ui9rJKdul47M8/057ugge0d50xRovf4n9AXSWc/EstIHY33LRcX6jPnKa3denr+uLa/qUOg/SbCsMvadhXwBJ620G6YSmlPcgVL2mRqMfu1if06Gt0mCCjxXOwYwkEtLdud2oPL7FXqC1qSk2ZvnXPnKBv0dfO0XXVY3TTRutICAT0OZR4+KEZjprctVrU9q2CLfO1YLXmucqRoJR2atIGWfH/lXpfY2cd/EGu36s9+yMYlO2YCAT0NZncwrAcSunQYePG8SjoCEG/1drWQ5aH/jwwVKmW7kM0keyh+/wB+v1+Hr+fOZg6f+CA8TCe+dloztz1MPLdM03SA2LHpqy3z2K7gq+m6S3z1XP1BfPJ/zSknX47nH7HwcW2HqV0D4ze47XXBNrDryrS8eC2ZMdi7fnF99Bxx+a9PQwGw1FzrN0W9wKNm/YMK60x1wHTAZRS34qIB0gF8o7c3PClotbHos35wTcVH5i7ITh85i9P78vwjATyymqYPDAdmfceAGXubmzpMpM6Typj3LuwrZujH/iA9h4nPaVF2Bmje3BkTtAeyb/P1beak+5snfchAiMvb5rWUuy6LWjSQBzf3bwMhkiiNYK+DOgvIlloIb8UuKxZnt3AFOAlERmMvooPHN4twvn3NzsP8MbX7yvj6lMyuWNGo9vPvA06Pj3tj8RP+DWjG6/wo6Zee4vYnbonhMFgMDTisIKulPKJyK+AT9BdEl9QSq0TkfuB5Uqp94HbgGdF5Dfox9RXq0j7RtUhKK328uTCrTy/eEdwYKdYqnjojFhOSHfSZUiz/rBzZ+v48gkXhsZgg8EQkbSqN7/Vp3xus7R7Gk2vBya0rWnhw5MLtwY/HnBSZhInVC/nl3vvxPGNFQ//0APDfqzfwnPH6b7PE25p34dUBoOh02Fe/W8DXvxqZ3D6xsn9GPrl3wC/7iYV20V3yfr+Zf0D3Y974m9CYqvBYIhcjKAfAaXVXrz+AKmxbvwBxao9xVTU+oNjb/9u+kD9ELRgsw6nTLtfrzjuBt1X97mpgMClr7XNOCUGg8HQCCPoR8C0h78kr7yWnQ+ezafrc4PfDQQ9Zve0IV30yxzFO3WIpR4R/Qbh/+zV/VQP18XQYDAYjgKjLK1kY25ZcCD/+z9Yz5efz2Wx+xauteuxPoIfGNi/Vr9Jlnrg0LiAEXODwdBuGA+9lTT+6O1/lu/hRZ6lp+Rxj/NlopK7k5E0E7Z8Bq//RL+m3mt8CK01GAydESPoLfDXjzfy0Zp97C6qolezr5z87UfDuWSwG/6xQY9rsnMxs6sfB+/N8MX9+qWgK9+DhI7//JTBYOjcdFpBL63yEh/loM4faPLVc4/TzpwV2cFvbjpswok9ExnQJY7yGi8XJG6Dl2brjZz1gB5kaM418GdrkKOp9+kR5wwGg6GD6ZSCnldew9gHPufOGYPYll/BW8uzAbhoZA/+5+zB5JfXMjwjgTXZpdx65gDOGW6JdUU+PHmyHlFt6MXQdbj+wo4jSg8n2ncKTLwldAUzGAydmk4p6O+v0oPu/2XeRrrGexjZKxG7CIu3FvDIp/pjwr87axAxbjsj6j+YvPNrPX6K8sPFL8DQH+n06GTde2X1G3pYTIPBYAgRnVLQv9zcMMxMblkNvzy9Dw67jbveXcurS3eTGO1kWI8E/dWgRX+HL/+uxxePTtFjUvc9o+kGbfYDB74yGAyGDqZTCfrTX27j2UXbKays44xB6Tx1xSgEweXQXQkvGdMThcJhs2G3Cbx3o/4aD+gvlpxxN6QNDGEJDAaD4eB0KkF/cF7Dl1WSY1xNvrsJBIUdXy28/rOG70zelXfgNx4NBoPhOKPTvOXSfPDHhKhmX4Df850e1hb0V4K2fKI/v3bbJiPmBoMhLOg0HvreEv1R237psfx8YpZ+TR/0V3ze/3XDF73r6Xmy/kJQ8+9MGgwGw3FKp/HQ/2N1Tfz92YO5dGwvUmLdUFcF7/4/SMjQA2iJdTj6ToHL3jRibjAYwopOoVh//2QjTyzYRmqsi8kD03WiUvCfq/T02f/QH0Y+/XbY/S30mQxO8+k0g8EQXkS8oJdWe3liwTYARvZK0om15fDv8yBnJYy9XnvkoIe0HTgjNIYaDAbDMRLxIZcnF24NTmelxsD+9fDKxVrMJ94KM/7auo8sGwwGw3FORHroPn+AJduLqPH6+XKTfonot2cO4JIxPeGN6ZDzPZz9MJx0XYgtNRgMhrYjIgV90ZZ8rn1peXD+4tEZ/OqM/rB3pRbzGX83Ym4wGCKOiAy51I+U2CctBrA+PlGeC89O1j1ZTvxJKM0zGAyGdiEiBb28xgfAryb3o09qDKf2T4Olz+iFZz4AnoQQWmcwGAztQ0SGXMosQT9/RA8uGpUBu5fCVw/D4PNg/A0hts5gMBjah4jz0LfnV7CjoJI4t0MPsKWUFnOAKfeE1jiDwWBoRyLKQ8/et4/zH/+KchVNn1QdP2fFi7D5Y/0lodT+oTXQYDAY2pFWCbqITAceA+zAc0qpB1vIcwnwB0ABq5VSl7WhnYfHV0f8i6fxlauCj6d+yrghWTrU8snvoc8kmHBzh5pjMBgMHc1hBV1E7MATwDQgG1gmIu8rpdY3ytMfuBOYoJQqFpH09jL4oNRVEF+XBwI/+XwCdPkPfHgLRCXD+U+al4cMBkPE0xoPfSywVSm1HUBE3gDOB9Y3yvML4AmlVDGAUiqvrQ09LAF/0/nXfqz/r/kYEnp0uDkGg8HQ0bTmoWgPYE+j+WwrrTEDgAEi8rWILLFCNAcgIrNEZLmILM/Pz28py9ET0D1bXk//DVw+R/c37zcNeo9v2/0YDAbDcUpbPRR1AP2BSUAGsEhEhimlShpnUkr9C/gXwJgxYxRtidIeus3ugP7T4Hfb9QcqDAaDoZPQGg99L9Cz0XyGldaYbOB9pZRXKbUD2IwW+I7D8tDtDutLRFFJYHceYgWDwWCILFoj6MuA/iKSJSIu4FLg/WZ53kV754hIKjoEs73tzGwFVgzdbkTcYDB0Ug4r6EopH/Ar4BNgA/CWUmqdiNwvIudZ2T4BCkVkPbAAmK2UKmwvo1vEEnSH036YjAaDwRCZtCqGrpSaC8xtlnZPo2kF3Gr9QoLX58UJOOyuUJlgMBgMISViXv2vq6sDwO4wHrrBYOicRIygB/z6oaiYGLrBYOikRJ6g24yHbjAYOicRMzhXoP5NUVvEFMlgMLSA1+slOzubmpqaUJvSrng8HjIyMnA6Wx91iBj1U8ZDNxg6BdnZ2cTFxZGZmYlE6BhNSikKCwvJzs4mKyur1etFYMglYtoog8HQAjU1NaSkpESsmAOICCkpKUd8FxJxgo7x0A2GiCeSxbyeoyljxAg6JoZuMBg6OREj6AFrLBebPWKKZDAYjkNKSkp48sknj3i9mTNnUlJS0vYGNSK81c9XC+9cD/NuR6qLdZrN9EM3GAztx8EE3efzHXK9uXPnkpiY2E5WacI7PrHhA1jzBgCxvc4ATC8Xg6Ezcd8H61ifU9am2xzSPZ57zz3hoMvvuOMOtm3bxogRI3A6nXg8HpKSkti4cSObN2/mggsuYM+ePdTU1HDzzTcza9YsADIzM1m+fDkVFRXMmDGDiRMn8s0339CjRw/ee+89oqKijtn28PTQA36Yfze8MwvSBoMrDs/+FQCIPbzbKIPBcHzz4IMP0rdvX1atWsXf//53Vq5cyWOPPcbmzZsBeOGFF1ixYgXLly/n8ccfp7DwwHEKt2zZwo033si6detITEzk7bffbhPbwlP91r4N3zwOcd3hp6/BR7dh3/YFAPoTqAaDoTNwKE+6oxg7dmyTvuKPP/44//3vfwHYs2cPW7ZsISUlpck6WVlZjBgxAoDRo0ezc+fONrElPD30Zc9B6kD4zTpI7gOpA4KLzFguBoOhI4mJiQlOL1y4kM8++4xvv/2W1atXM3LkyBb7krvd7uC03W4/bPy9tYSnoBdsgcyJYLPMd8cHF4ktPItkMBjCg7i4OMrLy1tcVlpaSlJSEtHR0WzcuJElS5Z0qG3hF3Lxe6G6CGK7NKS544KTxkM3GAztSUpKChMmTGDo0KFERUXRpUuDFk2fPp2nn36awYMHM3DgQMaNG9ehtoWfoFfm6//YtGBSjT0GjzVtM71cDAZDO/Paa6+1mO52u5k3b16Ly+rj5KmpqaxduzaY/tvf/rbN7Aq/+ETFfv1veejPf7WD2e83fL7U5gi/NspgMBjagvBTvwrLQ49JB+DLzfnYadR/03joBoOhkxLGHroW9L3FVdTQ8B1RmxnLxWAwdFLCVtBVTBp/+nA92/IrqVMNIm4zLxYZDIZOSvip37gb4IQLyakSnvtqBwDpSXFQZS03HrrBYOikhJ+H7opmF12Z8OAXwaTTBmcEp81DUYPB0FkJP0EHNubqTv3902N54MKhDOmVHlxmNx66wWBoR452+FyARx99lKqqqsNnPErCUtALKmoBePm6k7n85N7EREcHl5k3RQ0GQ3tyPAt6q9xZEZkOPAbYgeeUUg8eJN+PgDnASUqp5W1mZTMKyusASI7RvVtiohvGUrDbIv/TVAaDwWLeHZD7Q9tus+swmNGixAFNh8+dNm0a6enpvPXWW9TW1nLhhRdy3333UVlZySWXXEJ2djZ+v5+7776b/fv3k5OTw+TJk0lNTWXBggVtazetEHTRwxc+AUwDsoFlIvK+Ump9s3xxwM3A0ja3shl7iqtIi3PjcmhvPCamwUM3gm4wGNqTBx98kLVr17Jq1Srmz5/PnDlz+O6771BKcd5557Fo0SLy8/Pp3r07H330EaDHeElISODhhx9mwYIFpKamtottrfHQxwJblVLbAUTkDeB8YH2zfH8E/grMblMLW2BjbhmDujaM3xIb1SDotk7w8ViDwWBxCE+6I5g/fz7z589n5MiRAFRUVLBlyxZOPfVUbrvtNm6//XbOOeccTj311A6xpzWC3gPY02g+Gzi5cQYRGQX0VEp9JCIHFXQRmQXMAujVq9eRW2uRX17LkG4NIyzanJ6GaaPnBoOhg1BKceedd3L99dcfsGzlypXMnTuXu+66iylTpnDPPfe0uz3H/ARRRGzAw8Bth8urlPqXUmqMUmpMWlra4bIflPIaH3GeRqMqNnoQ6jAPRQ0GQzvSePjcs846ixdeeIGKigoA9u7dS15eHjk5OURHR3PFFVcwe/ZsVq5cecC67UFrPPS9QM9G8xlWWj1xwFBgoehwR1fgfRE5rz0ejPr8Aarq/MR7Wh4m1+Mygm4wGNqPxsPnzpgxg8suu4zx48cDEBsbyyuvvMLWrVuZPXs2NpsNp9PJU089BcCsWbOYPn063bt3D81DUWAZ0F9EstBCfilwWf1CpVQpEIzwi8hC4Lft1culvEZ/2SPO07LpMS7TD91gMLQvzYfPvfnmm5vM9+3bl7POOuuA9W666SZuuummdrPrsO6sUsoH/Ar4BNgAvKWUWici94vIee1m2UF4deku4OCCHuU0oy0aDIbOSavcWaXUXGBus7QWI/xKqUnHbtbBGdAljgtH9uDU/i3H4G3mqajBYOikhF184swTunLmCV1DbYbBYAghSikkwrsoK6WOeB3zBNFgMIQVHo+HwsLCoxK8cEEpRWFhIR6P5/CZGxF2HvrBeCfhSlxluzgn1IYYDIZ2JSMjg+zsbPLz80NtSrvi8XjIyMg4fMZGRIygX/Sb/w21CQaDoQNwOp1kZWWF2ozjEhNyMRgMhgjBCLrBYDBECEbQDQaDIUKQUD0pFpF8YNdRrp4KFLShOeGAKXPnwJS5c3AsZe6tlGrxRZyQCfqxICLLlVJjQm1HR2LK3DkwZe4ctFeZTcjFYDAYIgQj6AaDwRAhhKug/yvUBoQAU+bOgSlz56BdyhyWMXSDwWAwHEi4eugGg8FgaIYRdIPBYIgQwk7QRWS6iGwSka0ickeo7WkrRKSniCwQkfUisk5EbrbSk0XkUxHZYv0nWekiIo9bx2GN9aHusENE7CLyvYh8aM1nichSq1xviojLSndb81ut5ZkhNfwoEZFEEZkjIhtFZIOIjO8Edfwb65xeKyKvi4gnEutZRF4QkTwRWdso7YjrVkSusvJvEZGrjsSGsBJ0EbEDTwAzgCHAT0VkSGitajN8wG1KqSHAOOBGq2x3AJ8rpfoDn1vzoI9Bf+s3C3iq401uE25Gfwmrnr8Cjyil+gHFwHVW+nVAsZX+iJUvHHkM+FgpNQg4EV32iK1jEekB/BoYo5QaCtjRn7GMxHp+CZjeLO2I6lZEkoF7gZOBscC99Y1Aq1BKhc0PGA980mj+TuDOUNvVTmV9D5gGbAK6WWndgE3W9DPATxvlD+YLlx/6g+OfA2cAHwKCfnvO0by+0Z9AHG9NO6x8EuoyHGF5E4Adze2O8DruAewBkq16+xA4K1LrGcgE1h5t3QI/BZ5plN4k3+F+YeWh03By1JNtpUUU1m3mSGAp0EUptc9alAt0saYj4Vg8CvwOCFjzKUCJ0t+xhaZlCpbXWl5q5Q8nsoB84EUrzPSciMQQwXWslNoL/APYDexD19sKIrueG3OkdXtMdR5ugh7xiEgs8DZwi1KqrPEypZvsiOhnKiLnAHlKqRWhtqUDcQCjgKeUUiOBShpuwYHIqmMAK1xwProx6w7EcGBYolPQEXUbboK+F+jZaD7DSosIRMSJFvNXlVLvWMn7RaSbtbwbkGelh/uxmACcJyI7gTfQYZfHgEQRqf/wSuMyBctrLU8ACjvS4DYgG8hWSi215uegBT5S6xhgKrBDKZWvlPIC76DrPpLruTFHWrfHVOfhJujLgP7WE3IX+uHK+yG2qU0Q/cXb54ENSqmHGy16H6h/0n0VOrZen36l9bR8HFDa6NbuuEcpdadSKkMplYmuxy+UUpcDC4CLrWzNy1t/HC628oeVJ6uUygX2iMhAK2kKsJ4IrWOL3cA4EYm2zvH6MkdsPTfjSOv2E+BMEUmy7m7OtNJaR6gfIhzFQ4eZwGZgG/D7UNvThuWaiL4dWwOssn4z0fHDz4EtwGdAspVf0D1+tgE/oHsRhLwcR1n2ScCH1nQf4DtgK/AfwG2le6z5rdbyPqG2+yjLOgJYbtXzu0BSpNcxcB+wEVgLvAy4I7GegdfRzwm86Lux646mboFrrfJvBa45EhvMq/8Gg8EQIYRbyMVgMBgMB8EIusFgMEQIRtANBoMhQjCCbjAYDBGCEXSDwWCIEIygGwwGQ4RgBN1gMBgihP8P+BevzuFjZ88AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss learning curves\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Cross-Entropy Loss', pad=-40)\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "\n",
    "# plot accuracy learning curves\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy', pad=-40)\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Horizontal Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be many ways to implement a horizontal voting ensemble. Perhaps the simplest is to drive the training process manually, one epoch at a time, then save models at the end of the epoch if we have exceeded an upper limit on the number of epochs. For example, we will train the model for 1,000 epochs with our test problem and perhaps save models from epoch 950 onwards (e.g., between and including epochs 950 and 999)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "n_epochs, n_save_after = 1000, 950\n",
    "for i in range(n_epochs):\n",
    "    # fit model for a single epoch\n",
    "    model.fit(trainX, trainy, epochs=1, verbose=0)\n",
    "\n",
    "    # check if we should save the model\n",
    "    if i >= n_save_after:\n",
    "        model.save('models/model_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models can be saved to a file using the save() function on the model and specifying a filename that includes the epoch number. We will save all models under a new models/ folder in the current working directory to avoid clutter with our source files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import makedirs\n",
    "\n",
    "# create directory for models\n",
    "makedirs('models', exist_ok =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, saving and loading neural network models in Keras requires that you have the h5py library installed. You can install this library using pip as follows:\n",
    "\n",
    "```\n",
    "pip install h5py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tying all of this together, the complete example of fitting the model on the training dataset and saving all models from the last 50 epochs is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save horizontal voting ensemble members during training\n",
    "from sklearn.datasets import make_blobs\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from os import makedirs\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "\n",
    "# one hot encode output variable\n",
    "y = to_categorical(y)\n",
    "\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=2, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# create directory for models\n",
    "makedirs('models',exist_ok =True)\n",
    "\n",
    "# fit model\n",
    "n_epochs, n_save_after = 1000, 950\n",
    "for i in range(n_epochs):\n",
    "    # fit model for a single epoch\n",
    "    model.fit(trainX, trainy, epochs=1, verbose=0)\n",
    "    \n",
    "    # check if we should save the model\n",
    "    if i >= n_save_after:\n",
    "        model.save('models/model_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example creates the models/ folder and saves 50 models into the directory. Note, to re-run this example, you must delete the models/ directory so that the script can recreate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Horizontal Ensemble Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created the models, we can use them in a horizontal voting ensemble. First, we need to load the models into memory. This is reasonable as the models are small. If you are developing a horizontal voting ensemble with very large models, it might be easier to load models one at a time, make a prediction, then load the next model and repeat the process. The function `load_all_models()` below will load models from the models/ directory. It takes the start and end epochs as arguments so that you can experiment with different groups of models saved over contiguous epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models from file\n",
    "def load_all_models(n_start, n_end):\n",
    "    all_models = list()\n",
    "    for epoch in range(n_start, n_end):\n",
    "        # define filename for this ensemble\n",
    "        filename = 'models/model_' + str(epoch) + '.h5'\n",
    "\n",
    "        # load model from file\n",
    "        model = load_model(filename)\n",
    "\n",
    "        # add to list of members\n",
    "        all_models.append(model)\n",
    "        print('>loaded %s' % filename)\n",
    "    return all_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call the function to load all of the models. We can then reverse the list of models so that the models at the end of the run are at the beginning of the list. This will be helpful later when we test voting ensembles of different sizes, including models sequentially from the end of the run backward through training epochs, in case the best models really were at the end of the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">loaded models/model_950.h5\n",
      ">loaded models/model_951.h5\n",
      ">loaded models/model_952.h5\n",
      ">loaded models/model_953.h5\n",
      ">loaded models/model_954.h5\n",
      ">loaded models/model_955.h5\n",
      ">loaded models/model_956.h5\n",
      ">loaded models/model_957.h5\n",
      ">loaded models/model_958.h5\n",
      ">loaded models/model_959.h5\n",
      ">loaded models/model_960.h5\n",
      ">loaded models/model_961.h5\n",
      ">loaded models/model_962.h5\n",
      ">loaded models/model_963.h5\n",
      ">loaded models/model_964.h5\n",
      ">loaded models/model_965.h5\n",
      ">loaded models/model_966.h5\n",
      ">loaded models/model_967.h5\n",
      ">loaded models/model_968.h5\n",
      ">loaded models/model_969.h5\n",
      ">loaded models/model_970.h5\n",
      ">loaded models/model_971.h5\n",
      ">loaded models/model_972.h5\n",
      ">loaded models/model_973.h5\n",
      ">loaded models/model_974.h5\n",
      ">loaded models/model_975.h5\n",
      ">loaded models/model_976.h5\n",
      ">loaded models/model_977.h5\n",
      ">loaded models/model_978.h5\n",
      ">loaded models/model_979.h5\n",
      ">loaded models/model_980.h5\n",
      ">loaded models/model_981.h5\n",
      ">loaded models/model_982.h5\n",
      ">loaded models/model_983.h5\n",
      ">loaded models/model_984.h5\n",
      ">loaded models/model_985.h5\n",
      ">loaded models/model_986.h5\n",
      ">loaded models/model_987.h5\n",
      ">loaded models/model_988.h5\n",
      ">loaded models/model_989.h5\n",
      ">loaded models/model_990.h5\n",
      ">loaded models/model_991.h5\n",
      ">loaded models/model_992.h5\n",
      ">loaded models/model_993.h5\n",
      ">loaded models/model_994.h5\n",
      ">loaded models/model_995.h5\n",
      ">loaded models/model_996.h5\n",
      ">loaded models/model_997.h5\n",
      ">loaded models/model_998.h5\n",
      ">loaded models/model_999.h5\n",
      "Loaded 50 models\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# load models in order\n",
    "members = load_all_models(950, 1000)\n",
    "print('Loaded %d models' % len(members))\n",
    "\n",
    "# reverse loaded models so we build the ensemble with the last models first\n",
    "members = list(reversed(members))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can evaluate each saved model on the test dataset and a voting ensemble of the last n contiguous models from training. We want to know how well each model actually performed on the test dataset and, importantly, the distribution of model performance on the test dataset so that we know how well (or poorly) an average model chosen from the end of the run would perform in practice. We don't know how many members to include in the horizontal voting ensemble. Therefore, we can test different numbers of contiguous members, working backward from the final model.\n",
    "\n",
    "First, we need a function to make a prediction with a list of ensemble members. Each member predicts the probabilities for each of the three output classes. The probabilities are added, and we use an argmax to select the class with the most support. The `ensemble_predictions()` function below implements this voting-based prediction scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an ensemble prediction for multiclass classification\n",
    "def ensemble_predictions(members, testX):\n",
    "    # make predictions\n",
    "    yhats = [model.predict(testX) for model in members]\n",
    "    yhats = array(yhats)\n",
    "\n",
    "    # sum across ensemble members\n",
    "    summed = numpy.sum(yhats, axis=0)\n",
    "\n",
    "    # argmax across classes\n",
    "    result = argmax(summed, axis=1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a function to evaluate a subset of the ensemble members of a given size. The subset needs to be selected, predictions made, and the ensemble's performance estimated by comparing the predictions to the expected values. The `evaluate_n_members()` function below implements this ensemble size evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a specific number of members in an ensemble\n",
    "def evaluate_n_members(members, n_members, testX, testy):\n",
    "    # select a subset of members\n",
    "    subset = members[:n_members]\n",
    "\n",
    "    # make prediction\n",
    "    yhat = ensemble_predictions(subset, testX)\n",
    "\n",
    "    # calculate accuracy\n",
    "    return accuracy_score(testy, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now enumerate through different sized horizontal voting ensembles from 1 to 50. Each member is evaluated alone, then the ensemble of that size is evaluated, and scores are recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multilabel-indicator and multiclass targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-601a4ef990a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmembers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# evaluate model with i members\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mensemble_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_n_members\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmembers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# evaluate the i'th model standalone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-3075e990c2a6>\u001b[0m in \u001b[0;36mevaluate_n_members\u001b[0;34m(members, n_members, testX, testy)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# calculate accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 93\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multilabel-indicator and multiclass targets"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from numpy import array, argmax\n",
    "import numpy\n",
    "\n",
    "# evaluate different numbers of ensembles on hold out set\n",
    "single_scores, ensemble_scores = list(), list()\n",
    "for i in range(1, len(members)+1):\n",
    "    # evaluate model with i members\n",
    "    ensemble_score = evaluate_n_members(members, i, testX, testy)\n",
    "\n",
    "    # evaluate the i'th model standalone\n",
    "    testy_enc = to_categorical(testy)\n",
    "    _, single_score = members[i-1].evaluate(testX, testy_enc, verbose=0)\n",
    "\n",
    "    # summarize this step\n",
    "    print('> %d: single=%.3f, ensemble=%.3f' % (i, single_score, ensemble_score))\n",
    "    ensemble_scores.append(ensemble_score)\n",
    "    single_scores.append(single_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the evaluations, we report the distribution of scores of single models on the test dataset. The average score is what we would expect on average if we picked any of the saved models as a final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean, std\n",
    "\n",
    "# summarize average accuracy of a single final model\n",
    "print('Accuracy %.3f (%.3f)' % (mean(single_scores), std(single_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot the scores. The scores of each standalone model are plotted as blue dots, and a line plot is created for each ensemble of contiguous models (orange)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot score vs number of ensemble members\n",
    "x_axis = [i for i in range(1, len(members)+1)]\n",
    "pyplot.plot(x_axis, single_scores, marker='o', linestyle='None')\n",
    "pyplot.plot(x_axis, ensemble_scores, marker='o')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our expectation is that a fair-sized ensemble will outperform a randomly selected model and that there is a point of diminishing returns in choosing the ensemble size. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models and make predictions using a horizontal voting ensemble\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from numpy import mean, std, array, argmax\n",
    "import numpy\n",
    "\n",
    "# load models from file\n",
    "def load_all_models(n_start, n_end):\n",
    "    all_models = list()\n",
    "    for epoch in range(n_start, n_end):\n",
    "        # define filename for this ensemble\n",
    "        filename = 'models/model_' + str(epoch) + '.h5'\n",
    "\n",
    "        # load model from file\n",
    "        model = load_model(filename)\n",
    "\n",
    "        # add to list of members\n",
    "        all_models.append(model)\n",
    "        print('>loaded %s' % filename)\n",
    "\n",
    "    return all_models\n",
    "\n",
    "# make an ensemble prediction for multi-class classification\n",
    "def ensemble_predictions(members, testX):\n",
    "    # make predictions\n",
    "    yhats = [model.predict(testX) for model in members]\n",
    "    yhats = array(yhats)\n",
    "\n",
    "    # sum across ensemble members\n",
    "    summed = numpy.sum(yhats, axis=0)\n",
    "\n",
    "    # argmax across classes\n",
    "    result = argmax(summed, axis=1)\n",
    "\n",
    "    return result\n",
    "\n",
    "# evaluate a specific number of members in an ensemble\n",
    "def evaluate_n_members(members, n_members, testX, testy):\n",
    "    # select a subset of members\n",
    "    subset = members[:n_members]\n",
    "\n",
    "    # make prediction\n",
    "    yhat = ensemble_predictions(subset, testX)\n",
    "\n",
    "    # calculate accuracy\n",
    "    return accuracy_score(testy, yhat)\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the 50 saved models are loaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">loaded models/model_950.h5\n",
      ">loaded models/model_951.h5\n",
      ">loaded models/model_952.h5\n",
      ">loaded models/model_953.h5\n",
      ">loaded models/model_954.h5\n",
      ">loaded models/model_955.h5\n",
      ">loaded models/model_956.h5\n",
      ">loaded models/model_957.h5\n",
      ">loaded models/model_958.h5\n",
      ">loaded models/model_959.h5\n",
      ">loaded models/model_960.h5\n",
      ">loaded models/model_961.h5\n",
      ">loaded models/model_962.h5\n",
      ">loaded models/model_963.h5\n",
      ">loaded models/model_964.h5\n",
      ">loaded models/model_965.h5\n",
      ">loaded models/model_966.h5\n",
      ">loaded models/model_967.h5\n",
      ">loaded models/model_968.h5\n",
      ">loaded models/model_969.h5\n",
      ">loaded models/model_970.h5\n",
      ">loaded models/model_971.h5\n",
      ">loaded models/model_972.h5\n",
      ">loaded models/model_973.h5\n",
      ">loaded models/model_974.h5\n",
      ">loaded models/model_975.h5\n",
      ">loaded models/model_976.h5\n",
      ">loaded models/model_977.h5\n",
      ">loaded models/model_978.h5\n",
      ">loaded models/model_979.h5\n",
      ">loaded models/model_980.h5\n",
      ">loaded models/model_981.h5\n",
      ">loaded models/model_982.h5\n",
      ">loaded models/model_983.h5\n",
      ">loaded models/model_984.h5\n",
      ">loaded models/model_985.h5\n",
      ">loaded models/model_986.h5\n",
      ">loaded models/model_987.h5\n",
      ">loaded models/model_988.h5\n",
      ">loaded models/model_989.h5\n",
      ">loaded models/model_990.h5\n",
      ">loaded models/model_991.h5\n",
      ">loaded models/model_992.h5\n",
      ">loaded models/model_993.h5\n",
      ">loaded models/model_994.h5\n",
      ">loaded models/model_995.h5\n",
      ">loaded models/model_996.h5\n",
      ">loaded models/model_997.h5\n",
      ">loaded models/model_998.h5\n",
      ">loaded models/model_999.h5\n",
      "Loaded 50 models\n"
     ]
    }
   ],
   "source": [
    "# load models in order\n",
    "members = load_all_models(950, 1000)\n",
    "print('Loaded %d models' % len(members))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the performance of every single model is evaluated on the holdout test dataset, and the ensemble of that size (1, 2, 3, etc.) is created and evaluated on the holdout test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 1: single=0.802, ensemble=0.802\n",
      "> 2: single=0.802, ensemble=0.802\n",
      "> 3: single=0.802, ensemble=0.802\n",
      "> 4: single=0.802, ensemble=0.802\n",
      "> 5: single=0.802, ensemble=0.802\n",
      "> 6: single=0.802, ensemble=0.802\n",
      "> 7: single=0.802, ensemble=0.802\n",
      "> 8: single=0.801, ensemble=0.802\n",
      "> 9: single=0.801, ensemble=0.802\n",
      "> 10: single=0.802, ensemble=0.801\n",
      "> 11: single=0.807, ensemble=0.800\n",
      "> 12: single=0.807, ensemble=0.800\n",
      "> 13: single=0.808, ensemble=0.801\n",
      "> 14: single=0.808, ensemble=0.802\n",
      "> 15: single=0.808, ensemble=0.801\n",
      "> 16: single=0.804, ensemble=0.802\n",
      "> 17: single=0.804, ensemble=0.802\n",
      "> 18: single=0.805, ensemble=0.803\n",
      "> 19: single=0.806, ensemble=0.802\n",
      "> 20: single=0.805, ensemble=0.802\n",
      "> 21: single=0.806, ensemble=0.802\n",
      "> 22: single=0.805, ensemble=0.803\n",
      "> 23: single=0.805, ensemble=0.803\n",
      "> 24: single=0.805, ensemble=0.803\n",
      "> 25: single=0.805, ensemble=0.802\n",
      "> 26: single=0.805, ensemble=0.802\n",
      "> 27: single=0.805, ensemble=0.802\n",
      "> 28: single=0.806, ensemble=0.801\n",
      "> 29: single=0.803, ensemble=0.801\n",
      "> 30: single=0.805, ensemble=0.802\n",
      "> 31: single=0.804, ensemble=0.801\n",
      "> 32: single=0.809, ensemble=0.802\n",
      "> 33: single=0.815, ensemble=0.802\n",
      "> 34: single=0.815, ensemble=0.804\n",
      "> 35: single=0.817, ensemble=0.804\n",
      "> 36: single=0.816, ensemble=0.804\n",
      "> 37: single=0.815, ensemble=0.805\n",
      "> 38: single=0.810, ensemble=0.804\n",
      "> 39: single=0.805, ensemble=0.802\n",
      "> 40: single=0.800, ensemble=0.802\n",
      "> 41: single=0.801, ensemble=0.802\n",
      "> 42: single=0.802, ensemble=0.802\n",
      "> 43: single=0.805, ensemble=0.802\n",
      "> 44: single=0.804, ensemble=0.802\n",
      "> 45: single=0.808, ensemble=0.803\n",
      "> 46: single=0.807, ensemble=0.803\n",
      "> 47: single=0.808, ensemble=0.803\n",
      "> 48: single=0.807, ensemble=0.803\n",
      "> 49: single=0.807, ensemble=0.803\n",
      "> 50: single=0.805, ensemble=0.803\n"
     ]
    }
   ],
   "source": [
    "# reverse loaded models so we build the ensemble with the last models first\n",
    "members = list(reversed(members))\n",
    "\n",
    "# evaluate different numbers of ensembles on hold out set\n",
    "single_scores, ensemble_scores = list(), list()\n",
    "for i in range(1, len(members)+1):\n",
    "    # evaluate model with i members\n",
    "    ensemble_score = evaluate_n_members(members, i, testX, testy)\n",
    "\n",
    "    # evaluate the i'th model standalone\n",
    "    testy_enc = to_categorical(testy)\n",
    "    _, single_score = members[i-1].evaluate(testX, testy_enc, verbose=0)\n",
    "\n",
    "    # summarize this step\n",
    "    print('> %d: single=%.3f, ensemble=%.3f' % (i, single_score, ensemble_score))\n",
    "    ensemble_scores.append(ensemble_score)\n",
    "    single_scores.append(single_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly, we can see that the ensemble appears to outperform most single models, consistently achieving an accuracy of around 81.8%. Next, the distribution of the accuracy of single models is reported. We can see that picking any of the saved models at random would result in a model with an accuracy of 81.6% on average with a reasonably tight standard deviation of 0.3%. We would require that a horizontal ensemble outperform this average to be useful.\n",
    "\n",
    "**Note:** Your specific results may vary, given the stochastic nature of the learning algorithm. Consider running the example a few times and compare the average performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.806 (0.004)\n"
     ]
    }
   ],
   "source": [
    "# summarize average accuracy of a single final model\n",
    "print('Accuracy %.3f (%.3f)' % (mean(single_scores), std(single_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a graph is created summarizing the performance of every single model (blue dot) and the ensemble of each size from 1 to 50 members. We can see from the blue dots that there is no structure to the models over the epochs, e.g., if the last models during training were better, there would be a downward trend in the accuracy from left to right. We can see that as we add more ensemble members, the better the performance of the horizontal voting ensemble in the orange line. We can see a flattening of performance on this problem, perhaps between 23 and 33 epochs; that might be a good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot score vs number of ensemble members\n",
    "x_axis = [i for i in range(1, len(members)+1)]\n",
    "pyplot.plot(x_axis, single_scores, marker='o', linestyle='None')\n",
    "pyplot.plot(x_axis, ensemble_scores, marker='o')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section lists some ideas for extending the tutorial that you may wish to explore.\n",
    "\n",
    "* **Dataset Size**. Repeat the experiments with a smaller or larger-sized dataset with a similar ratio of training to test examples.\n",
    "* **Larger Ensemble**. Re-run the example with hundreds of final models and report the impact of the large ensemble sizes of accuracy on the test set.\n",
    "* **Random Sampling of Models**. Re-run the example and compare the performance of ensembles of the same size with models saved over contiguous epochs to a random selection of saved models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you discovered how to reduce the variance of a final deep learning neural network model using a horizontal voting ensemble. Specifically, you learned:\n",
    "\n",
    "* It is challenging to choose a final neural network model that has high variance on a training dataset.\n",
    "* Horizontal voting ensembles provide a way to reduce variance and improve average model performance for models with high variance using a single training run.\n",
    "* How to develop a horizontal voting ensemble in Python using Keras to improve the performance of a final  Multilayer Perceptron model for multiclass classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
