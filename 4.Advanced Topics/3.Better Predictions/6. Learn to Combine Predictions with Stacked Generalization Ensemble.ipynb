{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn to Combine Predictions With Stacked Generalization Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model averaging is an ensemble technique where multiple submodels contribute equally to a combined prediction. Model averaging can be improved by weighting the contributions of each submodel to the combined prediction by the expected performance of the submodel. This can be extended further by training an entirely new model to learn how to best combine the contributions from each submodel. This approach is called stacked generalization, or stacking for short, and can result in better predictive performance than any single contributing model. In this tutorial, you will discover how to develop a stacked generalization ensemble for deep learning neural networks. After completing this tutorial, you will know:\n",
    "\n",
    "* Stacked generalization is an ensemble method where a new model learns how to best combine the predictions from multiple existing models.\n",
    "* Developing a stacking model using neural networks as a submodel and a scikit-learn classifier as the meta-learner.\n",
    "* How to develop a stacking model where neural network submodels are embedded in a larger stacking ensemble model for training and prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Generalization Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model averaging ensemble combines the predictions from multiple trained models. A limitation of this approach is that each model contributes the same amount to the ensemble prediction, regardless of how well the model performed. A variation of this approach, called a weighted average ensemble, weighs the contribution of each ensemble member by the trust or expected performance of the model on a hold-out dataset. This allows well-performing models to contribute more and less-well-performing models to contribute less. The weighted average ensemble provides an improvement over the model average ensemble.\n",
    "\n",
    "A further generalization of this approach is replacing the linear weighted sum (e.g., linear regression) model used to combine the predictions of the submodels with any learning algorithm. This approach is called stacked generalization, or stacking for short. In stacking, an algorithm takes the outputs of submodels as input and attempts to learn how to combine the input predictions best to make a better output prediction. It may be helpful to think of the stacking procedure as having two levels: level 0 and level 1.\n",
    "\n",
    "* Level 0: The level 0 data is the training dataset inputs, and level 0 models learn to make predictions from this data.\n",
    "* Level 1: The level 1 data takes the output of the level 0 models as input, and the single level 1 model, or meta-learner, learns to make predictions from this data. \n",
    "\n",
    "Unlike a weighted average ensemble, a stacked generalization ensemble can use the set of predictions as a context and conditionally decide to weigh the input predictions differently, potentially resulting in better performance. Interestingly, although stacking is described as an ensemble learning method with two or more level 0 models, it can be used in the case of only a single level, 0 model. In this case, the level 1, or meta-learner, model corrects predictions from the level 0 model.\n",
    "\n",
    "The meta-learner must be trained on a separate dataset to the examples used to train the level 0 models to avoid overfitting. A simple way that this can be achieved is by splitting the training dataset into a train and validation set. The level 0 models are then trained on the train set. The level 1 model is then trained using the validation set, where the raw inputs are first fed through the level 0 models to get predictions that are used as inputs to the level 1 model. A limitation of the hold-out validation set approach to training a stacking model is that level 0, and level 1 models are not trained on the full dataset.\n",
    "\n",
    "A more sophisticated approach to training a stacked model involves using k-fold cross-validation to develop the training dataset for the meta-learner model. Each level 0 model is trained using k-fold cross-validation (or even leave-one-out cross-validation for maximum effect); the models are then discarded, but the predictions are retained. This means for each model, there are predictions made by a version of the model that was not trained on those examples, e.g., having hold-out examples, but in this case for the entire training dataset. The predictions are then used as inputs to train the meta-learner. Level 0 models are then trained on the entire training dataset, and together with the meta-learner, the stacked model can be used to make predictions on new data. In practice, it is common to use different algorithms to prepare each of the level 0 models to provide a diverse set of predictions.\n",
    "\n",
    "It is also common to use a simple linear model to combine the predictions. Because the use of a linear model is common, stacking is more recently referred to as model blending or simply blending, especially in machine learning competitions.\n",
    "\n",
    "A stacked generalization ensemble can be developed for regression and classification problems. In the case of classification problems, better results have been seen when using the prediction of class probabilities as input to the meta-learner instead of class labels.\n",
    "\n",
    "Now that we are familiar with stacked generalization, we can develop a stacked deep learning model through a case study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Generalization Ensemble Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will demonstrate how to use the stacking ensemble to reduce the variance of an MLP on a simple multiclass classification problem. This example provides a template for applying the stacking ensemble to your neural network for classification and regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a small multiclass classification problem as the basis to demonstrate the stacking ensemble. The scikit-learn class provides the `make_blobs()` function that can be used to create a multiclass classification problem with the prescribed number of samples, input variables, classes, and variance of samples within a class. We use this problem with 1,000 examples, with input variables (to represent the x and y coordinates of the points) and a standard deviation of 2.0 for points within each group. We will use the same random state (seed for the pseudorandom number generator) to ensure that we always get the same data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are the input and output elements of a dataset that we can model. In order to get a feeling for the complexity of the problem, we can graph each point on a two-dimensional scatter plot and color each point by class value. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# scatter plot of blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib import pyplot\n",
    "from numpy import where\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "\n",
    "# scatter plot for each class value\n",
    "for class_value in range(3):\n",
    "    # select indices of points with the class label\n",
    "    row_ix = where(y == class_value)\n",
    "\n",
    "    # scatter plot for points with a different color\n",
    "    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n",
    "\n",
    "# show plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example creates a scatter plot of the entire dataset. We can see that the standard deviation of 2.0 means that the classes are not linearly separable (separable by a line), causing many ambiguous points. This is desirable because the problem is non-trivial and will allow a neural network model to find many different *good enough* candidate solutions resulting in a high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we define a model, we need to contrive an appropriate problem for the stacking ensemble. In our problem, the training dataset is relatively small. Specifically, there is a 10:1 ratio of examples in the training dataset to the holdout dataset. This mimics a situation where we may have a vast number of unlabeled examples and a small number of labeled examples with which to train a model. We will create 1,100 data points from the blobs problem. The model will be trained on the first 100 points, and the remaining 1,000 will be held back in a test dataset, unavailable to the model.\n",
    "\n",
    "The problem is a multiclass classification problem, and we will model it using a softmax activation function on the output layer. This means that the model will predict a vector with three elements with the probability that the sample belongs to each of the three classes. Therefore, the first step is to one-hot encode the class values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2) (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "# scatter plot of blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "\n",
    "# one hot encode output variable\n",
    "y = to_categorical(y)\n",
    "\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "print(trainX.shape, testX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can define and compile the model. The model will expect samples with two input variables. The model then has a single hidden layer with 25 nodes and a rectified linear activation function, an output layer with three nodes to predict the probability of each of the three classes, and a softmax activation function. Because the problem is multiclass, we will use the categorical cross-entropy loss function to optimize the model and the efficient Adam flavor of stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=2, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is fit for 500 training epochs, and we will evaluate each epoch on the test set, using the test set as a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "4/4 [==============================] - 1s 98ms/step - loss: 2.2156 - accuracy: 0.2608 - val_loss: 2.1241 - val_accuracy: 0.3560\n",
      "Epoch 2/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 2.1970 - accuracy: 0.2446 - val_loss: 2.0144 - val_accuracy: 0.3440\n",
      "Epoch 3/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 2.1160 - accuracy: 0.2467 - val_loss: 1.9098 - val_accuracy: 0.3380\n",
      "Epoch 4/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 1.9762 - accuracy: 0.2872 - val_loss: 1.8095 - val_accuracy: 0.3320\n",
      "Epoch 5/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 1.9046 - accuracy: 0.2372 - val_loss: 1.7155 - val_accuracy: 0.3300\n",
      "Epoch 6/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.7538 - accuracy: 0.2806 - val_loss: 1.6263 - val_accuracy: 0.3260\n",
      "Epoch 7/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.7157 - accuracy: 0.2953 - val_loss: 1.5421 - val_accuracy: 0.3230\n",
      "Epoch 8/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 1.6950 - accuracy: 0.2561 - val_loss: 1.4670 - val_accuracy: 0.3160\n",
      "Epoch 9/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.5702 - accuracy: 0.2469 - val_loss: 1.3955 - val_accuracy: 0.3160\n",
      "Epoch 10/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.4811 - accuracy: 0.2649 - val_loss: 1.3325 - val_accuracy: 0.3150\n",
      "Epoch 11/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.4880 - accuracy: 0.2352 - val_loss: 1.2773 - val_accuracy: 0.3260\n",
      "Epoch 12/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.3961 - accuracy: 0.2623 - val_loss: 1.2260 - val_accuracy: 0.3240\n",
      "Epoch 13/500\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 1.2807 - accuracy: 0.2880 - val_loss: 1.1825 - val_accuracy: 0.3310\n",
      "Epoch 14/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.2435 - accuracy: 0.2868 - val_loss: 1.1418 - val_accuracy: 0.3350\n",
      "Epoch 15/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.2310 - accuracy: 0.3002 - val_loss: 1.1024 - val_accuracy: 0.3420\n",
      "Epoch 16/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.1898 - accuracy: 0.3341 - val_loss: 1.0693 - val_accuracy: 0.3770\n",
      "Epoch 17/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.1401 - accuracy: 0.3101 - val_loss: 1.0415 - val_accuracy: 0.4040\n",
      "Epoch 18/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.0780 - accuracy: 0.3558 - val_loss: 1.0172 - val_accuracy: 0.4410\n",
      "Epoch 19/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.0575 - accuracy: 0.3973 - val_loss: 0.9957 - val_accuracy: 0.5030\n",
      "Epoch 20/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.0044 - accuracy: 0.4491 - val_loss: 0.9766 - val_accuracy: 0.5120\n",
      "Epoch 21/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.0035 - accuracy: 0.4871 - val_loss: 0.9570 - val_accuracy: 0.5220\n",
      "Epoch 22/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.9766 - accuracy: 0.5312 - val_loss: 0.9394 - val_accuracy: 0.5310\n",
      "Epoch 23/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.9859 - accuracy: 0.4842 - val_loss: 0.9235 - val_accuracy: 0.5480\n",
      "Epoch 24/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.9365 - accuracy: 0.5174 - val_loss: 0.9093 - val_accuracy: 0.5520\n",
      "Epoch 25/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.9284 - accuracy: 0.5327 - val_loss: 0.8971 - val_accuracy: 0.5530\n",
      "Epoch 26/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.8627 - accuracy: 0.6198 - val_loss: 0.8875 - val_accuracy: 0.5520\n",
      "Epoch 27/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.8909 - accuracy: 0.5584 - val_loss: 0.8782 - val_accuracy: 0.5550\n",
      "Epoch 28/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.8512 - accuracy: 0.5627 - val_loss: 0.8688 - val_accuracy: 0.5540\n",
      "Epoch 29/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.8855 - accuracy: 0.5157 - val_loss: 0.8605 - val_accuracy: 0.5590\n",
      "Epoch 30/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.8692 - accuracy: 0.5521 - val_loss: 0.8522 - val_accuracy: 0.5650\n",
      "Epoch 31/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.8279 - accuracy: 0.5613 - val_loss: 0.8444 - val_accuracy: 0.5700\n",
      "Epoch 32/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.8403 - accuracy: 0.5890 - val_loss: 0.8359 - val_accuracy: 0.5740\n",
      "Epoch 33/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.7676 - accuracy: 0.6119 - val_loss: 0.8273 - val_accuracy: 0.5770\n",
      "Epoch 34/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.8325 - accuracy: 0.5919 - val_loss: 0.8188 - val_accuracy: 0.5860\n",
      "Epoch 35/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.7770 - accuracy: 0.6178 - val_loss: 0.8100 - val_accuracy: 0.5960\n",
      "Epoch 36/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.7840 - accuracy: 0.6112 - val_loss: 0.8017 - val_accuracy: 0.6110\n",
      "Epoch 37/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.7765 - accuracy: 0.6642 - val_loss: 0.7946 - val_accuracy: 0.6180\n",
      "Epoch 38/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.7526 - accuracy: 0.6609 - val_loss: 0.7884 - val_accuracy: 0.6220\n",
      "Epoch 39/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.7813 - accuracy: 0.6203 - val_loss: 0.7817 - val_accuracy: 0.6250\n",
      "Epoch 40/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.7881 - accuracy: 0.6150 - val_loss: 0.7758 - val_accuracy: 0.6270\n",
      "Epoch 41/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.7199 - accuracy: 0.6796 - val_loss: 0.7709 - val_accuracy: 0.6310\n",
      "Epoch 42/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.7206 - accuracy: 0.6755 - val_loss: 0.7653 - val_accuracy: 0.6330\n",
      "Epoch 43/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.7449 - accuracy: 0.6588 - val_loss: 0.7602 - val_accuracy: 0.6370\n",
      "Epoch 44/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6994 - accuracy: 0.6618 - val_loss: 0.7548 - val_accuracy: 0.6420\n",
      "Epoch 45/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.7159 - accuracy: 0.6618 - val_loss: 0.7497 - val_accuracy: 0.6420\n",
      "Epoch 46/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6860 - accuracy: 0.7114 - val_loss: 0.7451 - val_accuracy: 0.6470\n",
      "Epoch 47/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.7396 - accuracy: 0.6480 - val_loss: 0.7409 - val_accuracy: 0.6500\n",
      "Epoch 48/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6962 - accuracy: 0.6803 - val_loss: 0.7369 - val_accuracy: 0.6550\n",
      "Epoch 49/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6616 - accuracy: 0.7095 - val_loss: 0.7333 - val_accuracy: 0.6570\n",
      "Epoch 50/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.6512 - accuracy: 0.7166 - val_loss: 0.7303 - val_accuracy: 0.6540\n",
      "Epoch 51/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.6664 - accuracy: 0.6824 - val_loss: 0.7278 - val_accuracy: 0.6530\n",
      "Epoch 52/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.7005 - accuracy: 0.6605 - val_loss: 0.7250 - val_accuracy: 0.6530\n",
      "Epoch 53/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6562 - accuracy: 0.6958 - val_loss: 0.7224 - val_accuracy: 0.6530\n",
      "Epoch 54/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6838 - accuracy: 0.6854 - val_loss: 0.7194 - val_accuracy: 0.6540\n",
      "Epoch 55/500\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.6370 - accuracy: 0.7197 - val_loss: 0.7162 - val_accuracy: 0.6550\n",
      "Epoch 56/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6955 - accuracy: 0.6656 - val_loss: 0.7126 - val_accuracy: 0.6570\n",
      "Epoch 57/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6257 - accuracy: 0.7250 - val_loss: 0.7087 - val_accuracy: 0.6630\n",
      "Epoch 58/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6274 - accuracy: 0.7177 - val_loss: 0.7052 - val_accuracy: 0.6680\n",
      "Epoch 59/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.7044 - accuracy: 0.6541 - val_loss: 0.7021 - val_accuracy: 0.6690\n",
      "Epoch 60/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6460 - accuracy: 0.6895 - val_loss: 0.6986 - val_accuracy: 0.6690\n",
      "Epoch 61/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6202 - accuracy: 0.6897 - val_loss: 0.6948 - val_accuracy: 0.6730\n",
      "Epoch 62/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6372 - accuracy: 0.6845 - val_loss: 0.6911 - val_accuracy: 0.6770\n",
      "Epoch 63/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6500 - accuracy: 0.7017 - val_loss: 0.6876 - val_accuracy: 0.6820\n",
      "Epoch 64/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6066 - accuracy: 0.7370 - val_loss: 0.6846 - val_accuracy: 0.6870\n",
      "Epoch 65/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6145 - accuracy: 0.7382 - val_loss: 0.6819 - val_accuracy: 0.6870\n",
      "Epoch 66/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6223 - accuracy: 0.7246 - val_loss: 0.6798 - val_accuracy: 0.6890\n",
      "Epoch 67/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6441 - accuracy: 0.7109 - val_loss: 0.6782 - val_accuracy: 0.6900\n",
      "Epoch 68/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6283 - accuracy: 0.7193 - val_loss: 0.6759 - val_accuracy: 0.6900\n",
      "Epoch 69/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6081 - accuracy: 0.7265 - val_loss: 0.6738 - val_accuracy: 0.6900\n",
      "Epoch 70/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6185 - accuracy: 0.7203 - val_loss: 0.6720 - val_accuracy: 0.6910\n",
      "Epoch 71/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6294 - accuracy: 0.7213 - val_loss: 0.6691 - val_accuracy: 0.6960\n",
      "Epoch 72/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5984 - accuracy: 0.7349 - val_loss: 0.6659 - val_accuracy: 0.6960\n",
      "Epoch 73/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6263 - accuracy: 0.7100 - val_loss: 0.6630 - val_accuracy: 0.6970\n",
      "Epoch 74/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5753 - accuracy: 0.7288 - val_loss: 0.6605 - val_accuracy: 0.6980\n",
      "Epoch 75/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5967 - accuracy: 0.7328 - val_loss: 0.6586 - val_accuracy: 0.7000\n",
      "Epoch 76/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5900 - accuracy: 0.7297 - val_loss: 0.6569 - val_accuracy: 0.7010\n",
      "Epoch 77/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5939 - accuracy: 0.7161 - val_loss: 0.6554 - val_accuracy: 0.7010\n",
      "Epoch 78/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6202 - accuracy: 0.7140 - val_loss: 0.6554 - val_accuracy: 0.7000\n",
      "Epoch 79/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5880 - accuracy: 0.7286 - val_loss: 0.6561 - val_accuracy: 0.7000\n",
      "Epoch 80/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6052 - accuracy: 0.7193 - val_loss: 0.6560 - val_accuracy: 0.7000\n",
      "Epoch 81/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6443 - accuracy: 0.6910 - val_loss: 0.6559 - val_accuracy: 0.6980\n",
      "Epoch 82/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6035 - accuracy: 0.7316 - val_loss: 0.6554 - val_accuracy: 0.6970\n",
      "Epoch 83/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5990 - accuracy: 0.7233 - val_loss: 0.6537 - val_accuracy: 0.6980\n",
      "Epoch 84/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5689 - accuracy: 0.7443 - val_loss: 0.6523 - val_accuracy: 0.6980\n",
      "Epoch 85/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5631 - accuracy: 0.7453 - val_loss: 0.6502 - val_accuracy: 0.6980\n",
      "Epoch 86/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6256 - accuracy: 0.6878 - val_loss: 0.6460 - val_accuracy: 0.7000\n",
      "Epoch 87/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5668 - accuracy: 0.7349 - val_loss: 0.6418 - val_accuracy: 0.7010\n",
      "Epoch 88/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5781 - accuracy: 0.7276 - val_loss: 0.6384 - val_accuracy: 0.7000\n",
      "Epoch 89/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5611 - accuracy: 0.7411 - val_loss: 0.6358 - val_accuracy: 0.7020\n",
      "Epoch 90/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5774 - accuracy: 0.7297 - val_loss: 0.6330 - val_accuracy: 0.7020\n",
      "Epoch 91/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5536 - accuracy: 0.7401 - val_loss: 0.6313 - val_accuracy: 0.7060\n",
      "Epoch 92/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5964 - accuracy: 0.6911 - val_loss: 0.6309 - val_accuracy: 0.7040\n",
      "Epoch 93/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5691 - accuracy: 0.7224 - val_loss: 0.6311 - val_accuracy: 0.7050\n",
      "Epoch 94/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5737 - accuracy: 0.7099 - val_loss: 0.6299 - val_accuracy: 0.7070\n",
      "Epoch 95/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5654 - accuracy: 0.7128 - val_loss: 0.6272 - val_accuracy: 0.7080\n",
      "Epoch 96/500\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.5527 - accuracy: 0.7297 - val_loss: 0.6247 - val_accuracy: 0.7080\n",
      "Epoch 97/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5703 - accuracy: 0.7130 - val_loss: 0.6215 - val_accuracy: 0.7080\n",
      "Epoch 98/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5712 - accuracy: 0.7068 - val_loss: 0.6201 - val_accuracy: 0.7080\n",
      "Epoch 99/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5463 - accuracy: 0.7297 - val_loss: 0.6179 - val_accuracy: 0.7060\n",
      "Epoch 100/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5547 - accuracy: 0.7213 - val_loss: 0.6160 - val_accuracy: 0.7080\n",
      "Epoch 101/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5465 - accuracy: 0.7557 - val_loss: 0.6142 - val_accuracy: 0.7090\n",
      "Epoch 102/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5559 - accuracy: 0.7047 - val_loss: 0.6125 - val_accuracy: 0.7130\n",
      "Epoch 103/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5434 - accuracy: 0.7276 - val_loss: 0.6115 - val_accuracy: 0.7140\n",
      "Epoch 104/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5813 - accuracy: 0.7130 - val_loss: 0.6110 - val_accuracy: 0.7140\n",
      "Epoch 105/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5888 - accuracy: 0.6932 - val_loss: 0.6103 - val_accuracy: 0.7140\n",
      "Epoch 106/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5477 - accuracy: 0.7203 - val_loss: 0.6097 - val_accuracy: 0.7150\n",
      "Epoch 107/500\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.5251 - accuracy: 0.7432 - val_loss: 0.6086 - val_accuracy: 0.7160\n",
      "Epoch 108/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5702 - accuracy: 0.6984 - val_loss: 0.6080 - val_accuracy: 0.7160\n",
      "Epoch 109/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5423 - accuracy: 0.7318 - val_loss: 0.6070 - val_accuracy: 0.7170\n",
      "Epoch 110/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5626 - accuracy: 0.7057 - val_loss: 0.6069 - val_accuracy: 0.7140\n",
      "Epoch 111/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5463 - accuracy: 0.7450 - val_loss: 0.6060 - val_accuracy: 0.7130\n",
      "Epoch 112/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5205 - accuracy: 0.7523 - val_loss: 0.6053 - val_accuracy: 0.7150\n",
      "Epoch 113/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5275 - accuracy: 0.7595 - val_loss: 0.6054 - val_accuracy: 0.7160\n",
      "Epoch 114/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5565 - accuracy: 0.7387 - val_loss: 0.6056 - val_accuracy: 0.7140\n",
      "Epoch 115/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5308 - accuracy: 0.7469 - val_loss: 0.6046 - val_accuracy: 0.7150\n",
      "Epoch 116/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5456 - accuracy: 0.7623 - val_loss: 0.6028 - val_accuracy: 0.7160\n",
      "Epoch 117/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5059 - accuracy: 0.7781 - val_loss: 0.6006 - val_accuracy: 0.7170\n",
      "Epoch 118/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5288 - accuracy: 0.7655 - val_loss: 0.5989 - val_accuracy: 0.7180\n",
      "Epoch 119/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5453 - accuracy: 0.7561 - val_loss: 0.5968 - val_accuracy: 0.7200\n",
      "Epoch 120/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5514 - accuracy: 0.7498 - val_loss: 0.5948 - val_accuracy: 0.7220\n",
      "Epoch 121/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4986 - accuracy: 0.7740 - val_loss: 0.5927 - val_accuracy: 0.7240\n",
      "Epoch 122/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5135 - accuracy: 0.7561 - val_loss: 0.5917 - val_accuracy: 0.7230\n",
      "Epoch 123/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5218 - accuracy: 0.7540 - val_loss: 0.5902 - val_accuracy: 0.7260\n",
      "Epoch 124/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5399 - accuracy: 0.7167 - val_loss: 0.5891 - val_accuracy: 0.7260\n",
      "Epoch 125/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5087 - accuracy: 0.7677 - val_loss: 0.5883 - val_accuracy: 0.7260\n",
      "Epoch 126/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5227 - accuracy: 0.7377 - val_loss: 0.5866 - val_accuracy: 0.7280\n",
      "Epoch 127/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4909 - accuracy: 0.7503 - val_loss: 0.5841 - val_accuracy: 0.7270\n",
      "Epoch 128/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5277 - accuracy: 0.7305 - val_loss: 0.5829 - val_accuracy: 0.7280\n",
      "Epoch 129/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5270 - accuracy: 0.7128 - val_loss: 0.5814 - val_accuracy: 0.7310\n",
      "Epoch 130/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5371 - accuracy: 0.7137 - val_loss: 0.5802 - val_accuracy: 0.7310\n",
      "Epoch 131/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5692 - accuracy: 0.7045 - val_loss: 0.5799 - val_accuracy: 0.7320\n",
      "Epoch 132/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4733 - accuracy: 0.7514 - val_loss: 0.5795 - val_accuracy: 0.7310\n",
      "Epoch 133/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5194 - accuracy: 0.7128 - val_loss: 0.5784 - val_accuracy: 0.7320\n",
      "Epoch 134/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4902 - accuracy: 0.7545 - val_loss: 0.5763 - val_accuracy: 0.7360\n",
      "Epoch 135/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5085 - accuracy: 0.7430 - val_loss: 0.5741 - val_accuracy: 0.7370\n",
      "Epoch 136/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5568 - accuracy: 0.6941 - val_loss: 0.5716 - val_accuracy: 0.7420\n",
      "Epoch 137/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5323 - accuracy: 0.7233 - val_loss: 0.5702 - val_accuracy: 0.7420\n",
      "Epoch 138/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5095 - accuracy: 0.7389 - val_loss: 0.5696 - val_accuracy: 0.7390\n",
      "Epoch 139/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4676 - accuracy: 0.7576 - val_loss: 0.5692 - val_accuracy: 0.7380\n",
      "Epoch 140/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4985 - accuracy: 0.7472 - val_loss: 0.5688 - val_accuracy: 0.7350\n",
      "Epoch 141/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5066 - accuracy: 0.7347 - val_loss: 0.5684 - val_accuracy: 0.7380\n",
      "Epoch 142/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5269 - accuracy: 0.7137 - val_loss: 0.5675 - val_accuracy: 0.7380\n",
      "Epoch 143/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5394 - accuracy: 0.7252 - val_loss: 0.5666 - val_accuracy: 0.7390\n",
      "Epoch 144/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4953 - accuracy: 0.7325 - val_loss: 0.5658 - val_accuracy: 0.7390\n",
      "Epoch 145/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4728 - accuracy: 0.7543 - val_loss: 0.5650 - val_accuracy: 0.7410\n",
      "Epoch 146/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5058 - accuracy: 0.7418 - val_loss: 0.5644 - val_accuracy: 0.7420\n",
      "Epoch 147/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5306 - accuracy: 0.7252 - val_loss: 0.5642 - val_accuracy: 0.7410\n",
      "Epoch 148/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5193 - accuracy: 0.7231 - val_loss: 0.5634 - val_accuracy: 0.7430\n",
      "Epoch 149/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4934 - accuracy: 0.7418 - val_loss: 0.5615 - val_accuracy: 0.7450\n",
      "Epoch 150/500\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.4941 - accuracy: 0.7635 - val_loss: 0.5598 - val_accuracy: 0.7500\n",
      "Epoch 151/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4880 - accuracy: 0.7470 - val_loss: 0.5583 - val_accuracy: 0.7500\n",
      "Epoch 152/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5200 - accuracy: 0.7179 - val_loss: 0.5569 - val_accuracy: 0.7480\n",
      "Epoch 153/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4626 - accuracy: 0.7727 - val_loss: 0.5554 - val_accuracy: 0.7540\n",
      "Epoch 154/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4659 - accuracy: 0.7851 - val_loss: 0.5531 - val_accuracy: 0.7540\n",
      "Epoch 155/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4819 - accuracy: 0.7675 - val_loss: 0.5507 - val_accuracy: 0.7570\n",
      "Epoch 156/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4898 - accuracy: 0.7477 - val_loss: 0.5500 - val_accuracy: 0.7610\n",
      "Epoch 157/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4936 - accuracy: 0.7696 - val_loss: 0.5492 - val_accuracy: 0.7650\n",
      "Epoch 158/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5325 - accuracy: 0.7332 - val_loss: 0.5484 - val_accuracy: 0.7650\n",
      "Epoch 159/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4944 - accuracy: 0.7425 - val_loss: 0.5479 - val_accuracy: 0.7650\n",
      "Epoch 160/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4881 - accuracy: 0.7592 - val_loss: 0.5468 - val_accuracy: 0.7660\n",
      "Epoch 161/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4733 - accuracy: 0.7552 - val_loss: 0.5462 - val_accuracy: 0.7670\n",
      "Epoch 162/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5061 - accuracy: 0.7521 - val_loss: 0.5451 - val_accuracy: 0.7670\n",
      "Epoch 163/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4869 - accuracy: 0.7312 - val_loss: 0.5427 - val_accuracy: 0.7700\n",
      "Epoch 164/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4514 - accuracy: 0.8139 - val_loss: 0.5409 - val_accuracy: 0.7720\n",
      "Epoch 165/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4611 - accuracy: 0.8025 - val_loss: 0.5398 - val_accuracy: 0.7740\n",
      "Epoch 166/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4734 - accuracy: 0.7848 - val_loss: 0.5393 - val_accuracy: 0.7710\n",
      "Epoch 167/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4714 - accuracy: 0.7632 - val_loss: 0.5385 - val_accuracy: 0.7710\n",
      "Epoch 168/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4866 - accuracy: 0.7775 - val_loss: 0.5360 - val_accuracy: 0.7750\n",
      "Epoch 169/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5036 - accuracy: 0.7587 - val_loss: 0.5350 - val_accuracy: 0.7740\n",
      "Epoch 170/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4557 - accuracy: 0.7962 - val_loss: 0.5352 - val_accuracy: 0.7730\n",
      "Epoch 171/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5067 - accuracy: 0.7775 - val_loss: 0.5353 - val_accuracy: 0.7720\n",
      "Epoch 172/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4613 - accuracy: 0.8014 - val_loss: 0.5360 - val_accuracy: 0.7690\n",
      "Epoch 173/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5033 - accuracy: 0.7813 - val_loss: 0.5359 - val_accuracy: 0.7680\n",
      "Epoch 174/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4554 - accuracy: 0.8219 - val_loss: 0.5354 - val_accuracy: 0.7700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4413 - accuracy: 0.8313 - val_loss: 0.5345 - val_accuracy: 0.7700\n",
      "Epoch 176/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4619 - accuracy: 0.8157 - val_loss: 0.5337 - val_accuracy: 0.7700\n",
      "Epoch 177/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4593 - accuracy: 0.8065 - val_loss: 0.5325 - val_accuracy: 0.7720\n",
      "Epoch 178/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4557 - accuracy: 0.8210 - val_loss: 0.5324 - val_accuracy: 0.7710\n",
      "Epoch 179/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4505 - accuracy: 0.8084 - val_loss: 0.5324 - val_accuracy: 0.7710\n",
      "Epoch 180/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4280 - accuracy: 0.8230 - val_loss: 0.5309 - val_accuracy: 0.7710\n",
      "Epoch 181/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4808 - accuracy: 0.8002 - val_loss: 0.5266 - val_accuracy: 0.7760\n",
      "Epoch 182/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4729 - accuracy: 0.7785 - val_loss: 0.5238 - val_accuracy: 0.7880\n",
      "Epoch 183/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4734 - accuracy: 0.7879 - val_loss: 0.5222 - val_accuracy: 0.7900\n",
      "Epoch 184/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4621 - accuracy: 0.7858 - val_loss: 0.5213 - val_accuracy: 0.7900\n",
      "Epoch 185/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4522 - accuracy: 0.7743 - val_loss: 0.5212 - val_accuracy: 0.7910\n",
      "Epoch 186/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4621 - accuracy: 0.7712 - val_loss: 0.5211 - val_accuracy: 0.7920\n",
      "Epoch 187/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4761 - accuracy: 0.7679 - val_loss: 0.5213 - val_accuracy: 0.7900\n",
      "Epoch 188/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4471 - accuracy: 0.8263 - val_loss: 0.5209 - val_accuracy: 0.7890\n",
      "Epoch 189/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4106 - accuracy: 0.8450 - val_loss: 0.5196 - val_accuracy: 0.7880\n",
      "Epoch 190/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4530 - accuracy: 0.7867 - val_loss: 0.5182 - val_accuracy: 0.7900\n",
      "Epoch 191/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4523 - accuracy: 0.8033 - val_loss: 0.5174 - val_accuracy: 0.7920\n",
      "Epoch 192/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4467 - accuracy: 0.8013 - val_loss: 0.5161 - val_accuracy: 0.7930\n",
      "Epoch 193/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4803 - accuracy: 0.7898 - val_loss: 0.5143 - val_accuracy: 0.7950\n",
      "Epoch 194/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4470 - accuracy: 0.8030 - val_loss: 0.5128 - val_accuracy: 0.7930\n",
      "Epoch 195/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4565 - accuracy: 0.8261 - val_loss: 0.5120 - val_accuracy: 0.7930\n",
      "Epoch 196/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4386 - accuracy: 0.8334 - val_loss: 0.5134 - val_accuracy: 0.7970\n",
      "Epoch 197/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4411 - accuracy: 0.8466 - val_loss: 0.5155 - val_accuracy: 0.7920\n",
      "Epoch 198/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4620 - accuracy: 0.8082 - val_loss: 0.5176 - val_accuracy: 0.7820\n",
      "Epoch 199/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4502 - accuracy: 0.8020 - val_loss: 0.5170 - val_accuracy: 0.7810\n",
      "Epoch 200/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4103 - accuracy: 0.8363 - val_loss: 0.5148 - val_accuracy: 0.7840\n",
      "Epoch 201/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4413 - accuracy: 0.8092 - val_loss: 0.5123 - val_accuracy: 0.7880\n",
      "Epoch 202/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4264 - accuracy: 0.8082 - val_loss: 0.5112 - val_accuracy: 0.7890\n",
      "Epoch 203/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4097 - accuracy: 0.8395 - val_loss: 0.5096 - val_accuracy: 0.7940\n",
      "Epoch 204/500\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.4330 - accuracy: 0.8219 - val_loss: 0.5087 - val_accuracy: 0.7970\n",
      "Epoch 205/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4636 - accuracy: 0.7990 - val_loss: 0.5065 - val_accuracy: 0.7970\n",
      "Epoch 206/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4244 - accuracy: 0.8240 - val_loss: 0.5050 - val_accuracy: 0.7950\n",
      "Epoch 207/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4497 - accuracy: 0.8063 - val_loss: 0.5041 - val_accuracy: 0.7950\n",
      "Epoch 208/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4411 - accuracy: 0.8073 - val_loss: 0.5037 - val_accuracy: 0.7960\n",
      "Epoch 209/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4389 - accuracy: 0.8178 - val_loss: 0.5036 - val_accuracy: 0.7960\n",
      "Epoch 210/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4194 - accuracy: 0.8271 - val_loss: 0.5036 - val_accuracy: 0.7960\n",
      "Epoch 211/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4200 - accuracy: 0.8198 - val_loss: 0.5031 - val_accuracy: 0.7940\n",
      "Epoch 212/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4451 - accuracy: 0.8032 - val_loss: 0.5025 - val_accuracy: 0.7960\n",
      "Epoch 213/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4444 - accuracy: 0.8115 - val_loss: 0.5026 - val_accuracy: 0.7960\n",
      "Epoch 214/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4493 - accuracy: 0.7990 - val_loss: 0.5016 - val_accuracy: 0.7950\n",
      "Epoch 215/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4175 - accuracy: 0.8250 - val_loss: 0.5020 - val_accuracy: 0.7960\n",
      "Epoch 216/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4210 - accuracy: 0.8261 - val_loss: 0.5017 - val_accuracy: 0.7960\n",
      "Epoch 217/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4637 - accuracy: 0.7771 - val_loss: 0.5005 - val_accuracy: 0.7940\n",
      "Epoch 218/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4106 - accuracy: 0.8219 - val_loss: 0.5001 - val_accuracy: 0.7970\n",
      "Epoch 219/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4096 - accuracy: 0.8384 - val_loss: 0.5003 - val_accuracy: 0.7960\n",
      "Epoch 220/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4446 - accuracy: 0.8113 - val_loss: 0.5007 - val_accuracy: 0.7960\n",
      "Epoch 221/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4146 - accuracy: 0.8061 - val_loss: 0.5002 - val_accuracy: 0.7970\n",
      "Epoch 222/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4513 - accuracy: 0.7967 - val_loss: 0.4986 - val_accuracy: 0.7970\n",
      "Epoch 223/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4322 - accuracy: 0.8176 - val_loss: 0.4969 - val_accuracy: 0.7980\n",
      "Epoch 224/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4522 - accuracy: 0.7905 - val_loss: 0.4964 - val_accuracy: 0.7940\n",
      "Epoch 225/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4020 - accuracy: 0.8405 - val_loss: 0.4957 - val_accuracy: 0.7960\n",
      "Epoch 226/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4108 - accuracy: 0.8197 - val_loss: 0.4944 - val_accuracy: 0.7970\n",
      "Epoch 227/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3876 - accuracy: 0.8426 - val_loss: 0.4923 - val_accuracy: 0.8010\n",
      "Epoch 228/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4264 - accuracy: 0.7978 - val_loss: 0.4912 - val_accuracy: 0.8000\n",
      "Epoch 229/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4094 - accuracy: 0.8363 - val_loss: 0.4902 - val_accuracy: 0.8010\n",
      "Epoch 230/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4278 - accuracy: 0.8268 - val_loss: 0.4898 - val_accuracy: 0.8020\n",
      "Epoch 231/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4224 - accuracy: 0.8145 - val_loss: 0.4895 - val_accuracy: 0.8020\n",
      "Epoch 232/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3957 - accuracy: 0.8499 - val_loss: 0.4893 - val_accuracy: 0.8020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4100 - accuracy: 0.8332 - val_loss: 0.4889 - val_accuracy: 0.8000\n",
      "Epoch 234/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4489 - accuracy: 0.7811 - val_loss: 0.4889 - val_accuracy: 0.8010\n",
      "Epoch 235/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4222 - accuracy: 0.8207 - val_loss: 0.4895 - val_accuracy: 0.8010\n",
      "Epoch 236/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4237 - accuracy: 0.8238 - val_loss: 0.4902 - val_accuracy: 0.8010\n",
      "Epoch 237/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4148 - accuracy: 0.8124 - val_loss: 0.4907 - val_accuracy: 0.8000\n",
      "Epoch 238/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4002 - accuracy: 0.8362 - val_loss: 0.4918 - val_accuracy: 0.7970\n",
      "Epoch 239/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4194 - accuracy: 0.8216 - val_loss: 0.4926 - val_accuracy: 0.7970\n",
      "Epoch 240/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3861 - accuracy: 0.8641 - val_loss: 0.4928 - val_accuracy: 0.7960\n",
      "Epoch 241/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4098 - accuracy: 0.8113 - val_loss: 0.4912 - val_accuracy: 0.7990\n",
      "Epoch 242/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3932 - accuracy: 0.8342 - val_loss: 0.4906 - val_accuracy: 0.8010\n",
      "Epoch 243/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4132 - accuracy: 0.8270 - val_loss: 0.4906 - val_accuracy: 0.8010\n",
      "Epoch 244/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4366 - accuracy: 0.8092 - val_loss: 0.4906 - val_accuracy: 0.7990\n",
      "Epoch 245/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4094 - accuracy: 0.8176 - val_loss: 0.4905 - val_accuracy: 0.8000\n",
      "Epoch 246/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4170 - accuracy: 0.8092 - val_loss: 0.4901 - val_accuracy: 0.8000\n",
      "Epoch 247/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4220 - accuracy: 0.8113 - val_loss: 0.4884 - val_accuracy: 0.8000\n",
      "Epoch 248/500\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.4220 - accuracy: 0.7999 - val_loss: 0.4870 - val_accuracy: 0.8010\n",
      "Epoch 249/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4190 - accuracy: 0.8176 - val_loss: 0.4866 - val_accuracy: 0.8010\n",
      "Epoch 250/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4209 - accuracy: 0.8040 - val_loss: 0.4859 - val_accuracy: 0.8000\n",
      "Epoch 251/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3932 - accuracy: 0.8270 - val_loss: 0.4844 - val_accuracy: 0.8010\n",
      "Epoch 252/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4045 - accuracy: 0.8061 - val_loss: 0.4835 - val_accuracy: 0.8010\n",
      "Epoch 253/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3791 - accuracy: 0.8363 - val_loss: 0.4827 - val_accuracy: 0.8010\n",
      "Epoch 254/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3970 - accuracy: 0.8113 - val_loss: 0.4818 - val_accuracy: 0.8020\n",
      "Epoch 255/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4038 - accuracy: 0.8270 - val_loss: 0.4809 - val_accuracy: 0.8040\n",
      "Epoch 256/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3878 - accuracy: 0.8280 - val_loss: 0.4801 - val_accuracy: 0.8030\n",
      "Epoch 257/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3946 - accuracy: 0.8332 - val_loss: 0.4799 - val_accuracy: 0.8030\n",
      "Epoch 258/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3731 - accuracy: 0.8395 - val_loss: 0.4799 - val_accuracy: 0.8030\n",
      "Epoch 259/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3974 - accuracy: 0.8362 - val_loss: 0.4803 - val_accuracy: 0.7990\n",
      "Epoch 260/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4095 - accuracy: 0.8268 - val_loss: 0.4803 - val_accuracy: 0.8010\n",
      "Epoch 261/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3954 - accuracy: 0.8332 - val_loss: 0.4796 - val_accuracy: 0.8010\n",
      "Epoch 262/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4066 - accuracy: 0.8186 - val_loss: 0.4791 - val_accuracy: 0.7990\n",
      "Epoch 263/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3730 - accuracy: 0.8301 - val_loss: 0.4783 - val_accuracy: 0.8000\n",
      "Epoch 264/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3773 - accuracy: 0.8395 - val_loss: 0.4782 - val_accuracy: 0.8000\n",
      "Epoch 265/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3785 - accuracy: 0.8290 - val_loss: 0.4792 - val_accuracy: 0.7990\n",
      "Epoch 266/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3793 - accuracy: 0.8209 - val_loss: 0.4790 - val_accuracy: 0.7990\n",
      "Epoch 267/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4078 - accuracy: 0.8021 - val_loss: 0.4776 - val_accuracy: 0.8000\n",
      "Epoch 268/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3689 - accuracy: 0.8386 - val_loss: 0.4768 - val_accuracy: 0.7990\n",
      "Epoch 269/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4008 - accuracy: 0.8207 - val_loss: 0.4756 - val_accuracy: 0.8040\n",
      "Epoch 270/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3786 - accuracy: 0.8457 - val_loss: 0.4755 - val_accuracy: 0.8040\n",
      "Epoch 271/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3395 - accuracy: 0.8707 - val_loss: 0.4771 - val_accuracy: 0.8000\n",
      "Epoch 272/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4038 - accuracy: 0.8238 - val_loss: 0.4789 - val_accuracy: 0.8010\n",
      "Epoch 273/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3911 - accuracy: 0.8310 - val_loss: 0.4802 - val_accuracy: 0.8010\n",
      "Epoch 274/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3802 - accuracy: 0.8495 - val_loss: 0.4814 - val_accuracy: 0.8010\n",
      "Epoch 275/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3734 - accuracy: 0.8424 - val_loss: 0.4812 - val_accuracy: 0.8000\n",
      "Epoch 276/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4195 - accuracy: 0.8018 - val_loss: 0.4813 - val_accuracy: 0.8010\n",
      "Epoch 277/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4099 - accuracy: 0.8143 - val_loss: 0.4806 - val_accuracy: 0.8000\n",
      "Epoch 278/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3993 - accuracy: 0.8330 - val_loss: 0.4793 - val_accuracy: 0.8010\n",
      "Epoch 279/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4061 - accuracy: 0.8091 - val_loss: 0.4787 - val_accuracy: 0.8010\n",
      "Epoch 280/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4017 - accuracy: 0.8176 - val_loss: 0.4774 - val_accuracy: 0.8000\n",
      "Epoch 281/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3873 - accuracy: 0.8207 - val_loss: 0.4767 - val_accuracy: 0.8000\n",
      "Epoch 282/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3676 - accuracy: 0.8342 - val_loss: 0.4753 - val_accuracy: 0.8010\n",
      "Epoch 283/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3976 - accuracy: 0.8237 - val_loss: 0.4737 - val_accuracy: 0.8010\n",
      "Epoch 284/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3877 - accuracy: 0.8176 - val_loss: 0.4715 - val_accuracy: 0.8010\n",
      "Epoch 285/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3681 - accuracy: 0.8197 - val_loss: 0.4693 - val_accuracy: 0.8090\n",
      "Epoch 286/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3832 - accuracy: 0.8186 - val_loss: 0.4685 - val_accuracy: 0.8080\n",
      "Epoch 287/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3789 - accuracy: 0.8382 - val_loss: 0.4682 - val_accuracy: 0.8080\n",
      "Epoch 288/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3940 - accuracy: 0.8362 - val_loss: 0.4681 - val_accuracy: 0.8080\n",
      "Epoch 289/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3803 - accuracy: 0.8342 - val_loss: 0.4681 - val_accuracy: 0.8080\n",
      "Epoch 290/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4091 - accuracy: 0.7948 - val_loss: 0.4689 - val_accuracy: 0.8080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 291/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4021 - accuracy: 0.8032 - val_loss: 0.4693 - val_accuracy: 0.8010\n",
      "Epoch 292/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3905 - accuracy: 0.8063 - val_loss: 0.4698 - val_accuracy: 0.7990\n",
      "Epoch 293/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3827 - accuracy: 0.8063 - val_loss: 0.4709 - val_accuracy: 0.8000\n",
      "Epoch 294/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3823 - accuracy: 0.8145 - val_loss: 0.4724 - val_accuracy: 0.8020\n",
      "Epoch 295/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3872 - accuracy: 0.8322 - val_loss: 0.4740 - val_accuracy: 0.8000\n",
      "Epoch 296/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3929 - accuracy: 0.8247 - val_loss: 0.4751 - val_accuracy: 0.7980\n",
      "Epoch 297/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3834 - accuracy: 0.8185 - val_loss: 0.4761 - val_accuracy: 0.8000\n",
      "Epoch 298/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3923 - accuracy: 0.8382 - val_loss: 0.4763 - val_accuracy: 0.8030\n",
      "Epoch 299/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3494 - accuracy: 0.8653 - val_loss: 0.4745 - val_accuracy: 0.8020\n",
      "Epoch 300/500\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.4195 - accuracy: 0.8278 - val_loss: 0.4722 - val_accuracy: 0.8010\n",
      "Epoch 301/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3366 - accuracy: 0.8509 - val_loss: 0.4708 - val_accuracy: 0.8010\n",
      "Epoch 302/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4019 - accuracy: 0.7967 - val_loss: 0.4693 - val_accuracy: 0.8050\n",
      "Epoch 303/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3889 - accuracy: 0.8217 - val_loss: 0.4684 - val_accuracy: 0.8070\n",
      "Epoch 304/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3709 - accuracy: 0.8249 - val_loss: 0.4679 - val_accuracy: 0.8100\n",
      "Epoch 305/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3613 - accuracy: 0.8207 - val_loss: 0.4680 - val_accuracy: 0.8080\n",
      "Epoch 306/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3845 - accuracy: 0.8238 - val_loss: 0.4679 - val_accuracy: 0.8070\n",
      "Epoch 307/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3783 - accuracy: 0.8259 - val_loss: 0.4689 - val_accuracy: 0.8000\n",
      "Epoch 308/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3564 - accuracy: 0.8426 - val_loss: 0.4705 - val_accuracy: 0.7960\n",
      "Epoch 309/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3992 - accuracy: 0.8112 - val_loss: 0.4719 - val_accuracy: 0.7930\n",
      "Epoch 310/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4053 - accuracy: 0.8266 - val_loss: 0.4712 - val_accuracy: 0.7930\n",
      "Epoch 311/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3761 - accuracy: 0.8391 - val_loss: 0.4691 - val_accuracy: 0.7940\n",
      "Epoch 312/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3854 - accuracy: 0.8310 - val_loss: 0.4675 - val_accuracy: 0.7960\n",
      "Epoch 313/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3620 - accuracy: 0.8374 - val_loss: 0.4663 - val_accuracy: 0.7970\n",
      "Epoch 314/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3968 - accuracy: 0.7999 - val_loss: 0.4653 - val_accuracy: 0.8020\n",
      "Epoch 315/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3888 - accuracy: 0.8051 - val_loss: 0.4651 - val_accuracy: 0.8020\n",
      "Epoch 316/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3621 - accuracy: 0.8270 - val_loss: 0.4654 - val_accuracy: 0.8030\n",
      "Epoch 317/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4079 - accuracy: 0.7895 - val_loss: 0.4659 - val_accuracy: 0.8020\n",
      "Epoch 318/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3605 - accuracy: 0.8280 - val_loss: 0.4664 - val_accuracy: 0.8030\n",
      "Epoch 319/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3999 - accuracy: 0.7999 - val_loss: 0.4654 - val_accuracy: 0.8050\n",
      "Epoch 320/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3830 - accuracy: 0.8153 - val_loss: 0.4644 - val_accuracy: 0.8030\n",
      "Epoch 321/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3763 - accuracy: 0.8237 - val_loss: 0.4648 - val_accuracy: 0.8010\n",
      "Epoch 322/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3724 - accuracy: 0.8330 - val_loss: 0.4645 - val_accuracy: 0.8040\n",
      "Epoch 323/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3631 - accuracy: 0.8445 - val_loss: 0.4632 - val_accuracy: 0.8090\n",
      "Epoch 324/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3915 - accuracy: 0.8237 - val_loss: 0.4621 - val_accuracy: 0.8100\n",
      "Epoch 325/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3676 - accuracy: 0.8640 - val_loss: 0.4612 - val_accuracy: 0.8100\n",
      "Epoch 326/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3802 - accuracy: 0.8370 - val_loss: 0.4605 - val_accuracy: 0.8080\n",
      "Epoch 327/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3718 - accuracy: 0.8455 - val_loss: 0.4599 - val_accuracy: 0.8080\n",
      "Epoch 328/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3580 - accuracy: 0.8507 - val_loss: 0.4595 - val_accuracy: 0.8080\n",
      "Epoch 329/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3759 - accuracy: 0.8287 - val_loss: 0.4594 - val_accuracy: 0.8110\n",
      "Epoch 330/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3623 - accuracy: 0.8330 - val_loss: 0.4596 - val_accuracy: 0.8110\n",
      "Epoch 331/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3621 - accuracy: 0.8393 - val_loss: 0.4608 - val_accuracy: 0.8080\n",
      "Epoch 332/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3298 - accuracy: 0.8653 - val_loss: 0.4616 - val_accuracy: 0.8070\n",
      "Epoch 333/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3813 - accuracy: 0.8214 - val_loss: 0.4620 - val_accuracy: 0.8070\n",
      "Epoch 334/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3735 - accuracy: 0.8608 - val_loss: 0.4623 - val_accuracy: 0.8060\n",
      "Epoch 335/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3874 - accuracy: 0.8308 - val_loss: 0.4624 - val_accuracy: 0.8060\n",
      "Epoch 336/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3589 - accuracy: 0.8464 - val_loss: 0.4626 - val_accuracy: 0.8060\n",
      "Epoch 337/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3665 - accuracy: 0.8433 - val_loss: 0.4626 - val_accuracy: 0.8050\n",
      "Epoch 338/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3637 - accuracy: 0.8506 - val_loss: 0.4621 - val_accuracy: 0.8050\n",
      "Epoch 339/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3993 - accuracy: 0.8183 - val_loss: 0.4616 - val_accuracy: 0.8050\n",
      "Epoch 340/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3658 - accuracy: 0.8330 - val_loss: 0.4611 - val_accuracy: 0.8060\n",
      "Epoch 341/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3488 - accuracy: 0.8426 - val_loss: 0.4614 - val_accuracy: 0.8050\n",
      "Epoch 342/500\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3772 - accuracy: 0.8155 - val_loss: 0.4611 - val_accuracy: 0.8050\n",
      "Epoch 343/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3511 - accuracy: 0.8405 - val_loss: 0.4614 - val_accuracy: 0.8040\n",
      "Epoch 344/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3240 - accuracy: 0.8509 - val_loss: 0.4618 - val_accuracy: 0.8030\n",
      "Epoch 345/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3435 - accuracy: 0.8467 - val_loss: 0.4632 - val_accuracy: 0.8030\n",
      "Epoch 346/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3919 - accuracy: 0.8070 - val_loss: 0.4639 - val_accuracy: 0.8000\n",
      "Epoch 347/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3632 - accuracy: 0.8485 - val_loss: 0.4624 - val_accuracy: 0.8000\n",
      "Epoch 348/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3895 - accuracy: 0.8205 - val_loss: 0.4608 - val_accuracy: 0.8040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 349/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3844 - accuracy: 0.8124 - val_loss: 0.4596 - val_accuracy: 0.8070\n",
      "Epoch 350/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3754 - accuracy: 0.7999 - val_loss: 0.4590 - val_accuracy: 0.8120\n",
      "Epoch 351/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3432 - accuracy: 0.8395 - val_loss: 0.4587 - val_accuracy: 0.8110\n",
      "Epoch 352/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3836 - accuracy: 0.8186 - val_loss: 0.4580 - val_accuracy: 0.8110\n",
      "Epoch 353/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3350 - accuracy: 0.8591 - val_loss: 0.4571 - val_accuracy: 0.8100\n",
      "Epoch 354/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3589 - accuracy: 0.8412 - val_loss: 0.4562 - val_accuracy: 0.8110\n",
      "Epoch 355/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3599 - accuracy: 0.8330 - val_loss: 0.4558 - val_accuracy: 0.8110\n",
      "Epoch 356/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3296 - accuracy: 0.8848 - val_loss: 0.4559 - val_accuracy: 0.8120\n",
      "Epoch 357/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3705 - accuracy: 0.8306 - val_loss: 0.4560 - val_accuracy: 0.8110\n",
      "Epoch 358/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3508 - accuracy: 0.8650 - val_loss: 0.4561 - val_accuracy: 0.8110\n",
      "Epoch 359/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3517 - accuracy: 0.8422 - val_loss: 0.4572 - val_accuracy: 0.8120\n",
      "Epoch 360/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3238 - accuracy: 0.8797 - val_loss: 0.4585 - val_accuracy: 0.8100\n",
      "Epoch 361/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3820 - accuracy: 0.8370 - val_loss: 0.4599 - val_accuracy: 0.8080\n",
      "Epoch 362/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3750 - accuracy: 0.8362 - val_loss: 0.4609 - val_accuracy: 0.8060\n",
      "Epoch 363/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3459 - accuracy: 0.8549 - val_loss: 0.4618 - val_accuracy: 0.8060\n",
      "Epoch 364/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3584 - accuracy: 0.8402 - val_loss: 0.4618 - val_accuracy: 0.8070\n",
      "Epoch 365/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3769 - accuracy: 0.8226 - val_loss: 0.4616 - val_accuracy: 0.8060\n",
      "Epoch 366/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3446 - accuracy: 0.8414 - val_loss: 0.4602 - val_accuracy: 0.8080\n",
      "Epoch 367/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3732 - accuracy: 0.8217 - val_loss: 0.4603 - val_accuracy: 0.8070\n",
      "Epoch 368/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3691 - accuracy: 0.8176 - val_loss: 0.4605 - val_accuracy: 0.8070\n",
      "Epoch 369/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3551 - accuracy: 0.8155 - val_loss: 0.4608 - val_accuracy: 0.8060\n",
      "Epoch 370/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3520 - accuracy: 0.8197 - val_loss: 0.4607 - val_accuracy: 0.8070\n",
      "Epoch 371/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3821 - accuracy: 0.8238 - val_loss: 0.4606 - val_accuracy: 0.8070\n",
      "Epoch 372/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3453 - accuracy: 0.8342 - val_loss: 0.4608 - val_accuracy: 0.8070\n",
      "Epoch 373/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3506 - accuracy: 0.8301 - val_loss: 0.4610 - val_accuracy: 0.8060\n",
      "Epoch 374/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3687 - accuracy: 0.7988 - val_loss: 0.4615 - val_accuracy: 0.8050\n",
      "Epoch 375/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3146 - accuracy: 0.8624 - val_loss: 0.4617 - val_accuracy: 0.8040\n",
      "Epoch 376/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3388 - accuracy: 0.8478 - val_loss: 0.4613 - val_accuracy: 0.8050\n",
      "Epoch 377/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3849 - accuracy: 0.8061 - val_loss: 0.4602 - val_accuracy: 0.8090\n",
      "Epoch 378/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3489 - accuracy: 0.8124 - val_loss: 0.4597 - val_accuracy: 0.8090\n",
      "Epoch 379/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3314 - accuracy: 0.8384 - val_loss: 0.4589 - val_accuracy: 0.8090\n",
      "Epoch 380/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3525 - accuracy: 0.8301 - val_loss: 0.4573 - val_accuracy: 0.8110\n",
      "Epoch 381/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3549 - accuracy: 0.8485 - val_loss: 0.4564 - val_accuracy: 0.8100\n",
      "Epoch 382/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3808 - accuracy: 0.8245 - val_loss: 0.4558 - val_accuracy: 0.8100\n",
      "Epoch 383/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3409 - accuracy: 0.8424 - val_loss: 0.4554 - val_accuracy: 0.8130\n",
      "Epoch 384/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3519 - accuracy: 0.8320 - val_loss: 0.4552 - val_accuracy: 0.8170\n",
      "Epoch 385/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3759 - accuracy: 0.8186 - val_loss: 0.4547 - val_accuracy: 0.8160\n",
      "Epoch 386/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3895 - accuracy: 0.7967 - val_loss: 0.4543 - val_accuracy: 0.8170\n",
      "Epoch 387/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3623 - accuracy: 0.8051 - val_loss: 0.4540 - val_accuracy: 0.8150\n",
      "Epoch 388/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3519 - accuracy: 0.8301 - val_loss: 0.4540 - val_accuracy: 0.8140\n",
      "Epoch 389/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3514 - accuracy: 0.8310 - val_loss: 0.4548 - val_accuracy: 0.8150\n",
      "Epoch 390/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3436 - accuracy: 0.8247 - val_loss: 0.4564 - val_accuracy: 0.8090\n",
      "Epoch 391/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3648 - accuracy: 0.8028 - val_loss: 0.4580 - val_accuracy: 0.8080\n",
      "Epoch 392/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3549 - accuracy: 0.8297 - val_loss: 0.4585 - val_accuracy: 0.8080\n",
      "Epoch 393/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3502 - accuracy: 0.8475 - val_loss: 0.4578 - val_accuracy: 0.8070\n",
      "Epoch 394/500\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3483 - accuracy: 0.8424 - val_loss: 0.4565 - val_accuracy: 0.8090\n",
      "Epoch 395/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3681 - accuracy: 0.8028 - val_loss: 0.4559 - val_accuracy: 0.8090\n",
      "Epoch 396/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3303 - accuracy: 0.8603 - val_loss: 0.4553 - val_accuracy: 0.8070\n",
      "Epoch 397/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3387 - accuracy: 0.8301 - val_loss: 0.4546 - val_accuracy: 0.8080\n",
      "Epoch 398/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3493 - accuracy: 0.8341 - val_loss: 0.4548 - val_accuracy: 0.8070\n",
      "Epoch 399/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3892 - accuracy: 0.7926 - val_loss: 0.4552 - val_accuracy: 0.8070\n",
      "Epoch 400/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3619 - accuracy: 0.8051 - val_loss: 0.4551 - val_accuracy: 0.8070\n",
      "Epoch 401/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3287 - accuracy: 0.8374 - val_loss: 0.4553 - val_accuracy: 0.8090\n",
      "Epoch 402/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3329 - accuracy: 0.8259 - val_loss: 0.4554 - val_accuracy: 0.8090\n",
      "Epoch 403/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3376 - accuracy: 0.8332 - val_loss: 0.4555 - val_accuracy: 0.8080\n",
      "Epoch 404/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3821 - accuracy: 0.7811 - val_loss: 0.4557 - val_accuracy: 0.8080\n",
      "Epoch 405/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3454 - accuracy: 0.8270 - val_loss: 0.4567 - val_accuracy: 0.8060\n",
      "Epoch 406/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3512 - accuracy: 0.8124 - val_loss: 0.4592 - val_accuracy: 0.8030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 407/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3424 - accuracy: 0.8268 - val_loss: 0.4602 - val_accuracy: 0.8010\n",
      "Epoch 408/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3460 - accuracy: 0.8495 - val_loss: 0.4596 - val_accuracy: 0.8010\n",
      "Epoch 409/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3358 - accuracy: 0.8527 - val_loss: 0.4593 - val_accuracy: 0.8010\n",
      "Epoch 410/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3152 - accuracy: 0.8662 - val_loss: 0.4599 - val_accuracy: 0.7990\n",
      "Epoch 411/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3366 - accuracy: 0.8702 - val_loss: 0.4612 - val_accuracy: 0.7980\n",
      "Epoch 412/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3502 - accuracy: 0.8586 - val_loss: 0.4619 - val_accuracy: 0.7990\n",
      "Epoch 413/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3330 - accuracy: 0.8640 - val_loss: 0.4616 - val_accuracy: 0.8010\n",
      "Epoch 414/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3713 - accuracy: 0.8422 - val_loss: 0.4597 - val_accuracy: 0.8010\n",
      "Epoch 415/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3466 - accuracy: 0.8391 - val_loss: 0.4580 - val_accuracy: 0.8030\n",
      "Epoch 416/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3511 - accuracy: 0.8339 - val_loss: 0.4572 - val_accuracy: 0.8020\n",
      "Epoch 417/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3223 - accuracy: 0.8631 - val_loss: 0.4564 - val_accuracy: 0.8040\n",
      "Epoch 418/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3723 - accuracy: 0.8339 - val_loss: 0.4552 - val_accuracy: 0.8060\n",
      "Epoch 419/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3084 - accuracy: 0.8765 - val_loss: 0.4538 - val_accuracy: 0.8070\n",
      "Epoch 420/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3651 - accuracy: 0.8390 - val_loss: 0.4522 - val_accuracy: 0.8090\n",
      "Epoch 421/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3608 - accuracy: 0.8400 - val_loss: 0.4522 - val_accuracy: 0.8090\n",
      "Epoch 422/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3418 - accuracy: 0.8732 - val_loss: 0.4525 - val_accuracy: 0.8080\n",
      "Epoch 423/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3748 - accuracy: 0.8421 - val_loss: 0.4533 - val_accuracy: 0.8050\n",
      "Epoch 424/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3611 - accuracy: 0.8555 - val_loss: 0.4544 - val_accuracy: 0.7980\n",
      "Epoch 425/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3405 - accuracy: 0.8494 - val_loss: 0.4554 - val_accuracy: 0.8010\n",
      "Epoch 426/500\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.3575 - accuracy: 0.8556 - val_loss: 0.4564 - val_accuracy: 0.8010\n",
      "Epoch 427/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3414 - accuracy: 0.8525 - val_loss: 0.4574 - val_accuracy: 0.8010\n",
      "Epoch 428/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3597 - accuracy: 0.8358 - val_loss: 0.4574 - val_accuracy: 0.8010\n",
      "Epoch 429/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3293 - accuracy: 0.8483 - val_loss: 0.4569 - val_accuracy: 0.8050\n",
      "Epoch 430/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3569 - accuracy: 0.8391 - val_loss: 0.4546 - val_accuracy: 0.8070\n",
      "Epoch 431/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3622 - accuracy: 0.8370 - val_loss: 0.4537 - val_accuracy: 0.8070\n",
      "Epoch 432/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3468 - accuracy: 0.8494 - val_loss: 0.4532 - val_accuracy: 0.8060\n",
      "Epoch 433/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3495 - accuracy: 0.8463 - val_loss: 0.4532 - val_accuracy: 0.8090\n",
      "Epoch 434/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3697 - accuracy: 0.8402 - val_loss: 0.4536 - val_accuracy: 0.8090\n",
      "Epoch 435/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3451 - accuracy: 0.8556 - val_loss: 0.4540 - val_accuracy: 0.8080\n",
      "Epoch 436/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3315 - accuracy: 0.8525 - val_loss: 0.4546 - val_accuracy: 0.8060\n",
      "Epoch 437/500\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.3322 - accuracy: 0.8558 - val_loss: 0.4550 - val_accuracy: 0.8060\n",
      "Epoch 438/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3279 - accuracy: 0.8610 - val_loss: 0.4552 - val_accuracy: 0.8030\n",
      "Epoch 439/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3565 - accuracy: 0.8266 - val_loss: 0.4560 - val_accuracy: 0.8040\n",
      "Epoch 440/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3532 - accuracy: 0.8174 - val_loss: 0.4572 - val_accuracy: 0.8060\n",
      "Epoch 441/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3545 - accuracy: 0.8247 - val_loss: 0.4579 - val_accuracy: 0.8030\n",
      "Epoch 442/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3617 - accuracy: 0.8226 - val_loss: 0.4589 - val_accuracy: 0.8010\n",
      "Epoch 443/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3263 - accuracy: 0.8487 - val_loss: 0.4605 - val_accuracy: 0.8000\n",
      "Epoch 444/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3557 - accuracy: 0.8268 - val_loss: 0.4615 - val_accuracy: 0.8010\n",
      "Epoch 445/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3260 - accuracy: 0.8610 - val_loss: 0.4621 - val_accuracy: 0.8000\n",
      "Epoch 446/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3281 - accuracy: 0.8433 - val_loss: 0.4633 - val_accuracy: 0.8020\n",
      "Epoch 447/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3616 - accuracy: 0.8308 - val_loss: 0.4625 - val_accuracy: 0.8020\n",
      "Epoch 448/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3384 - accuracy: 0.8485 - val_loss: 0.4614 - val_accuracy: 0.8010\n",
      "Epoch 449/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3207 - accuracy: 0.8560 - val_loss: 0.4601 - val_accuracy: 0.8010\n",
      "Epoch 450/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3590 - accuracy: 0.8268 - val_loss: 0.4587 - val_accuracy: 0.8000\n",
      "Epoch 451/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3446 - accuracy: 0.8310 - val_loss: 0.4575 - val_accuracy: 0.8050\n",
      "Epoch 452/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3579 - accuracy: 0.8185 - val_loss: 0.4566 - val_accuracy: 0.8060\n",
      "Epoch 453/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3269 - accuracy: 0.8445 - val_loss: 0.4561 - val_accuracy: 0.8060\n",
      "Epoch 454/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3567 - accuracy: 0.8185 - val_loss: 0.4561 - val_accuracy: 0.8060\n",
      "Epoch 455/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3511 - accuracy: 0.8185 - val_loss: 0.4561 - val_accuracy: 0.8060\n",
      "Epoch 456/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3215 - accuracy: 0.8528 - val_loss: 0.4569 - val_accuracy: 0.8050\n",
      "Epoch 457/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3441 - accuracy: 0.8351 - val_loss: 0.4579 - val_accuracy: 0.8070\n",
      "Epoch 458/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3449 - accuracy: 0.8237 - val_loss: 0.4580 - val_accuracy: 0.8110\n",
      "Epoch 459/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3600 - accuracy: 0.8195 - val_loss: 0.4569 - val_accuracy: 0.8120\n",
      "Epoch 460/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3541 - accuracy: 0.8464 - val_loss: 0.4553 - val_accuracy: 0.8130\n",
      "Epoch 461/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3450 - accuracy: 0.8494 - val_loss: 0.4544 - val_accuracy: 0.8100\n",
      "Epoch 462/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3170 - accuracy: 0.8765 - val_loss: 0.4541 - val_accuracy: 0.8100\n",
      "Epoch 463/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3526 - accuracy: 0.8494 - val_loss: 0.4539 - val_accuracy: 0.8140\n",
      "Epoch 464/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3364 - accuracy: 0.8711 - val_loss: 0.4534 - val_accuracy: 0.8110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 465/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3134 - accuracy: 0.8579 - val_loss: 0.4535 - val_accuracy: 0.8110\n",
      "Epoch 466/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3355 - accuracy: 0.8527 - val_loss: 0.4549 - val_accuracy: 0.8120\n",
      "Epoch 467/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3302 - accuracy: 0.8454 - val_loss: 0.4559 - val_accuracy: 0.8120\n",
      "Epoch 468/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3513 - accuracy: 0.8297 - val_loss: 0.4563 - val_accuracy: 0.8120\n",
      "Epoch 469/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3453 - accuracy: 0.8433 - val_loss: 0.4555 - val_accuracy: 0.8130\n",
      "Epoch 470/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3702 - accuracy: 0.8205 - val_loss: 0.4536 - val_accuracy: 0.8130\n",
      "Epoch 471/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3336 - accuracy: 0.8485 - val_loss: 0.4521 - val_accuracy: 0.8140\n",
      "Epoch 472/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3391 - accuracy: 0.8422 - val_loss: 0.4517 - val_accuracy: 0.8120\n",
      "Epoch 473/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3265 - accuracy: 0.8575 - val_loss: 0.4513 - val_accuracy: 0.8080\n",
      "Epoch 474/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3451 - accuracy: 0.8711 - val_loss: 0.4516 - val_accuracy: 0.8090\n",
      "Epoch 475/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3035 - accuracy: 0.8888 - val_loss: 0.4520 - val_accuracy: 0.8070\n",
      "Epoch 476/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3633 - accuracy: 0.8369 - val_loss: 0.4519 - val_accuracy: 0.8060\n",
      "Epoch 477/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3420 - accuracy: 0.8504 - val_loss: 0.4524 - val_accuracy: 0.8070\n",
      "Epoch 478/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3384 - accuracy: 0.8556 - val_loss: 0.4538 - val_accuracy: 0.8080\n",
      "Epoch 479/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3400 - accuracy: 0.8341 - val_loss: 0.4553 - val_accuracy: 0.8080\n",
      "Epoch 480/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3598 - accuracy: 0.8216 - val_loss: 0.4577 - val_accuracy: 0.8040\n",
      "Epoch 481/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3369 - accuracy: 0.8247 - val_loss: 0.4588 - val_accuracy: 0.8040\n",
      "Epoch 482/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3268 - accuracy: 0.8402 - val_loss: 0.4590 - val_accuracy: 0.8050\n",
      "Epoch 483/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3308 - accuracy: 0.8527 - val_loss: 0.4588 - val_accuracy: 0.8050\n",
      "Epoch 484/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3440 - accuracy: 0.8185 - val_loss: 0.4582 - val_accuracy: 0.8060\n",
      "Epoch 485/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3397 - accuracy: 0.8435 - val_loss: 0.4576 - val_accuracy: 0.8060\n",
      "Epoch 486/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3663 - accuracy: 0.8143 - val_loss: 0.4566 - val_accuracy: 0.8070\n",
      "Epoch 487/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3145 - accuracy: 0.8653 - val_loss: 0.4552 - val_accuracy: 0.8080\n",
      "Epoch 488/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3327 - accuracy: 0.8278 - val_loss: 0.4538 - val_accuracy: 0.8050\n",
      "Epoch 489/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3279 - accuracy: 0.8546 - val_loss: 0.4540 - val_accuracy: 0.8010\n",
      "Epoch 490/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3375 - accuracy: 0.8433 - val_loss: 0.4553 - val_accuracy: 0.7980\n",
      "Epoch 491/500\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.3503 - accuracy: 0.8297 - val_loss: 0.4570 - val_accuracy: 0.8010\n",
      "Epoch 492/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3093 - accuracy: 0.8785 - val_loss: 0.4592 - val_accuracy: 0.8010\n",
      "Epoch 493/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3584 - accuracy: 0.8214 - val_loss: 0.4621 - val_accuracy: 0.8020\n",
      "Epoch 494/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3169 - accuracy: 0.8610 - val_loss: 0.4631 - val_accuracy: 0.8010\n",
      "Epoch 495/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3249 - accuracy: 0.8433 - val_loss: 0.4633 - val_accuracy: 0.8010\n",
      "Epoch 496/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3354 - accuracy: 0.8454 - val_loss: 0.4624 - val_accuracy: 0.8020\n",
      "Epoch 497/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3459 - accuracy: 0.8308 - val_loss: 0.4612 - val_accuracy: 0.8010\n",
      "Epoch 498/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3496 - accuracy: 0.8402 - val_loss: 0.4604 - val_accuracy: 0.8000\n",
      "Epoch 499/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3266 - accuracy: 0.8495 - val_loss: 0.4600 - val_accuracy: 0.7990\n",
      "Epoch 500/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3834 - accuracy: 0.8150 - val_loss: 0.4600 - val_accuracy: 0.7990\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=500, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the run, we will evaluate the model's performance on both the train and the test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.860, Test: 0.799\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then finally, we will plot model loss and accuracy learning curves over each training epoch on both the training and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss learning curves\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Cross-Entropy Loss', pad=-40)\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "\n",
    "# plot accuracy learning curves\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy', pad=-40)\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.830, Test: 0.808\n"
     ]
    }
   ],
   "source": [
    "# develop an mlp for blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "\n",
    "# one hot encode output variable\n",
    "y = to_categorical(y)\n",
    "\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=2, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=500, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example, it first prints the performance of the final model on the train and test datasets.\n",
    "\n",
    "**Note:** Your specific results may vary, given the stochastic nature of the learning algorithm. Consider running the example a few times and compare the average performance.\n",
    "\n",
    "In this case, we can see that the model achieved about 83% accuracy on the training dataset, which we know is optimistic, and about 81% on the test dataset, which we would expect to be more realistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A line plot also shows the learning curves for the model accuracy on the train and test sets over each training epoch. We can see that training accuracy is more optimistic over the whole run, as we noted with the final scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss learning curves\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Cross-Entropy Loss', pad=-40)\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "\n",
    "# plot accuracy learning curves\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy', pad=-40)\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look at using instances of this model as part of a stacking ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Save Sub-Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use multiple instances of the same model as level-0 or submodels in the stacking ensemble to keep this example simple. We will also use a holdout validation dataset to train the level-1 or meta-learner in the ensemble. A more advanced example may use different types of MLP models (deeper, wider, etc.) as submodels and train the meta-learner using k-fold cross-validation. In this section, we will train multiple submodels and save them to file for later use in our stacking ensembles. The first step is to create a function that defines and fits an MLP model on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model on dataset\n",
    "def fit_model(trainX, trainy):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, input_dim=2, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # fit model\n",
    "    model.fit(trainX, trainy, epochs=500, verbose=0)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can create a subdirectory to store the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import makedirs\n",
    "\n",
    "# create directory for models\n",
    "makedirs('models', exist_ok =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create multiple instances of the MLP and save each to the models/ subdirectory with a unique filename. In this case, we will create five submodels, but you can experiment with a different number of models and see how it impacts model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# fit and save models\n",
    "n_members = 5\n",
    "for i in range(n_members):\n",
    "    # fit model\n",
    "    model = fit_model(trainX, trainy)\n",
    "\n",
    "    # save model\n",
    "    filename = 'models/model_' + str(i + 1) + '.h5'\n",
    "    model.save(filename)\n",
    "    print('>Saved %s' % filename)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tie all of these elements together; the complete example of training the submodels and saving them to file is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of saving sub-models for later use in a stacking ensemble\n",
    "from sklearn.datasets import make_blobs\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from os import makedirs\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "\n",
    "# one hot encode output variable\n",
    "y = to_categorical(y)\n",
    "\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "\n",
    "# create directory for models\n",
    "makedirs('models', exist_ok =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example creates the models/ subfolder and saves five trained models with unique filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Saved models/model_1.h5\n",
      ">Saved models/model_2.h5\n",
      ">Saved models/model_3.h5\n",
      ">Saved models/model_4.h5\n",
      ">Saved models/model_5.h5\n"
     ]
    }
   ],
   "source": [
    "# fit and save models\n",
    "n_members = 5\n",
    "for i in range(n_members):\n",
    "    # fit model\n",
    "    model = fit_model(trainX, trainy)\n",
    "\n",
    "    # save model\n",
    "    filename = 'models/model_' + str(i + 1) + '.h5'\n",
    "    model.save(filename)\n",
    "    print('>Saved %s' % filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can look at training a meta-learner to make the best use of the predictions from these submodels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate Stacking Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train a meta-learner that will best combine the predictions from the submodels and ideally perform better than any single submodel. The first step is to load the saved models, and we can use the `load_model()` Keras function and create a Python list of loaded models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models from file\n",
    "def load_all_models(n_models):\n",
    "    all_models = list()\n",
    "    for i in range(n_models):\n",
    "        # define filename for this ensemble\n",
    "        filename = 'models/model_' + str(i + 1) + '.h5'\n",
    "\n",
    "        # load model from file\n",
    "        model = load_model(filename)\n",
    "\n",
    "        # add to list of members\n",
    "        all_models.append(model)\n",
    "        print('>loaded %s' % filename)\n",
    "\n",
    "    return all_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call this function to load our five saved models from the models/ sub-directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# load all models\n",
    "n_members = 5\n",
    "members = load_all_models(n_members)\n",
    "print('Loaded %d models' % len(members))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be useful to know how well the single models perform on the test dataset as we would expect a stacking model to perform better. We can easily evaluate every single model on the training dataset and establish a baseline of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# evaluate standalone models on test dataset\n",
    "for model in members:\n",
    "    testy_enc = to_categorical(testy)\n",
    "    _, acc = model.evaluate(testX, testy_enc, verbose=0)\n",
    "    print('Model Accuracy: %.3f' % acc)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can train our meta-learner. This requires two steps:\n",
    "\n",
    "* Prepare a training dataset for the meta-learner.\n",
    "* Use the prepared training dataset to fit a meta-learner model.\n",
    "\n",
    "We will prepare a training dataset for the meta-learner by providing examples from the test set to each submodel and collecting the predictions. In this case, each model will output three predictions for each example for the probabilities that a given example belongs to the three classes. Therefore, the 1,000 examples in the test set will result in five arrays with the shape [1000, 3]. We can combine these arrays into a three-dimensional array with the shape [1000, 5, 3] by using the `dstack()` NumPy function to stack each new set of predictions.\n",
    "\n",
    "As input for a new model, we will require 1,000 examples with some number of features. Given that we have five models and each model makes three predictions per example, we would have 15 (3 X 5) features for each example provided to the submodels. We can transform the [1000, 5, 3] shaped predictions from the submodels into a [1000, 15] shaped array to be used to train a meta-learner using the `reshape()` NumPy function and flattening the final two dimensions. The `stacked_dataset()` function implements this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create stacked model input dataset as outputs from the ensemble\n",
    "def stacked_dataset(members, inputX):\n",
    "    stackX = None\n",
    "    for model in members:\n",
    "        # make prediction\n",
    "        yhat = model.predict(inputX, verbose=0)\n",
    "\n",
    "        # stack predictions into [rows, members, probabilities]\n",
    "        if stackX is None:\n",
    "            stackX = yhat\n",
    "        else:\n",
    "            stackX = dstack((stackX, yhat))\n",
    "\n",
    "    # flatten predictions to [rows, members x probabilities]\n",
    "    stackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
    "    \n",
    "    return stackX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once prepared, we can use this input dataset and the output, or y part, of the test set to train a new meta-learner. In this case, we will train a simple logistic regression algorithm from the scikit-learn library. Logistic regression only supports binary classification, although implementing logistic regression in scikit-learn in the `LogisticRegression` class supports multiclass classification (more than two classes) using a multinomial scheme. The function `fit_stacked_model()` below will prepare the training dataset for the meta-learner by calling the `stacked_dataset()` function, then fit a logistic regression model that is then returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model based on the outputs from the ensemble members\n",
    "def fit_stacked_model(members, inputX, inputy):\n",
    "    # create dataset using ensemble\n",
    "    stackedX = stacked_dataset(members, inputX)\n",
    "\n",
    "    # fit standalone model\n",
    "    model = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "    model.fit(stackedX, inputy)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call this function and pass in the list of loaded models and the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# fit stacked model using the ensemble\n",
    "model = fit_stacked_model(members, testX, testy)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once fit, we can use the stacked model, including the members and the meta-learner, to make predictions on new data. This can be achieved by first using the submodels to make an input dataset for the meta-learner, e.g., by calling the `stacked_dataset()` function, then predicting with the meta-learner. The `stacked_prediction()` function below implements this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction with the stacked model\n",
    "def stacked_prediction(members, model, inputX):\n",
    "    # create dataset using ensemble\n",
    "    stackedX = stacked_dataset(members, inputX)\n",
    "\n",
    "    # make a prediction\n",
    "    yhat = model.predict(stackedX)\n",
    "\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this function to predict new data; in this case, we can demonstrate it by making predictions on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# evaluate model on test set\n",
    "yhat = stacked_prediction(members, model, testX)\n",
    "acc = accuracy_score(testy, yhat)\n",
    "print('Stacked Test Accuracy: %.3f' % acc)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tying all of these elements together, the complete example of fitting a linear meta-learner for the stacking ensemble of MLP submodels is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">loaded models/model_1.h5\n",
      ">loaded models/model_2.h5\n",
      ">loaded models/model_3.h5\n",
      ">loaded models/model_4.h5\n",
      ">loaded models/model_5.h5\n",
      "Loaded 5 models\n",
      "Model Accuracy: 0.817\n",
      "Model Accuracy: 0.805\n",
      "Model Accuracy: 0.818\n",
      "Model Accuracy: 0.810\n",
      "Model Accuracy: 0.807\n"
     ]
    }
   ],
   "source": [
    "# stacked generalization with linear meta model on blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from numpy import dstack\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "\n",
    "# load all models\n",
    "n_members = 5\n",
    "members = load_all_models(n_members)\n",
    "print('Loaded %d models' % len(members))\n",
    "\n",
    "# evaluate standalone models on test dataset\n",
    "for model in members:\n",
    "    testy_enc = to_categorical(testy)\n",
    "    _, acc = model.evaluate(testX, testy_enc, verbose=0)\n",
    "    print('Model Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first loads the submodels into a list and evaluates the performance of each. We can see that the best-performing model is the final model with an accuracy of about 81.8%.\n",
    "\n",
    "**Note:** Your specific results may vary, given the stochastic nature of the learning algorithm. Consider running the example a few times and compare the average performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a logistic regression meta-learner is trained on the predicted probabilities from each submodel on the test set, then the entire stacking model is evaluated on the test set. In this case, we can see that the meta-learner outperformed each submodel on the test set, achieving an accuracy of about 83.1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Test Accuracy: 0.831\n"
     ]
    }
   ],
   "source": [
    "# fit stacked model using the ensemble\n",
    "model = fit_stacked_model(members, testX, testy)\n",
    "\n",
    "# evaluate model on test set\n",
    "yhat = stacked_prediction(members, model, testX)\n",
    "acc = accuracy_score(testy, yhat)\n",
    "print('Stacked Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrated Stacking Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using neural networks as submodels, it may be desirable to use a neural network as a meta-learner. Specifically, the sub-networks can be embedded in a larger multi-headed neural network that then learns how to best combine the predictions from each input submodel. It allows the stacking ensemble to be treated as a single large model. The benefit of this approach is that the outputs of the submodels are provided directly to the meta-learner. Further, it is also possible to update the weights of the submodels in conjunction with the meta-learner model if this is desirable. This can be achieved using the Keras functional interface for developing models.\n",
    "\n",
    "After the models are loaded as a list, a larger stacking ensemble model can be defined where each of the loaded models is used as a separate input-head to the model. This requires that all layers in each loaded model be marked as not trainable, so the weights cannot be updated when the new larger model is trained. Keras also requires that each layer has a unique name, therefore the names of each layer in each of the loaded models will have to be updated to indicate to which ensemble member they belong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# update all layers in all models to not be trainable\n",
    "for i in range(len(members)):\n",
    "    model = members[i]\n",
    "    for layer in model.layers:\n",
    "        # make not trainable\n",
    "        layer.trainable = False\n",
    "        \n",
    "        # rename to avoid 'unique layer name' issue\n",
    "        layer._name = 'ensemble_' + str(i+1) + '_' + layer.name\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the submodels have been prepared, we can define the stacking ensemble model. The input layer for each submodel will be used as a separate input head to this new model. This means that k copies of any input data will have to be provided to the model, where k is the number of input models, in this case, 5. The outputs of each of the models can then be merged. In this case, we will use a simple concatenation merge, where a single 15-element vector will be created from the three-class probabilities predicted by each of the five models. We will then define a hidden layer to interpret this input to the meta-learner and an output layer to make its probabilistic prediction. The `define_stacked_model()` function below implements this and will return a stacked generalization neural network model given a list of trained submodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stacked model from multiple member input models\n",
    "def define_stacked_model(members):\n",
    "    # update all layers in all models to not be trainable\n",
    "    for i in range(len(members)):\n",
    "        model = members[i]\n",
    "        for layer in model.layers:\n",
    "            # make not trainable\n",
    "            layer.trainable = False\n",
    "\n",
    "            # rename to avoid 'unique layer name' issue\n",
    "            layer._name = 'ensemble_' + str(i+1) + '_' + layer.name\n",
    "\n",
    "    # define multi-headed input\n",
    "    ensemble_visible = [model.input for model in members]\n",
    "\n",
    "    # concatenate merge output from each model\n",
    "    ensemble_outputs = [model.output for model in members]\n",
    "    merge = concatenate(ensemble_outputs)\n",
    "    hidden = Dense(10, activation='relu')(merge)\n",
    "    output = Dense(3, activation='softmax')(hidden)\n",
    "    model = Model(inputs=ensemble_visible, outputs=output)\n",
    "\n",
    "    # plot graph of ensemble\n",
    "    plot_model(model, show_shapes=True, to_file='model_graph.png')\n",
    "\n",
    "    # compile\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot of the network graph is created when this function is called to give an idea of how the ensemble model fits together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import concatenate, Dense\n",
    "\n",
    "# define ensemble model\n",
    "stacked_model = define_stacked_model(members)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the plot requires that `pygraphviz` is installed. If this is a challenge on your workstation, you can comment out the call to the `plot_model()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is defined, it can be fit. We can fit it directly on the holdout test dataset. Because the submodels are not trainable, their weights will not be updated during training, and only the weights of the new hidden and output layer will be updated. The `fit_stacked_model()` the function below will fit the stacking neural network model for 300 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a stacked model\n",
    "def fit_stacked_model(model, inputX, inputy):\n",
    "    # prepare input data\n",
    "    X = [inputX for _ in range(len(model.input))]\n",
    "\n",
    "    # encode output data\n",
    "    inputy_enc = to_categorical(inputy)\n",
    "\n",
    "    # fit model\n",
    "    model.fit(X, inputy_enc, epochs=300, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call this function providing the defined stacking model and the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# fit stacked model on test dataset\n",
    "fit_stacked_model(stacked_model, testX, testy)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once fit, we can use the new stacked model to predict new data. This is as simple as calling the `predict()` function on the model. One minor change is that we require k copies of the input data in a list to be provided to the model for each k submodel. The `predict_stacked_model()` function below simplifies this process of predicting with the stacking model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction with a stacked model\n",
    "def predict_stacked_model(model, inputX):\n",
    "    # prepare input data\n",
    "    X = [inputX for _ in range(len(model.input))]\n",
    "\n",
    "    # make prediction\n",
    "    return model.predict(X, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call this function to make a prediction for the test dataset and report the accuracy. We would expect the performance of the neural network learner to be better than any individual submodel and perhaps competitive with the linear meta-learner used in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# make predictions and evaluate\n",
    "yhat = predict_stacked_model(stacked_model, testX)\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(testy, yhat)\n",
    "print('Stacked Test Accuracy: %.3f' % acc)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tying all of these elements together, the complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">loaded models/model_1.h5\n",
      ">loaded models/model_2.h5\n",
      ">loaded models/model_3.h5\n",
      ">loaded models/model_4.h5\n",
      ">loaded models/model_5.h5\n",
      "Loaded 5 models\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n",
      "Stacked Test Accuracy: 0.838\n"
     ]
    }
   ],
   "source": [
    "# stacked generalization with neural net meta model on blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import concatenate, Dense\n",
    "from numpy import argmax\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "\n",
    "# load all models\n",
    "n_members = 5\n",
    "members = load_all_models(n_members)\n",
    "print('Loaded %d models' % len(members))\n",
    "\n",
    "# define ensemble model\n",
    "stacked_model = define_stacked_model(members)\n",
    "\n",
    "# fit stacked model on test dataset\n",
    "fit_stacked_model(stacked_model, testX, testy)\n",
    "\n",
    "# make predictions and evaluate\n",
    "yhat = predict_stacked_model(stacked_model, testX)\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(testy, yhat)\n",
    "print('Stacked Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first loads the five submodels.\n",
    "\n",
    "**Note:** Your specific results may vary, given the stochastic nature of the learning algorithm. Consider running the example a few times and compare the average performance.\n",
    "\n",
    "A larger stacking ensemble neural network is defined and fit on the test dataset, then the new model is used to predict the test dataset. We can see that, in this case, the model achieved an accuracy of about 83.8%, outperforming the linear model from the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section lists some ideas for extending the tutorial that you may wish to explore.\n",
    "\n",
    "* **Alternate Meta-Learner**. Update the example to use an alternate meta-learner classifier model to the logistic regression model.\n",
    "* **Single Level 0 Model**. Update the example to use a single level-0 model and compare the results.\n",
    "* **Vary Level 0 Models**. Develop a study demonstrating the relationship between test classification accuracy and the number of submodels used in the stacked ensemble.\n",
    "* **Cross-Validation Stacking Ensemble**. Update the example to use k-fold cross-validation to prepare the training dataset for the meta-learner model.\n",
    "* **Use Raw Input in Meta-Learner**. Update the example so that the meta-learner algorithms take the raw input data for the sample and the output from the submodels and compare performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you discovered how to develop a stacked generalization ensemble for deep learning neural networks. Specifically, you learned:\n",
    "\n",
    "* Stacked generalization is an ensemble method where a new model learns how to best combine the predictions from multiple existing models.\n",
    "* How to develop a stacking model using neural networks as a submodel and a scikit-learn classifier as the meta-learner.\n",
    "* How to develop a stacking model where neural network submodels are embedded in a larger stacking ensemble model for training and prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
