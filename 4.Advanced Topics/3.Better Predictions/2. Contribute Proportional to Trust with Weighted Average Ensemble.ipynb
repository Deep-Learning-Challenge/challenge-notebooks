{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contribute Proportional to Trust With Weighted Average Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A weighted average ensemble is an approach that allows multiple models to contribute to a prediction in proportion to their trust or estimated performance. A modeling averaging ensemble combines the prediction from each model equally and often results in better performance on average than a given single model. Sometimes there are very good models that we wish to contribute more to an ensemble prediction, and perhaps less skillful models that may be useful but should contribute less to an ensemble prediction. In this tutorial, you will discover how to develop a weighted average ensemble of deep learning neural network models in Python with Keras. After completing this tutorial, you will know:\n",
    "\n",
    "* Model averaging ensembles are limited because they require that each ensemble member contribute equally to predictions.\n",
    "* Weighted average ensembles allow the contribution of each ensemble member to a prediction to be weighted proportionally to the trust or performance of the member on a holdout dataset.\n",
    "* How to implement a weighted average ensemble in Keras and compare results to a model averaging ensemble and standalone models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Average Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model averaging is an approach to ensemble learning where each ensemble member contributes an equal amount to the final prediction. In the case of regression, the ensemble prediction is calculated as the average of the member predictions. In the case of predicting a class label, the prediction is calculated as the mode of the member predictions. In the case of predicting a class probability, the prediction can be calculated as the argmax of the summed probabilities for each class label. A limitation of this approach is that each model has an equal contribution to the final prediction made by the ensemble. There is a requirement that all ensemble members have the skill compared to random chance, although some models are known to perform much better or much worse than others.\n",
    "\n",
    "A weighted ensemble is an extension of a model averaging ensemble where the model's performance weights the contribution of each member to the final prediction. The model weights are small positive values, and the sum of all weights equals one, allowing the weights to indicate the percentage of trust or expected performance from each model.\n",
    "\n",
    "Uniform values for the weights (e.g., $ \\frac{1}{k} $ where k is the number of ensemble members) mean that the weighted ensemble acts as a simple averaging ensemble. There is no analytical solution to finding the weights (we cannot calculate them); instead, the value for the weights can be estimated using either the training dataset or a holdout validation dataset. Finding the weights using the same training set used to fit the ensemble members will likely result in an overfit model. A more robust approach is to use a holdout validation dataset unseen by the ensemble members during training.\n",
    "\n",
    "The simplest, perhaps most exhaustive approach would be to grid search weight values between 0 and 1 for each ensemble member. Alternately, an optimization procedure such as a linear solver or gradient descent optimization can be used to estimate the weights using a unit norm weight constraint to ensure that the vector of weights sum to one. Unless the holdout validation dataset is large and representative, a weighted ensemble can overfit compared to a simple averaging ensemble. A simple alternative to adding more weight to a given model without calculating explicit weight coefficients is to add a given model more than once to the ensemble. Although less flexible, it allows a given well-performing model to contribute more than once to a given prediction made by the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Average Ensemble Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will demonstrate how to use the weighted average ensemble to reduce the variance of an MLP on a simple multiclass classification problem. This example provides a template for applying the weighted average ensemble to your neural network for classification and regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a small multiclass classification problem as the basis to demonstrate the weighted averaging ensemble. The scikit-learn class provides the `make_blobs()` function that can be used to create a multiclass classification problem with the prescribed number of samples, input variables, classes, and variance of samples within a class. The problem can be configured to have two input variables (to represent the x and y coordinates of the points) and a standard deviation of 2.0 for points within each group. We will use the same random state (seed for the pseudorandom number generator) to ensure that we always get the same data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are the input and output elements of a dataset that we can model. In order to get a feeling for the complexity of the problem, we can graph each point on a two-dimensional scatter plot and color each point by class value. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# scatter plot of blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib import pyplot\n",
    "from numpy import where\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "\n",
    "# scatter plot for each class value\n",
    "for class_value in range(3):\n",
    "    # select indices of points with the class label\n",
    "    row_ix = where(y == class_value)\n",
    "\n",
    "    # scatter plot for points with a different color\n",
    "    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n",
    "\n",
    "# show plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example creates a scatter plot of the entire dataset. We can see that the standard deviation of 2.0 means that the classes are not linearly separable (separable by a line), causing many ambiguous points. This is desirable because the problem is non-trivial and will allow a neural network model to find many different *good enough* candidate solutions resulting in a high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we define a model, we need to contrive an appropriate problem for the weighted average ensemble. In our problem, the training dataset is relatively small. Specifically, there is a 10:1 ratio of examples in the training dataset to the holdout dataset. This mimics a situation where we may have a vast number of unlabeled examples and a small number of labeled examples with which to train a model. We will create 1,100 data points from the blobs problem. The model will be trained on the first 100 points, and the remaining 1,000 will be held back in a test dataset, unavailable to the model.\n",
    "\n",
    "The problem is a multiclass classification problem, and we will model it using a softmax activation function on the output layer. This means that the model will predict a vector with three elements with the probability that the sample belongs to each of the three classes. Therefore, the first step is to one-hot encode the class values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2) (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "# scatter plot of blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "\n",
    "# one hot encode output variable\n",
    "y = to_categorical(y)\n",
    "\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "print(trainX.shape, testX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can define and compile the model. The model will expect samples with two input variables. The model then has a single hidden layer with 25 nodes and a rectified linear activation function, an output layer with three nodes to predict the probability of each of the three classes, and a softmax activation function. Because the problem is multiclass, we will use the categorical cross-entropy loss function to optimize the model and the efficient Adam flavor of stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=2, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is fit for 500 training epochs, and we will evaluate each epoch on the test set, using the test set as a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.2123 - accuracy: 0.4326 - val_loss: 1.2347 - val_accuracy: 0.3790\n",
      "Epoch 2/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 1.1811 - accuracy: 0.4138 - val_loss: 1.2106 - val_accuracy: 0.3810\n",
      "Epoch 3/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.1320 - accuracy: 0.4439 - val_loss: 1.1865 - val_accuracy: 0.3830\n",
      "Epoch 4/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.1023 - accuracy: 0.4470 - val_loss: 1.1629 - val_accuracy: 0.3840\n",
      "Epoch 5/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.1003 - accuracy: 0.4460 - val_loss: 1.1402 - val_accuracy: 0.3840\n",
      "Epoch 6/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 1.1412 - accuracy: 0.3678 - val_loss: 1.1192 - val_accuracy: 0.3890\n",
      "Epoch 7/500\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 1.0636 - accuracy: 0.4199 - val_loss: 1.0999 - val_accuracy: 0.3920\n",
      "Epoch 8/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 1.0584 - accuracy: 0.4468 - val_loss: 1.0824 - val_accuracy: 0.3940\n",
      "Epoch 9/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 1.0225 - accuracy: 0.4312 - val_loss: 1.0654 - val_accuracy: 0.3970\n",
      "Epoch 10/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.0195 - accuracy: 0.4468 - val_loss: 1.0483 - val_accuracy: 0.4000\n",
      "Epoch 11/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.0206 - accuracy: 0.4446 - val_loss: 1.0314 - val_accuracy: 0.4000\n",
      "Epoch 12/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.9442 - accuracy: 0.4612 - val_loss: 1.0159 - val_accuracy: 0.4020\n",
      "Epoch 13/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.9569 - accuracy: 0.4696 - val_loss: 1.0012 - val_accuracy: 0.4040\n",
      "Epoch 14/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.9478 - accuracy: 0.4540 - val_loss: 0.9873 - val_accuracy: 0.4130\n",
      "Epoch 15/500\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.9440 - accuracy: 0.4434 - val_loss: 0.9731 - val_accuracy: 0.4140\n",
      "Epoch 16/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.8890 - accuracy: 0.4910 - val_loss: 0.9590 - val_accuracy: 0.4210\n",
      "Epoch 17/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.8883 - accuracy: 0.4731 - val_loss: 0.9459 - val_accuracy: 0.4410\n",
      "Epoch 18/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.8794 - accuracy: 0.4786 - val_loss: 0.9335 - val_accuracy: 0.4500\n",
      "Epoch 19/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.8915 - accuracy: 0.4474 - val_loss: 0.9219 - val_accuracy: 0.4660\n",
      "Epoch 20/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.8462 - accuracy: 0.4957 - val_loss: 0.9109 - val_accuracy: 0.4770\n",
      "Epoch 21/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.8240 - accuracy: 0.5267 - val_loss: 0.9003 - val_accuracy: 0.4820\n",
      "Epoch 22/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.8759 - accuracy: 0.4941 - val_loss: 0.8889 - val_accuracy: 0.4940\n",
      "Epoch 23/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.8381 - accuracy: 0.5210 - val_loss: 0.8783 - val_accuracy: 0.5000\n",
      "Epoch 24/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.8300 - accuracy: 0.5570 - val_loss: 0.8673 - val_accuracy: 0.5090\n",
      "Epoch 25/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.7971 - accuracy: 0.5778 - val_loss: 0.8569 - val_accuracy: 0.5220\n",
      "Epoch 26/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.8209 - accuracy: 0.5454 - val_loss: 0.8469 - val_accuracy: 0.5340\n",
      "Epoch 27/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.7757 - accuracy: 0.5775 - val_loss: 0.8377 - val_accuracy: 0.5440\n",
      "Epoch 28/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.7528 - accuracy: 0.6129 - val_loss: 0.8274 - val_accuracy: 0.5550\n",
      "Epoch 29/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.7488 - accuracy: 0.6312 - val_loss: 0.8170 - val_accuracy: 0.5610\n",
      "Epoch 30/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.7487 - accuracy: 0.6148 - val_loss: 0.8067 - val_accuracy: 0.5710\n",
      "Epoch 31/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.7574 - accuracy: 0.6188 - val_loss: 0.7980 - val_accuracy: 0.5780\n",
      "Epoch 32/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.7274 - accuracy: 0.6322 - val_loss: 0.7902 - val_accuracy: 0.5790\n",
      "Epoch 33/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.7124 - accuracy: 0.6353 - val_loss: 0.7828 - val_accuracy: 0.5830\n",
      "Epoch 34/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.7130 - accuracy: 0.6334 - val_loss: 0.7769 - val_accuracy: 0.5810\n",
      "Epoch 35/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.7261 - accuracy: 0.6114 - val_loss: 0.7704 - val_accuracy: 0.5830\n",
      "Epoch 36/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.6859 - accuracy: 0.6239 - val_loss: 0.7640 - val_accuracy: 0.5910\n",
      "Epoch 37/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.7134 - accuracy: 0.6206 - val_loss: 0.7579 - val_accuracy: 0.5970\n",
      "Epoch 38/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6617 - accuracy: 0.6447 - val_loss: 0.7520 - val_accuracy: 0.6030\n",
      "Epoch 39/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6980 - accuracy: 0.6300 - val_loss: 0.7455 - val_accuracy: 0.6060\n",
      "Epoch 40/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6873 - accuracy: 0.6359 - val_loss: 0.7390 - val_accuracy: 0.6110\n",
      "Epoch 41/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.7069 - accuracy: 0.6543 - val_loss: 0.7332 - val_accuracy: 0.6170\n",
      "Epoch 42/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6494 - accuracy: 0.6814 - val_loss: 0.7282 - val_accuracy: 0.6200\n",
      "Epoch 43/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6683 - accuracy: 0.6741 - val_loss: 0.7233 - val_accuracy: 0.6240\n",
      "Epoch 44/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6939 - accuracy: 0.6387 - val_loss: 0.7172 - val_accuracy: 0.6290\n",
      "Epoch 45/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6164 - accuracy: 0.7137 - val_loss: 0.7118 - val_accuracy: 0.6310\n",
      "Epoch 46/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6566 - accuracy: 0.6595 - val_loss: 0.7065 - val_accuracy: 0.6360\n",
      "Epoch 47/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6088 - accuracy: 0.7126 - val_loss: 0.7016 - val_accuracy: 0.6400\n",
      "Epoch 48/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6469 - accuracy: 0.6739 - val_loss: 0.6970 - val_accuracy: 0.6410\n",
      "Epoch 49/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6323 - accuracy: 0.6822 - val_loss: 0.6931 - val_accuracy: 0.6430\n",
      "Epoch 50/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6152 - accuracy: 0.7069 - val_loss: 0.6889 - val_accuracy: 0.6510\n",
      "Epoch 51/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5937 - accuracy: 0.7342 - val_loss: 0.6849 - val_accuracy: 0.6580\n",
      "Epoch 52/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6062 - accuracy: 0.7040 - val_loss: 0.6812 - val_accuracy: 0.6600\n",
      "Epoch 53/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5926 - accuracy: 0.7196 - val_loss: 0.6771 - val_accuracy: 0.6650\n",
      "Epoch 54/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6301 - accuracy: 0.6913 - val_loss: 0.6734 - val_accuracy: 0.6660\n",
      "Epoch 55/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5962 - accuracy: 0.7340 - val_loss: 0.6698 - val_accuracy: 0.6700\n",
      "Epoch 56/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6444 - accuracy: 0.6819 - val_loss: 0.6674 - val_accuracy: 0.6710\n",
      "Epoch 57/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5892 - accuracy: 0.7288 - val_loss: 0.6662 - val_accuracy: 0.6720\n",
      "Epoch 58/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5857 - accuracy: 0.7112 - val_loss: 0.6651 - val_accuracy: 0.6720\n",
      "Epoch 59/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5970 - accuracy: 0.7422 - val_loss: 0.6641 - val_accuracy: 0.6710\n",
      "Epoch 60/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6099 - accuracy: 0.6984 - val_loss: 0.6614 - val_accuracy: 0.6710\n",
      "Epoch 61/500\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.5984 - accuracy: 0.7173 - val_loss: 0.6580 - val_accuracy: 0.6750\n",
      "Epoch 62/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5902 - accuracy: 0.7048 - val_loss: 0.6543 - val_accuracy: 0.6770\n",
      "Epoch 63/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5954 - accuracy: 0.7359 - val_loss: 0.6513 - val_accuracy: 0.6760\n",
      "Epoch 64/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5924 - accuracy: 0.7090 - val_loss: 0.6488 - val_accuracy: 0.6760\n",
      "Epoch 65/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5502 - accuracy: 0.7536 - val_loss: 0.6480 - val_accuracy: 0.6750\n",
      "Epoch 66/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5570 - accuracy: 0.7493 - val_loss: 0.6467 - val_accuracy: 0.6740\n",
      "Epoch 67/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5886 - accuracy: 0.7316 - val_loss: 0.6441 - val_accuracy: 0.6770\n",
      "Epoch 68/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5893 - accuracy: 0.7066 - val_loss: 0.6423 - val_accuracy: 0.6790\n",
      "Epoch 69/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.6008 - accuracy: 0.7180 - val_loss: 0.6406 - val_accuracy: 0.6810\n",
      "Epoch 70/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.6126 - accuracy: 0.7045 - val_loss: 0.6380 - val_accuracy: 0.6820\n",
      "Epoch 71/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5932 - accuracy: 0.6974 - val_loss: 0.6351 - val_accuracy: 0.6860\n",
      "Epoch 72/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5755 - accuracy: 0.7068 - val_loss: 0.6317 - val_accuracy: 0.6860\n",
      "Epoch 73/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5423 - accuracy: 0.7255 - val_loss: 0.6292 - val_accuracy: 0.6890\n",
      "Epoch 74/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5651 - accuracy: 0.7370 - val_loss: 0.6277 - val_accuracy: 0.6900\n",
      "Epoch 75/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5543 - accuracy: 0.7276 - val_loss: 0.6272 - val_accuracy: 0.6860\n",
      "Epoch 76/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5795 - accuracy: 0.6984 - val_loss: 0.6269 - val_accuracy: 0.6860\n",
      "Epoch 77/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5543 - accuracy: 0.7253 - val_loss: 0.6260 - val_accuracy: 0.6850\n",
      "Epoch 78/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5320 - accuracy: 0.7670 - val_loss: 0.6245 - val_accuracy: 0.6870\n",
      "Epoch 79/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5503 - accuracy: 0.7420 - val_loss: 0.6225 - val_accuracy: 0.6880\n",
      "Epoch 80/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5639 - accuracy: 0.7234 - val_loss: 0.6209 - val_accuracy: 0.6910\n",
      "Epoch 81/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5399 - accuracy: 0.7318 - val_loss: 0.6191 - val_accuracy: 0.6910\n",
      "Epoch 82/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5135 - accuracy: 0.7660 - val_loss: 0.6169 - val_accuracy: 0.6980\n",
      "Epoch 83/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5467 - accuracy: 0.7399 - val_loss: 0.6154 - val_accuracy: 0.7000\n",
      "Epoch 84/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5432 - accuracy: 0.7451 - val_loss: 0.6153 - val_accuracy: 0.6970\n",
      "Epoch 85/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5771 - accuracy: 0.6943 - val_loss: 0.6156 - val_accuracy: 0.6960\n",
      "Epoch 86/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5681 - accuracy: 0.7160 - val_loss: 0.6154 - val_accuracy: 0.6960\n",
      "Epoch 87/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5707 - accuracy: 0.7274 - val_loss: 0.6147 - val_accuracy: 0.6960\n",
      "Epoch 88/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5279 - accuracy: 0.7502 - val_loss: 0.6131 - val_accuracy: 0.6960\n",
      "Epoch 89/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5640 - accuracy: 0.7264 - val_loss: 0.6110 - val_accuracy: 0.6970\n",
      "Epoch 90/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5703 - accuracy: 0.7358 - val_loss: 0.6090 - val_accuracy: 0.6990\n",
      "Epoch 91/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5231 - accuracy: 0.7462 - val_loss: 0.6069 - val_accuracy: 0.7000\n",
      "Epoch 92/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5639 - accuracy: 0.7314 - val_loss: 0.6054 - val_accuracy: 0.7010\n",
      "Epoch 93/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5621 - accuracy: 0.6970 - val_loss: 0.6048 - val_accuracy: 0.7040\n",
      "Epoch 94/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5449 - accuracy: 0.7356 - val_loss: 0.6034 - val_accuracy: 0.7040\n",
      "Epoch 95/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5136 - accuracy: 0.7481 - val_loss: 0.6020 - val_accuracy: 0.7050\n",
      "Epoch 96/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5320 - accuracy: 0.7408 - val_loss: 0.6013 - val_accuracy: 0.7050\n",
      "Epoch 97/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5033 - accuracy: 0.7627 - val_loss: 0.6002 - val_accuracy: 0.7040\n",
      "Epoch 98/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5314 - accuracy: 0.7283 - val_loss: 0.5996 - val_accuracy: 0.7060\n",
      "Epoch 99/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4971 - accuracy: 0.7658 - val_loss: 0.5992 - val_accuracy: 0.7060\n",
      "Epoch 100/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5698 - accuracy: 0.7146 - val_loss: 0.5990 - val_accuracy: 0.7050\n",
      "Epoch 101/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5030 - accuracy: 0.7668 - val_loss: 0.5976 - val_accuracy: 0.7070\n",
      "Epoch 102/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5455 - accuracy: 0.7305 - val_loss: 0.5954 - val_accuracy: 0.7120\n",
      "Epoch 103/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5189 - accuracy: 0.7335 - val_loss: 0.5935 - val_accuracy: 0.7170\n",
      "Epoch 104/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5580 - accuracy: 0.7189 - val_loss: 0.5909 - val_accuracy: 0.7240\n",
      "Epoch 105/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5114 - accuracy: 0.7781 - val_loss: 0.5889 - val_accuracy: 0.7280\n",
      "Epoch 106/500\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.5131 - accuracy: 0.7417 - val_loss: 0.5869 - val_accuracy: 0.7300\n",
      "Epoch 107/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5186 - accuracy: 0.7571 - val_loss: 0.5850 - val_accuracy: 0.7310\n",
      "Epoch 108/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5328 - accuracy: 0.7292 - val_loss: 0.5832 - val_accuracy: 0.7380\n",
      "Epoch 109/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5087 - accuracy: 0.7378 - val_loss: 0.5818 - val_accuracy: 0.7400\n",
      "Epoch 110/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5214 - accuracy: 0.7161 - val_loss: 0.5811 - val_accuracy: 0.7440\n",
      "Epoch 111/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5468 - accuracy: 0.7274 - val_loss: 0.5804 - val_accuracy: 0.7410\n",
      "Epoch 112/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4867 - accuracy: 0.7691 - val_loss: 0.5799 - val_accuracy: 0.7390\n",
      "Epoch 113/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5232 - accuracy: 0.7410 - val_loss: 0.5795 - val_accuracy: 0.7370\n",
      "Epoch 114/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5307 - accuracy: 0.7531 - val_loss: 0.5794 - val_accuracy: 0.7350\n",
      "Epoch 115/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5373 - accuracy: 0.7344 - val_loss: 0.5795 - val_accuracy: 0.7320\n",
      "Epoch 116/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5117 - accuracy: 0.7615 - val_loss: 0.5802 - val_accuracy: 0.7310\n",
      "Epoch 117/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5148 - accuracy: 0.7429 - val_loss: 0.5807 - val_accuracy: 0.7290\n",
      "Epoch 118/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4914 - accuracy: 0.7625 - val_loss: 0.5805 - val_accuracy: 0.7290\n",
      "Epoch 119/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5331 - accuracy: 0.7271 - val_loss: 0.5789 - val_accuracy: 0.7320\n",
      "Epoch 120/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5518 - accuracy: 0.7219 - val_loss: 0.5771 - val_accuracy: 0.7340\n",
      "Epoch 121/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5234 - accuracy: 0.7542 - val_loss: 0.5761 - val_accuracy: 0.7350\n",
      "Epoch 122/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5124 - accuracy: 0.7479 - val_loss: 0.5751 - val_accuracy: 0.7350\n",
      "Epoch 123/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5462 - accuracy: 0.7073 - val_loss: 0.5745 - val_accuracy: 0.7360\n",
      "Epoch 124/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4915 - accuracy: 0.7573 - val_loss: 0.5736 - val_accuracy: 0.7350\n",
      "Epoch 125/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5279 - accuracy: 0.7385 - val_loss: 0.5728 - val_accuracy: 0.7350\n",
      "Epoch 126/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4774 - accuracy: 0.7719 - val_loss: 0.5718 - val_accuracy: 0.7350\n",
      "Epoch 127/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5308 - accuracy: 0.7292 - val_loss: 0.5708 - val_accuracy: 0.7370\n",
      "Epoch 128/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5029 - accuracy: 0.7594 - val_loss: 0.5706 - val_accuracy: 0.7350\n",
      "Epoch 129/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5158 - accuracy: 0.7531 - val_loss: 0.5700 - val_accuracy: 0.7340\n",
      "Epoch 130/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5067 - accuracy: 0.7562 - val_loss: 0.5694 - val_accuracy: 0.7370\n",
      "Epoch 131/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5068 - accuracy: 0.7615 - val_loss: 0.5688 - val_accuracy: 0.7410\n",
      "Epoch 132/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5341 - accuracy: 0.7156 - val_loss: 0.5682 - val_accuracy: 0.7410\n",
      "Epoch 133/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4602 - accuracy: 0.7769 - val_loss: 0.5677 - val_accuracy: 0.7420\n",
      "Epoch 134/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4840 - accuracy: 0.7986 - val_loss: 0.5672 - val_accuracy: 0.7400\n",
      "Epoch 135/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5094 - accuracy: 0.7590 - val_loss: 0.5659 - val_accuracy: 0.7450\n",
      "Epoch 136/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5366 - accuracy: 0.7455 - val_loss: 0.5649 - val_accuracy: 0.7470\n",
      "Epoch 137/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4858 - accuracy: 0.7830 - val_loss: 0.5631 - val_accuracy: 0.7520\n",
      "Epoch 138/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4903 - accuracy: 0.7820 - val_loss: 0.5617 - val_accuracy: 0.7590\n",
      "Epoch 139/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5375 - accuracy: 0.7467 - val_loss: 0.5606 - val_accuracy: 0.7570\n",
      "Epoch 140/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4934 - accuracy: 0.7767 - val_loss: 0.5597 - val_accuracy: 0.7570\n",
      "Epoch 141/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5225 - accuracy: 0.7528 - val_loss: 0.5594 - val_accuracy: 0.7600\n",
      "Epoch 142/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5465 - accuracy: 0.7340 - val_loss: 0.5587 - val_accuracy: 0.7590\n",
      "Epoch 143/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5051 - accuracy: 0.7663 - val_loss: 0.5584 - val_accuracy: 0.7590\n",
      "Epoch 144/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4851 - accuracy: 0.7790 - val_loss: 0.5584 - val_accuracy: 0.7570\n",
      "Epoch 145/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4766 - accuracy: 0.7530 - val_loss: 0.5584 - val_accuracy: 0.7560\n",
      "Epoch 146/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4673 - accuracy: 0.7842 - val_loss: 0.5590 - val_accuracy: 0.7540\n",
      "Epoch 147/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4839 - accuracy: 0.7757 - val_loss: 0.5601 - val_accuracy: 0.7490\n",
      "Epoch 148/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4652 - accuracy: 0.7924 - val_loss: 0.5597 - val_accuracy: 0.7520\n",
      "Epoch 149/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4952 - accuracy: 0.7622 - val_loss: 0.5581 - val_accuracy: 0.7560\n",
      "Epoch 150/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4919 - accuracy: 0.7715 - val_loss: 0.5574 - val_accuracy: 0.7590\n",
      "Epoch 151/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4931 - accuracy: 0.7601 - val_loss: 0.5576 - val_accuracy: 0.7570\n",
      "Epoch 152/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4661 - accuracy: 0.7955 - val_loss: 0.5578 - val_accuracy: 0.7530\n",
      "Epoch 153/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4663 - accuracy: 0.7778 - val_loss: 0.5562 - val_accuracy: 0.7580\n",
      "Epoch 154/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4708 - accuracy: 0.7872 - val_loss: 0.5550 - val_accuracy: 0.7620\n",
      "Epoch 155/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4554 - accuracy: 0.7912 - val_loss: 0.5541 - val_accuracy: 0.7650\n",
      "Epoch 156/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4807 - accuracy: 0.7797 - val_loss: 0.5525 - val_accuracy: 0.7680\n",
      "Epoch 157/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4685 - accuracy: 0.7839 - val_loss: 0.5499 - val_accuracy: 0.7700\n",
      "Epoch 158/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4362 - accuracy: 0.8057 - val_loss: 0.5475 - val_accuracy: 0.7700\n",
      "Epoch 159/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4662 - accuracy: 0.7932 - val_loss: 0.5463 - val_accuracy: 0.7700\n",
      "Epoch 160/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4518 - accuracy: 0.8035 - val_loss: 0.5448 - val_accuracy: 0.7740\n",
      "Epoch 161/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5133 - accuracy: 0.7618 - val_loss: 0.5432 - val_accuracy: 0.7760\n",
      "Epoch 162/500\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.4632 - accuracy: 0.8065 - val_loss: 0.5422 - val_accuracy: 0.7760\n",
      "Epoch 163/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4656 - accuracy: 0.8045 - val_loss: 0.5419 - val_accuracy: 0.7750\n",
      "Epoch 164/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4582 - accuracy: 0.7993 - val_loss: 0.5413 - val_accuracy: 0.7750\n",
      "Epoch 165/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4682 - accuracy: 0.7879 - val_loss: 0.5409 - val_accuracy: 0.7750\n",
      "Epoch 166/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5071 - accuracy: 0.7712 - val_loss: 0.5417 - val_accuracy: 0.7760\n",
      "Epoch 167/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4520 - accuracy: 0.8056 - val_loss: 0.5424 - val_accuracy: 0.7690\n",
      "Epoch 168/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4654 - accuracy: 0.7964 - val_loss: 0.5416 - val_accuracy: 0.7690\n",
      "Epoch 169/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4843 - accuracy: 0.7889 - val_loss: 0.5408 - val_accuracy: 0.7720\n",
      "Epoch 170/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4857 - accuracy: 0.7535 - val_loss: 0.5405 - val_accuracy: 0.7680\n",
      "Epoch 171/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4654 - accuracy: 0.8004 - val_loss: 0.5398 - val_accuracy: 0.7680\n",
      "Epoch 172/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4900 - accuracy: 0.7806 - val_loss: 0.5389 - val_accuracy: 0.7710\n",
      "Epoch 173/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4559 - accuracy: 0.7879 - val_loss: 0.5380 - val_accuracy: 0.7710\n",
      "Epoch 174/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4496 - accuracy: 0.8098 - val_loss: 0.5369 - val_accuracy: 0.7720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4618 - accuracy: 0.8190 - val_loss: 0.5353 - val_accuracy: 0.7760\n",
      "Epoch 176/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4697 - accuracy: 0.7908 - val_loss: 0.5344 - val_accuracy: 0.7800\n",
      "Epoch 177/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4643 - accuracy: 0.8065 - val_loss: 0.5340 - val_accuracy: 0.7810\n",
      "Epoch 178/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4637 - accuracy: 0.7960 - val_loss: 0.5350 - val_accuracy: 0.7780\n",
      "Epoch 179/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4830 - accuracy: 0.7867 - val_loss: 0.5358 - val_accuracy: 0.7740\n",
      "Epoch 180/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4635 - accuracy: 0.7867 - val_loss: 0.5361 - val_accuracy: 0.7730\n",
      "Epoch 181/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4285 - accuracy: 0.8221 - val_loss: 0.5354 - val_accuracy: 0.7740\n",
      "Epoch 182/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4678 - accuracy: 0.7981 - val_loss: 0.5353 - val_accuracy: 0.7740\n",
      "Epoch 183/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4479 - accuracy: 0.8127 - val_loss: 0.5349 - val_accuracy: 0.7760\n",
      "Epoch 184/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4630 - accuracy: 0.7992 - val_loss: 0.5352 - val_accuracy: 0.7750\n",
      "Epoch 185/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4474 - accuracy: 0.7931 - val_loss: 0.5366 - val_accuracy: 0.7710\n",
      "Epoch 186/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4530 - accuracy: 0.7837 - val_loss: 0.5377 - val_accuracy: 0.7680\n",
      "Epoch 187/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4855 - accuracy: 0.7587 - val_loss: 0.5386 - val_accuracy: 0.7680\n",
      "Epoch 188/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4253 - accuracy: 0.8191 - val_loss: 0.5375 - val_accuracy: 0.7690\n",
      "Epoch 189/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4797 - accuracy: 0.7941 - val_loss: 0.5358 - val_accuracy: 0.7710\n",
      "Epoch 190/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4247 - accuracy: 0.8150 - val_loss: 0.5340 - val_accuracy: 0.7710\n",
      "Epoch 191/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4402 - accuracy: 0.8118 - val_loss: 0.5317 - val_accuracy: 0.7740\n",
      "Epoch 192/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4436 - accuracy: 0.8096 - val_loss: 0.5291 - val_accuracy: 0.7800\n",
      "Epoch 193/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4660 - accuracy: 0.7804 - val_loss: 0.5267 - val_accuracy: 0.7820\n",
      "Epoch 194/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4385 - accuracy: 0.8242 - val_loss: 0.5251 - val_accuracy: 0.7840\n",
      "Epoch 195/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4593 - accuracy: 0.7940 - val_loss: 0.5244 - val_accuracy: 0.7840\n",
      "Epoch 196/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4627 - accuracy: 0.7877 - val_loss: 0.5241 - val_accuracy: 0.7830\n",
      "Epoch 197/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4722 - accuracy: 0.7877 - val_loss: 0.5242 - val_accuracy: 0.7810\n",
      "Epoch 198/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4645 - accuracy: 0.7783 - val_loss: 0.5242 - val_accuracy: 0.7800\n",
      "Epoch 199/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4234 - accuracy: 0.8085 - val_loss: 0.5237 - val_accuracy: 0.7810\n",
      "Epoch 200/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4953 - accuracy: 0.7658 - val_loss: 0.5230 - val_accuracy: 0.7820\n",
      "Epoch 201/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4499 - accuracy: 0.8252 - val_loss: 0.5228 - val_accuracy: 0.7830\n",
      "Epoch 202/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4447 - accuracy: 0.8148 - val_loss: 0.5225 - val_accuracy: 0.7830\n",
      "Epoch 203/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4412 - accuracy: 0.8023 - val_loss: 0.5216 - val_accuracy: 0.7820\n",
      "Epoch 204/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4260 - accuracy: 0.8294 - val_loss: 0.5206 - val_accuracy: 0.7850\n",
      "Epoch 205/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4088 - accuracy: 0.8212 - val_loss: 0.5195 - val_accuracy: 0.7910\n",
      "Epoch 206/500\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.4345 - accuracy: 0.8129 - val_loss: 0.5188 - val_accuracy: 0.7920\n",
      "Epoch 207/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4628 - accuracy: 0.7693 - val_loss: 0.5187 - val_accuracy: 0.7910\n",
      "Epoch 208/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4339 - accuracy: 0.7953 - val_loss: 0.5188 - val_accuracy: 0.7920\n",
      "Epoch 209/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4499 - accuracy: 0.7787 - val_loss: 0.5186 - val_accuracy: 0.7930\n",
      "Epoch 210/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4466 - accuracy: 0.7745 - val_loss: 0.5186 - val_accuracy: 0.7930\n",
      "Epoch 211/500\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.4146 - accuracy: 0.7993 - val_loss: 0.5188 - val_accuracy: 0.7880\n",
      "Epoch 212/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4634 - accuracy: 0.7848 - val_loss: 0.5188 - val_accuracy: 0.7860\n",
      "Epoch 213/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4174 - accuracy: 0.8148 - val_loss: 0.5191 - val_accuracy: 0.7840\n",
      "Epoch 214/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4625 - accuracy: 0.7917 - val_loss: 0.5192 - val_accuracy: 0.7860\n",
      "Epoch 215/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4705 - accuracy: 0.7896 - val_loss: 0.5188 - val_accuracy: 0.7880\n",
      "Epoch 216/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4523 - accuracy: 0.8188 - val_loss: 0.5183 - val_accuracy: 0.7910\n",
      "Epoch 217/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4138 - accuracy: 0.8158 - val_loss: 0.5179 - val_accuracy: 0.7890\n",
      "Epoch 218/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4240 - accuracy: 0.8315 - val_loss: 0.5178 - val_accuracy: 0.7900\n",
      "Epoch 219/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4398 - accuracy: 0.7815 - val_loss: 0.5179 - val_accuracy: 0.7890\n",
      "Epoch 220/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4626 - accuracy: 0.7928 - val_loss: 0.5183 - val_accuracy: 0.7870\n",
      "Epoch 221/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4592 - accuracy: 0.7740 - val_loss: 0.5174 - val_accuracy: 0.7880\n",
      "Epoch 222/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4190 - accuracy: 0.8219 - val_loss: 0.5170 - val_accuracy: 0.7870\n",
      "Epoch 223/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4347 - accuracy: 0.8157 - val_loss: 0.5170 - val_accuracy: 0.7860\n",
      "Epoch 224/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4197 - accuracy: 0.8178 - val_loss: 0.5168 - val_accuracy: 0.7860\n",
      "Epoch 225/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4406 - accuracy: 0.8219 - val_loss: 0.5167 - val_accuracy: 0.7890\n",
      "Epoch 226/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4210 - accuracy: 0.8032 - val_loss: 0.5152 - val_accuracy: 0.7910\n",
      "Epoch 227/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4134 - accuracy: 0.8384 - val_loss: 0.5112 - val_accuracy: 0.7980\n",
      "Epoch 228/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4430 - accuracy: 0.8002 - val_loss: 0.5090 - val_accuracy: 0.7990\n",
      "Epoch 229/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4411 - accuracy: 0.8075 - val_loss: 0.5079 - val_accuracy: 0.7990\n",
      "Epoch 230/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4179 - accuracy: 0.8065 - val_loss: 0.5072 - val_accuracy: 0.7980\n",
      "Epoch 231/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4224 - accuracy: 0.8096 - val_loss: 0.5065 - val_accuracy: 0.7990\n",
      "Epoch 232/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4426 - accuracy: 0.7877 - val_loss: 0.5060 - val_accuracy: 0.7970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4406 - accuracy: 0.7783 - val_loss: 0.5058 - val_accuracy: 0.7960\n",
      "Epoch 234/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3984 - accuracy: 0.8148 - val_loss: 0.5056 - val_accuracy: 0.7980\n",
      "Epoch 235/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4545 - accuracy: 0.7804 - val_loss: 0.5055 - val_accuracy: 0.7960\n",
      "Epoch 236/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4297 - accuracy: 0.7971 - val_loss: 0.5058 - val_accuracy: 0.7950\n",
      "Epoch 237/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3917 - accuracy: 0.8428 - val_loss: 0.5057 - val_accuracy: 0.7940\n",
      "Epoch 238/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4055 - accuracy: 0.8146 - val_loss: 0.5049 - val_accuracy: 0.7940\n",
      "Epoch 239/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4539 - accuracy: 0.7865 - val_loss: 0.5046 - val_accuracy: 0.7940\n",
      "Epoch 240/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4383 - accuracy: 0.8115 - val_loss: 0.5046 - val_accuracy: 0.7940\n",
      "Epoch 241/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3916 - accuracy: 0.8342 - val_loss: 0.5050 - val_accuracy: 0.7930\n",
      "Epoch 242/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4484 - accuracy: 0.8103 - val_loss: 0.5047 - val_accuracy: 0.7910\n",
      "Epoch 243/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4307 - accuracy: 0.8185 - val_loss: 0.5046 - val_accuracy: 0.7910\n",
      "Epoch 244/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4238 - accuracy: 0.8372 - val_loss: 0.5042 - val_accuracy: 0.7910\n",
      "Epoch 245/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4164 - accuracy: 0.8155 - val_loss: 0.5034 - val_accuracy: 0.7940\n",
      "Epoch 246/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4488 - accuracy: 0.7936 - val_loss: 0.5020 - val_accuracy: 0.7950\n",
      "Epoch 247/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4194 - accuracy: 0.8207 - val_loss: 0.5004 - val_accuracy: 0.7940\n",
      "Epoch 248/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4481 - accuracy: 0.7782 - val_loss: 0.4991 - val_accuracy: 0.7950\n",
      "Epoch 249/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4237 - accuracy: 0.8002 - val_loss: 0.4983 - val_accuracy: 0.7980\n",
      "Epoch 250/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4296 - accuracy: 0.7929 - val_loss: 0.4977 - val_accuracy: 0.7980\n",
      "Epoch 251/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4278 - accuracy: 0.7929 - val_loss: 0.4975 - val_accuracy: 0.7940\n",
      "Epoch 252/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4485 - accuracy: 0.7877 - val_loss: 0.4974 - val_accuracy: 0.7940\n",
      "Epoch 253/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4033 - accuracy: 0.8124 - val_loss: 0.4975 - val_accuracy: 0.7920\n",
      "Epoch 254/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3946 - accuracy: 0.8280 - val_loss: 0.4978 - val_accuracy: 0.7920\n",
      "Epoch 255/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4133 - accuracy: 0.8259 - val_loss: 0.4971 - val_accuracy: 0.7920\n",
      "Epoch 256/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4189 - accuracy: 0.8145 - val_loss: 0.4973 - val_accuracy: 0.7920\n",
      "Epoch 257/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4204 - accuracy: 0.7938 - val_loss: 0.4976 - val_accuracy: 0.7940\n",
      "Epoch 258/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3928 - accuracy: 0.8292 - val_loss: 0.4971 - val_accuracy: 0.7940\n",
      "Epoch 259/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4363 - accuracy: 0.7907 - val_loss: 0.4968 - val_accuracy: 0.7960\n",
      "Epoch 260/500\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3989 - accuracy: 0.8188 - val_loss: 0.4966 - val_accuracy: 0.7990\n",
      "Epoch 261/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4296 - accuracy: 0.7846 - val_loss: 0.4961 - val_accuracy: 0.7990\n",
      "Epoch 262/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3918 - accuracy: 0.8346 - val_loss: 0.4959 - val_accuracy: 0.7960\n",
      "Epoch 263/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4352 - accuracy: 0.7752 - val_loss: 0.4954 - val_accuracy: 0.7950\n",
      "Epoch 264/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4099 - accuracy: 0.7804 - val_loss: 0.4954 - val_accuracy: 0.7950\n",
      "Epoch 265/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4164 - accuracy: 0.8094 - val_loss: 0.4949 - val_accuracy: 0.7960\n",
      "Epoch 266/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4130 - accuracy: 0.8238 - val_loss: 0.4945 - val_accuracy: 0.7950\n",
      "Epoch 267/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4083 - accuracy: 0.8207 - val_loss: 0.4940 - val_accuracy: 0.7970\n",
      "Epoch 268/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4092 - accuracy: 0.8374 - val_loss: 0.4928 - val_accuracy: 0.7960\n",
      "Epoch 269/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4012 - accuracy: 0.8249 - val_loss: 0.4922 - val_accuracy: 0.7960\n",
      "Epoch 270/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4459 - accuracy: 0.8061 - val_loss: 0.4916 - val_accuracy: 0.7980\n",
      "Epoch 271/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3967 - accuracy: 0.8353 - val_loss: 0.4914 - val_accuracy: 0.7970\n",
      "Epoch 272/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3893 - accuracy: 0.8332 - val_loss: 0.4898 - val_accuracy: 0.7990\n",
      "Epoch 273/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4003 - accuracy: 0.8249 - val_loss: 0.4888 - val_accuracy: 0.7980\n",
      "Epoch 274/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4226 - accuracy: 0.7969 - val_loss: 0.4888 - val_accuracy: 0.7960\n",
      "Epoch 275/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3955 - accuracy: 0.8002 - val_loss: 0.4889 - val_accuracy: 0.7960\n",
      "Epoch 276/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3814 - accuracy: 0.8273 - val_loss: 0.4891 - val_accuracy: 0.7960\n",
      "Epoch 277/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4039 - accuracy: 0.8054 - val_loss: 0.4894 - val_accuracy: 0.7960\n",
      "Epoch 278/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4175 - accuracy: 0.7959 - val_loss: 0.4897 - val_accuracy: 0.7970\n",
      "Epoch 279/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3941 - accuracy: 0.8290 - val_loss: 0.4908 - val_accuracy: 0.7980\n",
      "Epoch 280/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3878 - accuracy: 0.8332 - val_loss: 0.4929 - val_accuracy: 0.7990\n",
      "Epoch 281/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3847 - accuracy: 0.8403 - val_loss: 0.4941 - val_accuracy: 0.7950\n",
      "Epoch 282/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3686 - accuracy: 0.8466 - val_loss: 0.4953 - val_accuracy: 0.7940\n",
      "Epoch 283/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4154 - accuracy: 0.8339 - val_loss: 0.4959 - val_accuracy: 0.7940\n",
      "Epoch 284/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4073 - accuracy: 0.8619 - val_loss: 0.4962 - val_accuracy: 0.7930\n",
      "Epoch 285/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3786 - accuracy: 0.8598 - val_loss: 0.4953 - val_accuracy: 0.7940\n",
      "Epoch 286/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4132 - accuracy: 0.8400 - val_loss: 0.4926 - val_accuracy: 0.8000\n",
      "Epoch 287/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3893 - accuracy: 0.8589 - val_loss: 0.4911 - val_accuracy: 0.8000\n",
      "Epoch 288/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3742 - accuracy: 0.8580 - val_loss: 0.4899 - val_accuracy: 0.8000\n",
      "Epoch 289/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4208 - accuracy: 0.8112 - val_loss: 0.4891 - val_accuracy: 0.8000\n",
      "Epoch 290/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3627 - accuracy: 0.8518 - val_loss: 0.4890 - val_accuracy: 0.8010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 291/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4063 - accuracy: 0.8341 - val_loss: 0.4893 - val_accuracy: 0.7990\n",
      "Epoch 292/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4001 - accuracy: 0.8525 - val_loss: 0.4910 - val_accuracy: 0.7980\n",
      "Epoch 293/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3601 - accuracy: 0.8756 - val_loss: 0.4926 - val_accuracy: 0.7930\n",
      "Epoch 294/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3627 - accuracy: 0.8745 - val_loss: 0.4933 - val_accuracy: 0.7900\n",
      "Epoch 295/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3730 - accuracy: 0.8620 - val_loss: 0.4921 - val_accuracy: 0.7890\n",
      "Epoch 296/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3963 - accuracy: 0.8216 - val_loss: 0.4877 - val_accuracy: 0.7950\n",
      "Epoch 297/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4061 - accuracy: 0.8329 - val_loss: 0.4858 - val_accuracy: 0.7970\n",
      "Epoch 298/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4019 - accuracy: 0.8412 - val_loss: 0.4846 - val_accuracy: 0.7990\n",
      "Epoch 299/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3902 - accuracy: 0.8310 - val_loss: 0.4841 - val_accuracy: 0.7990\n",
      "Epoch 300/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3883 - accuracy: 0.8330 - val_loss: 0.4840 - val_accuracy: 0.8010\n",
      "Epoch 301/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4121 - accuracy: 0.8278 - val_loss: 0.4838 - val_accuracy: 0.8020\n",
      "Epoch 302/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4048 - accuracy: 0.8247 - val_loss: 0.4836 - val_accuracy: 0.8020\n",
      "Epoch 303/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4181 - accuracy: 0.8122 - val_loss: 0.4834 - val_accuracy: 0.8010\n",
      "Epoch 304/500\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.3658 - accuracy: 0.8549 - val_loss: 0.4834 - val_accuracy: 0.7990\n",
      "Epoch 305/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3705 - accuracy: 0.8507 - val_loss: 0.4835 - val_accuracy: 0.7990\n",
      "Epoch 306/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3988 - accuracy: 0.8370 - val_loss: 0.4828 - val_accuracy: 0.8000\n",
      "Epoch 307/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4076 - accuracy: 0.8275 - val_loss: 0.4825 - val_accuracy: 0.8000\n",
      "Epoch 308/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4061 - accuracy: 0.8452 - val_loss: 0.4828 - val_accuracy: 0.7970\n",
      "Epoch 309/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4165 - accuracy: 0.8422 - val_loss: 0.4833 - val_accuracy: 0.7950\n",
      "Epoch 310/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3877 - accuracy: 0.8402 - val_loss: 0.4830 - val_accuracy: 0.7970\n",
      "Epoch 311/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3654 - accuracy: 0.8671 - val_loss: 0.4825 - val_accuracy: 0.7970\n",
      "Epoch 312/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3975 - accuracy: 0.8535 - val_loss: 0.4816 - val_accuracy: 0.7970\n",
      "Epoch 313/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4071 - accuracy: 0.8327 - val_loss: 0.4810 - val_accuracy: 0.7970\n",
      "Epoch 314/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3565 - accuracy: 0.8754 - val_loss: 0.4803 - val_accuracy: 0.8010\n",
      "Epoch 315/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4005 - accuracy: 0.8327 - val_loss: 0.4775 - val_accuracy: 0.8050\n",
      "Epoch 316/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3649 - accuracy: 0.8570 - val_loss: 0.4764 - val_accuracy: 0.8090\n",
      "Epoch 317/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4126 - accuracy: 0.8153 - val_loss: 0.4763 - val_accuracy: 0.8100\n",
      "Epoch 318/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3835 - accuracy: 0.8310 - val_loss: 0.4761 - val_accuracy: 0.8090\n",
      "Epoch 319/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4100 - accuracy: 0.7999 - val_loss: 0.4756 - val_accuracy: 0.8080\n",
      "Epoch 320/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3838 - accuracy: 0.8278 - val_loss: 0.4756 - val_accuracy: 0.8080\n",
      "Epoch 321/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3459 - accuracy: 0.8547 - val_loss: 0.4761 - val_accuracy: 0.8060\n",
      "Epoch 322/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3830 - accuracy: 0.8402 - val_loss: 0.4767 - val_accuracy: 0.8040\n",
      "Epoch 323/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4081 - accuracy: 0.8089 - val_loss: 0.4773 - val_accuracy: 0.8040\n",
      "Epoch 324/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3825 - accuracy: 0.8516 - val_loss: 0.4777 - val_accuracy: 0.8030\n",
      "Epoch 325/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3742 - accuracy: 0.8495 - val_loss: 0.4781 - val_accuracy: 0.8000\n",
      "Epoch 326/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3673 - accuracy: 0.8403 - val_loss: 0.4791 - val_accuracy: 0.7980\n",
      "Epoch 327/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3781 - accuracy: 0.8205 - val_loss: 0.4810 - val_accuracy: 0.7950\n",
      "Epoch 328/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3805 - accuracy: 0.8226 - val_loss: 0.4829 - val_accuracy: 0.7960\n",
      "Epoch 329/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3763 - accuracy: 0.8497 - val_loss: 0.4839 - val_accuracy: 0.7960\n",
      "Epoch 330/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3876 - accuracy: 0.8330 - val_loss: 0.4841 - val_accuracy: 0.7960\n",
      "Epoch 331/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3999 - accuracy: 0.8205 - val_loss: 0.4835 - val_accuracy: 0.7960\n",
      "Epoch 332/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3912 - accuracy: 0.8226 - val_loss: 0.4814 - val_accuracy: 0.7980\n",
      "Epoch 333/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4008 - accuracy: 0.8185 - val_loss: 0.4800 - val_accuracy: 0.7990\n",
      "Epoch 334/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3731 - accuracy: 0.8455 - val_loss: 0.4790 - val_accuracy: 0.8000\n",
      "Epoch 335/500\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.3948 - accuracy: 0.8145 - val_loss: 0.4783 - val_accuracy: 0.8000\n",
      "Epoch 336/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4099 - accuracy: 0.7926 - val_loss: 0.4778 - val_accuracy: 0.8030\n",
      "Epoch 337/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3724 - accuracy: 0.8261 - val_loss: 0.4778 - val_accuracy: 0.8030\n",
      "Epoch 338/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3983 - accuracy: 0.7936 - val_loss: 0.4772 - val_accuracy: 0.8040\n",
      "Epoch 339/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4087 - accuracy: 0.7967 - val_loss: 0.4767 - val_accuracy: 0.8040\n",
      "Epoch 340/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3769 - accuracy: 0.8270 - val_loss: 0.4765 - val_accuracy: 0.8040\n",
      "Epoch 341/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3867 - accuracy: 0.8270 - val_loss: 0.4762 - val_accuracy: 0.8040\n",
      "Epoch 342/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3659 - accuracy: 0.8207 - val_loss: 0.4759 - val_accuracy: 0.8030\n",
      "Epoch 343/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3555 - accuracy: 0.8219 - val_loss: 0.4759 - val_accuracy: 0.8020\n",
      "Epoch 344/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3635 - accuracy: 0.8219 - val_loss: 0.4758 - val_accuracy: 0.8010\n",
      "Epoch 345/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3372 - accuracy: 0.8530 - val_loss: 0.4765 - val_accuracy: 0.8020\n",
      "Epoch 346/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3851 - accuracy: 0.8176 - val_loss: 0.4768 - val_accuracy: 0.8020\n",
      "Epoch 347/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3545 - accuracy: 0.8310 - val_loss: 0.4766 - val_accuracy: 0.8020\n",
      "Epoch 348/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3269 - accuracy: 0.8603 - val_loss: 0.4764 - val_accuracy: 0.8000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 349/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3942 - accuracy: 0.8009 - val_loss: 0.4763 - val_accuracy: 0.8040\n",
      "Epoch 350/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4226 - accuracy: 0.7813 - val_loss: 0.4756 - val_accuracy: 0.8040\n",
      "Epoch 351/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3678 - accuracy: 0.8303 - val_loss: 0.4754 - val_accuracy: 0.8060\n",
      "Epoch 352/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3600 - accuracy: 0.8157 - val_loss: 0.4748 - val_accuracy: 0.8070\n",
      "Epoch 353/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3847 - accuracy: 0.8217 - val_loss: 0.4734 - val_accuracy: 0.8030\n",
      "Epoch 354/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4011 - accuracy: 0.7842 - val_loss: 0.4726 - val_accuracy: 0.8030\n",
      "Epoch 355/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3793 - accuracy: 0.8082 - val_loss: 0.4723 - val_accuracy: 0.8030\n",
      "Epoch 356/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4049 - accuracy: 0.8257 - val_loss: 0.4730 - val_accuracy: 0.8020\n",
      "Epoch 357/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3778 - accuracy: 0.8424 - val_loss: 0.4730 - val_accuracy: 0.8010\n",
      "Epoch 358/500\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.3819 - accuracy: 0.8310 - val_loss: 0.4729 - val_accuracy: 0.8010\n",
      "Epoch 359/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3430 - accuracy: 0.8455 - val_loss: 0.4731 - val_accuracy: 0.8000\n",
      "Epoch 360/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3822 - accuracy: 0.8247 - val_loss: 0.4736 - val_accuracy: 0.7970\n",
      "Epoch 361/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3699 - accuracy: 0.8268 - val_loss: 0.4740 - val_accuracy: 0.7970\n",
      "Epoch 362/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3934 - accuracy: 0.8247 - val_loss: 0.4740 - val_accuracy: 0.7980\n",
      "Epoch 363/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3604 - accuracy: 0.8516 - val_loss: 0.4739 - val_accuracy: 0.7990\n",
      "Epoch 364/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4025 - accuracy: 0.8327 - val_loss: 0.4733 - val_accuracy: 0.8020\n",
      "Epoch 365/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3795 - accuracy: 0.8183 - val_loss: 0.4723 - val_accuracy: 0.8020\n",
      "Epoch 366/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3716 - accuracy: 0.8391 - val_loss: 0.4718 - val_accuracy: 0.8020\n",
      "Epoch 367/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3703 - accuracy: 0.8370 - val_loss: 0.4721 - val_accuracy: 0.8040\n",
      "Epoch 368/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3687 - accuracy: 0.8464 - val_loss: 0.4720 - val_accuracy: 0.8030\n",
      "Epoch 369/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3807 - accuracy: 0.8308 - val_loss: 0.4713 - val_accuracy: 0.8060\n",
      "Epoch 370/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3716 - accuracy: 0.8174 - val_loss: 0.4706 - val_accuracy: 0.8100\n",
      "Epoch 371/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3986 - accuracy: 0.8091 - val_loss: 0.4703 - val_accuracy: 0.8120\n",
      "Epoch 372/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4014 - accuracy: 0.7896 - val_loss: 0.4698 - val_accuracy: 0.8110\n",
      "Epoch 373/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3408 - accuracy: 0.8386 - val_loss: 0.4697 - val_accuracy: 0.8120\n",
      "Epoch 374/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3865 - accuracy: 0.7938 - val_loss: 0.4695 - val_accuracy: 0.8110\n",
      "Epoch 375/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3595 - accuracy: 0.8230 - val_loss: 0.4697 - val_accuracy: 0.8120\n",
      "Epoch 376/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3809 - accuracy: 0.8000 - val_loss: 0.4701 - val_accuracy: 0.8130\n",
      "Epoch 377/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3735 - accuracy: 0.8278 - val_loss: 0.4703 - val_accuracy: 0.8110\n",
      "Epoch 378/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3945 - accuracy: 0.8070 - val_loss: 0.4704 - val_accuracy: 0.8080\n",
      "Epoch 379/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3594 - accuracy: 0.8289 - val_loss: 0.4704 - val_accuracy: 0.8060\n",
      "Epoch 380/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3620 - accuracy: 0.8330 - val_loss: 0.4701 - val_accuracy: 0.8070\n",
      "Epoch 381/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3675 - accuracy: 0.8391 - val_loss: 0.4690 - val_accuracy: 0.8070\n",
      "Epoch 382/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3855 - accuracy: 0.8214 - val_loss: 0.4681 - val_accuracy: 0.8070\n",
      "Epoch 383/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3608 - accuracy: 0.8516 - val_loss: 0.4680 - val_accuracy: 0.8040\n",
      "Epoch 384/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3645 - accuracy: 0.8516 - val_loss: 0.4684 - val_accuracy: 0.8040\n",
      "Epoch 385/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3784 - accuracy: 0.8372 - val_loss: 0.4685 - val_accuracy: 0.8040\n",
      "Epoch 386/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3606 - accuracy: 0.8247 - val_loss: 0.4687 - val_accuracy: 0.8030\n",
      "Epoch 387/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3405 - accuracy: 0.8466 - val_loss: 0.4686 - val_accuracy: 0.8030\n",
      "Epoch 388/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3816 - accuracy: 0.8204 - val_loss: 0.4684 - val_accuracy: 0.8060\n",
      "Epoch 389/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3603 - accuracy: 0.8402 - val_loss: 0.4685 - val_accuracy: 0.8060\n",
      "Epoch 390/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3804 - accuracy: 0.8172 - val_loss: 0.4685 - val_accuracy: 0.8060\n",
      "Epoch 391/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3854 - accuracy: 0.8152 - val_loss: 0.4682 - val_accuracy: 0.8070\n",
      "Epoch 392/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3523 - accuracy: 0.8485 - val_loss: 0.4683 - val_accuracy: 0.8070\n",
      "Epoch 393/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3406 - accuracy: 0.8725 - val_loss: 0.4688 - val_accuracy: 0.8020\n",
      "Epoch 394/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3529 - accuracy: 0.8268 - val_loss: 0.4696 - val_accuracy: 0.8040\n",
      "Epoch 395/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3841 - accuracy: 0.8391 - val_loss: 0.4698 - val_accuracy: 0.8030\n",
      "Epoch 396/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3433 - accuracy: 0.8495 - val_loss: 0.4699 - val_accuracy: 0.8020\n",
      "Epoch 397/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3653 - accuracy: 0.8330 - val_loss: 0.4697 - val_accuracy: 0.8030\n",
      "Epoch 398/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3965 - accuracy: 0.8172 - val_loss: 0.4699 - val_accuracy: 0.8040\n",
      "Epoch 399/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3437 - accuracy: 0.8485 - val_loss: 0.4695 - val_accuracy: 0.8040\n",
      "Epoch 400/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3415 - accuracy: 0.8495 - val_loss: 0.4684 - val_accuracy: 0.8040\n",
      "Epoch 401/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3653 - accuracy: 0.8308 - val_loss: 0.4678 - val_accuracy: 0.8020\n",
      "Epoch 402/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3794 - accuracy: 0.8402 - val_loss: 0.4680 - val_accuracy: 0.8020\n",
      "Epoch 403/500\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.3545 - accuracy: 0.8370 - val_loss: 0.4685 - val_accuracy: 0.8020\n",
      "Epoch 404/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3410 - accuracy: 0.8631 - val_loss: 0.4688 - val_accuracy: 0.8030\n",
      "Epoch 405/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3728 - accuracy: 0.8195 - val_loss: 0.4691 - val_accuracy: 0.8030\n",
      "Epoch 406/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3437 - accuracy: 0.8311 - val_loss: 0.4698 - val_accuracy: 0.8030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 407/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3626 - accuracy: 0.8113 - val_loss: 0.4698 - val_accuracy: 0.8040\n",
      "Epoch 408/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3794 - accuracy: 0.7988 - val_loss: 0.4688 - val_accuracy: 0.8030\n",
      "Epoch 409/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3479 - accuracy: 0.8216 - val_loss: 0.4686 - val_accuracy: 0.8060\n",
      "Epoch 410/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3779 - accuracy: 0.8185 - val_loss: 0.4689 - val_accuracy: 0.8060\n",
      "Epoch 411/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3600 - accuracy: 0.8487 - val_loss: 0.4698 - val_accuracy: 0.8040\n",
      "Epoch 412/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3436 - accuracy: 0.8435 - val_loss: 0.4709 - val_accuracy: 0.8040\n",
      "Epoch 413/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3864 - accuracy: 0.8185 - val_loss: 0.4717 - val_accuracy: 0.8000\n",
      "Epoch 414/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3616 - accuracy: 0.8391 - val_loss: 0.4722 - val_accuracy: 0.8000\n",
      "Epoch 415/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3646 - accuracy: 0.8360 - val_loss: 0.4722 - val_accuracy: 0.8000\n",
      "Epoch 416/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3454 - accuracy: 0.8516 - val_loss: 0.4718 - val_accuracy: 0.8000\n",
      "Epoch 417/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3241 - accuracy: 0.8455 - val_loss: 0.4701 - val_accuracy: 0.8030\n",
      "Epoch 418/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3774 - accuracy: 0.8091 - val_loss: 0.4680 - val_accuracy: 0.8050\n",
      "Epoch 419/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3733 - accuracy: 0.8145 - val_loss: 0.4671 - val_accuracy: 0.8050\n",
      "Epoch 420/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3493 - accuracy: 0.8238 - val_loss: 0.4664 - val_accuracy: 0.8060\n",
      "Epoch 421/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3246 - accuracy: 0.8374 - val_loss: 0.4663 - val_accuracy: 0.8050\n",
      "Epoch 422/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3611 - accuracy: 0.8153 - val_loss: 0.4660 - val_accuracy: 0.8050\n",
      "Epoch 423/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3466 - accuracy: 0.8299 - val_loss: 0.4661 - val_accuracy: 0.8050\n",
      "Epoch 424/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3712 - accuracy: 0.7967 - val_loss: 0.4667 - val_accuracy: 0.8060\n",
      "Epoch 425/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3634 - accuracy: 0.8092 - val_loss: 0.4674 - val_accuracy: 0.8050\n",
      "Epoch 426/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3264 - accuracy: 0.8549 - val_loss: 0.4683 - val_accuracy: 0.8040\n",
      "Epoch 427/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3683 - accuracy: 0.8268 - val_loss: 0.4700 - val_accuracy: 0.8020\n",
      "Epoch 428/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3759 - accuracy: 0.8060 - val_loss: 0.4718 - val_accuracy: 0.8010\n",
      "Epoch 429/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3841 - accuracy: 0.8079 - val_loss: 0.4727 - val_accuracy: 0.8000\n",
      "Epoch 430/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3652 - accuracy: 0.8381 - val_loss: 0.4733 - val_accuracy: 0.7990\n",
      "Epoch 431/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3669 - accuracy: 0.8370 - val_loss: 0.4739 - val_accuracy: 0.8010\n",
      "Epoch 432/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3512 - accuracy: 0.8402 - val_loss: 0.4736 - val_accuracy: 0.8010\n",
      "Epoch 433/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3565 - accuracy: 0.8216 - val_loss: 0.4736 - val_accuracy: 0.8020\n",
      "Epoch 434/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3558 - accuracy: 0.8330 - val_loss: 0.4739 - val_accuracy: 0.8010\n",
      "Epoch 435/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3473 - accuracy: 0.8403 - val_loss: 0.4745 - val_accuracy: 0.8010\n",
      "Epoch 436/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3594 - accuracy: 0.8174 - val_loss: 0.4749 - val_accuracy: 0.7990\n",
      "Epoch 437/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3458 - accuracy: 0.8487 - val_loss: 0.4750 - val_accuracy: 0.8000\n",
      "Epoch 438/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3546 - accuracy: 0.8341 - val_loss: 0.4736 - val_accuracy: 0.8040\n",
      "Epoch 439/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3365 - accuracy: 0.8341 - val_loss: 0.4729 - val_accuracy: 0.8000\n",
      "Epoch 440/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3711 - accuracy: 0.8216 - val_loss: 0.4728 - val_accuracy: 0.8020\n",
      "Epoch 441/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3724 - accuracy: 0.7967 - val_loss: 0.4741 - val_accuracy: 0.8000\n",
      "Epoch 442/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3597 - accuracy: 0.8051 - val_loss: 0.4749 - val_accuracy: 0.8000\n",
      "Epoch 443/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3584 - accuracy: 0.8299 - val_loss: 0.4755 - val_accuracy: 0.8000\n",
      "Epoch 444/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3304 - accuracy: 0.8591 - val_loss: 0.4756 - val_accuracy: 0.7990\n",
      "Epoch 445/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3704 - accuracy: 0.8091 - val_loss: 0.4754 - val_accuracy: 0.7990\n",
      "Epoch 446/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3558 - accuracy: 0.8278 - val_loss: 0.4756 - val_accuracy: 0.7990\n",
      "Epoch 447/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3515 - accuracy: 0.8278 - val_loss: 0.4751 - val_accuracy: 0.8000\n",
      "Epoch 448/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3689 - accuracy: 0.8122 - val_loss: 0.4740 - val_accuracy: 0.8010\n",
      "Epoch 449/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3517 - accuracy: 0.8310 - val_loss: 0.4736 - val_accuracy: 0.8010\n",
      "Epoch 450/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3476 - accuracy: 0.8487 - val_loss: 0.4733 - val_accuracy: 0.8010\n",
      "Epoch 451/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3456 - accuracy: 0.8362 - val_loss: 0.4731 - val_accuracy: 0.7990\n",
      "Epoch 452/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3656 - accuracy: 0.8153 - val_loss: 0.4726 - val_accuracy: 0.7990\n",
      "Epoch 453/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3562 - accuracy: 0.8330 - val_loss: 0.4722 - val_accuracy: 0.7990\n",
      "Epoch 454/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3581 - accuracy: 0.7997 - val_loss: 0.4723 - val_accuracy: 0.7990\n",
      "Epoch 455/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3210 - accuracy: 0.8580 - val_loss: 0.4727 - val_accuracy: 0.8000\n",
      "Epoch 456/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3327 - accuracy: 0.8568 - val_loss: 0.4729 - val_accuracy: 0.7990\n",
      "Epoch 457/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3142 - accuracy: 0.8620 - val_loss: 0.4730 - val_accuracy: 0.7990\n",
      "Epoch 458/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3351 - accuracy: 0.8516 - val_loss: 0.4737 - val_accuracy: 0.8010\n",
      "Epoch 459/500\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.3560 - accuracy: 0.8339 - val_loss: 0.4742 - val_accuracy: 0.8010\n",
      "Epoch 460/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3462 - accuracy: 0.8527 - val_loss: 0.4744 - val_accuracy: 0.8000\n",
      "Epoch 461/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3379 - accuracy: 0.8391 - val_loss: 0.4751 - val_accuracy: 0.8000\n",
      "Epoch 462/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3530 - accuracy: 0.8391 - val_loss: 0.4751 - val_accuracy: 0.8000\n",
      "Epoch 463/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3629 - accuracy: 0.8339 - val_loss: 0.4742 - val_accuracy: 0.8010\n",
      "Epoch 464/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3395 - accuracy: 0.8714 - val_loss: 0.4740 - val_accuracy: 0.8010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 465/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3302 - accuracy: 0.8558 - val_loss: 0.4740 - val_accuracy: 0.8010\n",
      "Epoch 466/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3214 - accuracy: 0.8620 - val_loss: 0.4742 - val_accuracy: 0.8010\n",
      "Epoch 467/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3271 - accuracy: 0.8414 - val_loss: 0.4745 - val_accuracy: 0.8010\n",
      "Epoch 468/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3548 - accuracy: 0.8164 - val_loss: 0.4742 - val_accuracy: 0.7990\n",
      "Epoch 469/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3605 - accuracy: 0.8257 - val_loss: 0.4730 - val_accuracy: 0.8020\n",
      "Epoch 470/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3753 - accuracy: 0.8060 - val_loss: 0.4722 - val_accuracy: 0.8020\n",
      "Epoch 471/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3575 - accuracy: 0.8153 - val_loss: 0.4715 - val_accuracy: 0.8060\n",
      "Epoch 472/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3497 - accuracy: 0.8226 - val_loss: 0.4709 - val_accuracy: 0.8050\n",
      "Epoch 473/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3615 - accuracy: 0.8174 - val_loss: 0.4699 - val_accuracy: 0.8050\n",
      "Epoch 474/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3489 - accuracy: 0.8237 - val_loss: 0.4699 - val_accuracy: 0.8050\n",
      "Epoch 475/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3681 - accuracy: 0.8091 - val_loss: 0.4702 - val_accuracy: 0.8010\n",
      "Epoch 476/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3077 - accuracy: 0.8528 - val_loss: 0.4704 - val_accuracy: 0.8020\n",
      "Epoch 477/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3371 - accuracy: 0.8164 - val_loss: 0.4697 - val_accuracy: 0.8050\n",
      "Epoch 478/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3535 - accuracy: 0.8205 - val_loss: 0.4682 - val_accuracy: 0.8050\n",
      "Epoch 479/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3509 - accuracy: 0.8330 - val_loss: 0.4679 - val_accuracy: 0.8040\n",
      "Epoch 480/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3343 - accuracy: 0.8341 - val_loss: 0.4679 - val_accuracy: 0.8040\n",
      "Epoch 481/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3374 - accuracy: 0.8268 - val_loss: 0.4678 - val_accuracy: 0.8020\n",
      "Epoch 482/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3330 - accuracy: 0.8610 - val_loss: 0.4679 - val_accuracy: 0.8040\n",
      "Epoch 483/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3489 - accuracy: 0.8299 - val_loss: 0.4681 - val_accuracy: 0.8030\n",
      "Epoch 484/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3556 - accuracy: 0.8433 - val_loss: 0.4681 - val_accuracy: 0.8030\n",
      "Epoch 485/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3676 - accuracy: 0.8598 - val_loss: 0.4679 - val_accuracy: 0.8050\n",
      "Epoch 486/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3380 - accuracy: 0.8504 - val_loss: 0.4672 - val_accuracy: 0.8040\n",
      "Epoch 487/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3764 - accuracy: 0.8181 - val_loss: 0.4662 - val_accuracy: 0.8050\n",
      "Epoch 488/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3906 - accuracy: 0.8265 - val_loss: 0.4663 - val_accuracy: 0.8050\n",
      "Epoch 489/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3125 - accuracy: 0.8765 - val_loss: 0.4661 - val_accuracy: 0.8050\n",
      "Epoch 490/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3227 - accuracy: 0.8608 - val_loss: 0.4662 - val_accuracy: 0.8050\n",
      "Epoch 491/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3501 - accuracy: 0.8400 - val_loss: 0.4657 - val_accuracy: 0.8060\n",
      "Epoch 492/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3177 - accuracy: 0.8713 - val_loss: 0.4655 - val_accuracy: 0.8060\n",
      "Epoch 493/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3092 - accuracy: 0.8619 - val_loss: 0.4658 - val_accuracy: 0.8050\n",
      "Epoch 494/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3477 - accuracy: 0.8431 - val_loss: 0.4660 - val_accuracy: 0.8050\n",
      "Epoch 495/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3594 - accuracy: 0.8277 - val_loss: 0.4641 - val_accuracy: 0.8050\n",
      "Epoch 496/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3415 - accuracy: 0.8650 - val_loss: 0.4630 - val_accuracy: 0.8060\n",
      "Epoch 497/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3124 - accuracy: 0.8608 - val_loss: 0.4631 - val_accuracy: 0.8100\n",
      "Epoch 498/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3210 - accuracy: 0.8650 - val_loss: 0.4634 - val_accuracy: 0.8060\n",
      "Epoch 499/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3217 - accuracy: 0.8577 - val_loss: 0.4635 - val_accuracy: 0.8050\n",
      "Epoch 500/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3049 - accuracy: 0.8838 - val_loss: 0.4630 - val_accuracy: 0.8050\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=500, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the run, we will evaluate the model's performance on both the train and the test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.840, Test: 0.805\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then finally, we will plot model loss and accuracy learning curves over each training epoch on both the training and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss learning curves\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Cross-Entropy Loss', pad=-40)\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "\n",
    "# plot accuracy learning curves\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy', pad=-40)\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.840, Test: 0.813\n"
     ]
    }
   ],
   "source": [
    "# fit high variance mlp on blobs classification problem\n",
    "from sklearn.datasets import make_blobs\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "\n",
    "# one hot encode output variable\n",
    "y = to_categorical(y)\n",
    "\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "\n",
    "#define model\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=2, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=500, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example, it first prints the performance of the final model on the train and test datasets.\n",
    "\n",
    "**Note:** Your specific results may vary, given the stochastic nature of the learning algorithm. Consider running the example a few times and compare the average performance.\n",
    "\n",
    "In this case, we can see that the model achieved about 85% accuracy on the training dataset, which we know is optimistic, and about 80% on the test dataset, which we would expect to be more realistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A line plot also shows the learning curves for the model loss and accuracy on the train and test sets over each training epoch. We can see that training accuracy is more optimistic over most of the run, as we noted with the final scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEFCAYAAADzHRw3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAABQDklEQVR4nO2deXxVxfn/389dcrPvCWSDhH1fZBFEBLcKqLhVq9aqrd9iW+3Pttav2lprF6ut37rVpdWWat2tKyoqoKCoKBAEZE+AQBZIQvb15i7z+2NOwgUCBEhyucm8X6/zuufMzDnnmXPv/ZznPDNnRpRSGAwGgyH0sQXbAIPBYDB0DkbQDQaDoYdgBN1gMBh6CEbQDQaDoYdgBN1gMBh6CEbQDQaDoYdgBN1gMBh6CEbQDceFiFwtIqtFpF5E9ojI+yJyehDtuV5EfJY9gUt6B/adKSJF3WFnRxCRAhE5J9h2GEIPI+iGY0ZEfgE8DPwJ6AP0A54ALjpMeUc3mbZCKRV90FLSGQfuxjoYDMeNEXTDMSEiccDvgZuUUm8opRqUUh6l1DtKqdusMveIyGsi8ryI1ALXi0i6iCwQkUoRyReRHwYcc7Ll7deKSKmIPGilh1vHqBCRahFZJSJ9jtPuAhH5pYisF5EaEXnFOn4U8D6QHujVH0cdWsu/IiJ1IrJGRMZaebeJyOsH2fOoiDxyjHVwicjDIlJiLQ+LiMvKSxaRd63rVCkiy0XEZuXdLiLFll1bReTs47mGhpMfI+iGY2UqEA68eZRyFwGvAfHAC8DLQBGQDnwb+JOInGWVfQR4RCkVCwwEXrXSrwPigCwgCfgR0HQCtl8BzAJygDHA9UqpBmA2UNKOV38sdWgt/18gEXgReEtEnMDzwCwRiYc2b/9K4D/HaP+vgSnAOGAsMBm4y8q71bItBf3U9CtAichQ4GZgklIqBjgPKDjG8xpCBCPohmMlCdinlPIepdwKpdRbSik/kAxMA25XSjUrpdYC/wSutcp6gEEikqyUqldKfRmQngQMUkr5lFK5SqnaI5xziuWhti7bD8p/VClVopSqBN5BC2Nn1QEgVyn1mlLKAzyIvvFNUUrtAT4FLrfKzUJfw9yjnP9gvgv8XilVppQqB34HfM/K8wBpQH/riWm50gM1+QAXMEJEnEqpAqXUwdfF0EMwgm44ViqA5A7ElAsD1tOBSqVUXUDaLiDDWr8BGAJsscIqF1jpzwEfAi9bIYa/iIhTRKYHhEc2BhzzS6VUfMAy8CCb9gasNwLRnViHA8pbN4FWbx7gWeAaa/0aq27HSrp1zsDztx7/ASAfWCQiO0TkDsuOfOBnwD1AmYi83JGGYkNoYgTdcKysANzAxUcpFziMZwmQKCIxAWn9gGIApVSeUuoqIBX4M/CaiERZnubvlFIjgNOAC4BrLe+zNTwyshPqdLghRztcB4us1hUrfp1p7QfwFjBGREah6/HCcdhZAvQ/6PwlAEqpOqXUrUqpAcBc4BetsXKl1ItKqdOtfRX6Ght6IEbQDceEUqoGuBt4XEQuFpFIy2ueLSJ/Ocw+hcAXwH1WQ+QYtFf+PICIXCMiKZZXW23t5heRM0VktIjYgVp0WMHfBdUqBZKsBt92OVodLCaIyKXW08vP0De+L639m9Hx+BeBlUqp3UexyWmdp3VxAC8Bd4lIiogko7+H1mt4gYgMEhEBatChFr+IDBWRs6zG02Z0G0RXXEPDSYARdMMxo5T6K/ALdINcOTrUcDPaCz0cVwHZaI/yTeC3SqklVt4sYKOI1KMbSK9USjUBfdEiWAtsBj7hyKGKqXJoP/RJHajPFrRY7rBi74cLSRypDgBvA98BqtCx7UuteHorzwKjj1KHVhaixbd1uQf4I7AaWA98A6yx0gAGA0uAevRT1BNKqaXo+Pn9wD50yCkVuLMD5zeEIGImuDAYThwRuQfdeHvNEcr0A7YAfY/SuGswHBfGQzcYugErpv4L4GUj5oauImgeenJyssrOzg7KuQ2GzqakpAS3201OTs4heT6fj/Xr1xMWFsbgwYMJCwsLgoWGnkJubu4+pVRKe3lBe505Ozub1atXB+v0BoPBEJKIyK7D5ZmQi8FgMPQQjKAbDAZDDyHkBP0/KwoY//tFeHymK63BYDAEEnJDgoY77FQ1ethT3Uy/pMhgm2MwGLoZj8dDUVERzc3NwTalSwkPDyczMxOn09nhfUJO0Ed41vMbx3MUVk42gm4w9EKKioqIiYkhOzsb/WJsz0MpRUVFBUVFRe32nDocIRdyyXDv5AbH+5TvOWxDr8Fg6ME0NzeTlJTUY8UcQERISko65qeQkBP02KxRALj3bg6yJQaDIVj0ZDFv5XjqGHKCbu8zXH/u2xZkSwwGg+HkIuQEnehUGiSaqNr8YFtiMBh6IdXV1TzxxBPHvN+cOXOorq7ufIMCCD1BF6E8IpvU5oJgW2IwGHohhxN0r/fIk3gtXLiQ+Pj4LrJKE3qCDjTEDqK/KqKx5WizoBkMBkPncscdd7B9+3bGjRvHpEmTmD59OnPnzmXEiBEAXHzxxUyYMIGRI0fy1FNPte2XnZ3Nvn37KCgoYPjw4fzwhz9k5MiRfOtb36Kp6USmyt1PyHVbBPAnDyV571tsLylioBngy2DotfzunY1sKuncwStHpMfy2wsPPxHW/fffz4YNG1i7di3Lli3j/PPPZ8OGDW3dC+fPn09iYiJNTU1MmjSJyy67jKSkpAOOkZeXx0svvcTTTz/NFVdcweuvv8411xx25OUOE5IeemSGvtiVO9cF2RKDwdDbmTx58gF9xR999FHGjh3LlClTKCwsJC8v75B9cnJyGDduHAATJkygoKCgU2wJSQ89deip8CF4d68ELgq2OQaDIUgcyZPuLqKiotrWly1bxpIlS1ixYgWRkZHMnDmz3b7kLperbd1ut3dayCUkPfSYxL4UkEFseW6wTTEYDL2MmJgY6urq2s2rqakhISGByMhItmzZwpdfftmttoWkhw5QEDWaCQ2fgd8PtpC8LxkMhhAkKSmJadOmMWrUKCIiIujTp09b3qxZs/j73//O8OHDGTp0KFOmTOlW20JW0OtSJhBT8AGesi04+44ItjkGg6EX8eKLL7ab7nK5eP/999vNa42TJycns2HDhrb0X/7yl51mV8i6tpGDpgFQvnFZcA0xGAyGk4SQFfRBw8ZSquLx5C0NtikGg8FwUhCygt4vKYoVMp7Usi/AZ14wMhgMhpAVdBGhMGkaEf56KDaTTRsMBkPICjqADDwTr7Lh2fJhsE0xGAyGoBPSgj5mUH9y1RBaNrwNSgXbHIPBYAgqIS3ok3MSWaimEVW7HfauD7Y5BoOhF3C8w+cCPPzwwzQ2NnayRfsJaUEPd9op7zcHL3ZY/2qwzTEYDL2Ak1nQQ/bFolYmDh/I0t1jOXP9azjO/T3Y7ME2yWAw9GACh88999xzSU1N5dVXX8XtdnPJJZfwu9/9joaGBq644gqKiorw+Xz85je/obS0lJKSEs4880ySk5NZurTzu1yHvKDPGJrCQwuncW7D36DgMxgwI9gmGQyG7uL9O2DvN517zL6jYfb9h80OHD530aJFvPbaa6xcuRKlFHPnzuXTTz+lvLyc9PR03nvvPUCP8RIXF8eDDz7I0qVLSU5O7lybLUI65AIwIDmKzbHTaJQo+Pr5YJtjMBh6EYsWLWLRokWMHz+eU045hS1btpCXl8fo0aNZvHgxt99+O8uXLycuLq5b7Dmqhy4i84ELgDKl1Kh28gV4BJgDNALXK6XWdLahR7CPKUMzeWPtdL676S3kvHshOrW7Tm8wGILJETzp7kApxZ133smNN954SN6aNWtYuHAhd911F2effTZ33313l9vTEQ/9GWDWEfJnA4OtZR7w5ImbdWycPzqN+S3nIL4WyH22u09vMBh6EYHD55533nnMnz+f+vp6AIqLiykrK6OkpITIyEiuueYabrvtNtasWXPIvl3BUT10pdSnIpJ9hCIXAf9RSingSxGJF5E0pdSezjLyaJw6IIm66Bw2OiYycvV8OP1nYHd21+kNBkMvInD43NmzZ3P11VczdepUAKKjo3n++efJz8/ntttuw2az4XQ6efJJ7efOmzePWbNmkZ6e3iWNoqI68EKOJejvHibk8i5wv1LqM2v7I+B2pdQh7+OLyDy0F0+/fv0m7Nq168SsD+CeBRvZs+pN/mF/AC79J4y5vNOObTAYTh42b97M8OHDg21Gt9BeXUUkVyk1sb3y3dooqpR6Sik1USk1MSUlpVOPPXdcOos8Y6mOGQLL/gQ+T6ce32AwGE52OkPQi4GsgO1MK61bGZ8VT0ZCFP9yXQOVO+Dr57rbBIPBYAgqnSHoC4BrRTMFqOnO+HkrIsLlE7L4W9FAmtMmwbI/Q0vXvZFlMBiCR0dCxaHO8dTxqIIuIi8BK4ChIlIkIjeIyI9E5EdWkYXADiAfeBr4yTFb0UlcNTkLh83GK7E3QP1e+OrvwTLFYDB0EeHh4VRUVPRoUVdKUVFRQXh4+DHt15FeLlcdJV8BNx3TWbuI1NhwzhvVlwe3Obhm4LnYP38EJt0A4d3Tqd9gMHQ9mZmZFBUVUV5eHmxTupTw8HAyMzOPaZ+Qf/X/YL43pT/vrd/DR2k/5FvbF8OXf4eZtwfbLIPB0Ek4nU5ycnKCbcZJSci/+n8wp+YkMqxvDPevc6GGng8rHofmmmCbZTAYDF1OjxN0EeGnZw1mR3kDn6Z/H9w1sHp+sM0yGAyGLqfHCTrA7FF9GZwazR9zw1ADzoIVT4CnOdhmGQwGQ5fSIwXdZhN+evZg8srq+TLjOmgog9X/CrZZBoPB0KX0SEEHPWDXwJQofrs+ATXgTPj0AWiqDrZZBoPB0GX0WEG324T/d/ZgtpXW80XOT6GpCj5/ONhmGQwGQ5fRYwUd4IIx6QxIjuIPuU7U6CvgyyehpttHJTAYDIZuoUcLut0m3HzWILbsrePTzBtB+WHpn4JtlsFgMHQJPVrQAeaOTSc7KZI/f9mEmvRDWPcilG4KtlkGg8HQ6fR4QXfYbdx81mA27allWer3ICwGFt8NPXgcCIPB0Dvp8YIOcPG4dPolRvLXz/ehzvgl5C+GDa8H2yyDwWDoVHqFoDvsNn561iA2FNfyftQlkDEBFt4G9WXBNs1gMBg6jV4h6ACXnpKpx3hZlE/LBX+Dlnp452cm9GIwGHoMvUbQ7TbhV3OGs7uykX9sCoNz7oGt78EXfwu2aQaDwdAp9BpBBzhjSArnj0njbx/nkz/gezB8Liy5Bwo+D7ZpBoPBcML0KkEHuOfCkUS67Nz+xgb8cx+DhGx47ftQvTvYphkMBsMJ0esEPSXGxW/OH0Huriqe+7oKrnwBvM3wzAVQXRhs8wwGg+G46XWCDnDpKRlMH5zMXz7YQqGjP3zvTT1w1/zzoGxLsM0zGAyG46JXCrqI8KdLRmOzCfOey6UheSx8/z3we7WoF64MtokGg8FwzPRKQQfISozkb1eNZ+veWm59dR3+1FFwwyKITIRn50LuM+D3BdtMg8Fg6DC9VtABZg5N5VdzhvPBxr08/FGebiD9wSL94tE7t8DTZ0HBZ6avusFgCAl6taAD3HB6Dt+ekMmjH+XxxpoiiE6B69+FS/8J9aXwzPnwj+lQujHYphoMBsMR6fWCLiLce8kopg5I4pf/XceCdSUgAmMuh5/mwoWP6CECnj4bPvkLuOuCbbLBYDC0S68XdACXw84/r5vIxP6J3PLy1/xz+Q6UUhAWBROuhxuXw6CzYem98Mg4M+m0wWA4KREVpPjwxIkT1erVq4Ny7sPR7PHx81fW8v6GvVw3tT+/Pn8EYY6Ae15RLnz8e9ixDKJSYfiFkDRIC3/yEMicBHZH0Ow3GAw9HxHJVUpNbDfPCPqB+P2Kexdu5l+f7WR4Wix/vXwsI9JjDyy04xNY9TTkfwSexv3p4XEwZDaMvAQGngUoaNgH0algd3ZrPQwGQ8/ECPpxsHhTKXe+8Q01TS1899T+3HL2YBKiwg4spJSefLqlHorXQN4i2PIuNNeAzaH7tQNEJsOs+2D05To+bzAYDMeJEfTjpLKhhfsWbuaNr4tx2IRrpvTnxjMGkBobfvidvC2wYyns/hKcERCRAOteguJc6H86zLwdsqcbYTcYDMeFEfQTZFtpHU99ukN3a0SP2njpKZl8a0Qfwp32ox/A79MvKi29FxorICwa4vtDvymQMx36joHEAUbkDQbDUTGC3kkU7Gvgv7mFvLmmmJKaZhKjwrhyUhbfndKfjPiIox/A0wSbFkDJGqjYDrs+3x+DT8iBQedA2lgdf4/L6NrKGAyGkMQIeifj9ys+376P51bsYsnmUvwKxmbFc+7wVM4Z0YehfWKQjnjbnmbYtw2KVsK2RbDzU/A26bzweBCb9trFDhmnaMFPyIb4fpAytCuraDAYTlKMoHchhZWNvPV1MUu2lLGusBqAzIQIzhnehxlDUshMiKB/UtSB3R8Ph98P+7ZC3mJrfHalG169bij49MAx2/tPg2m3QM4McB4hpm8wGHoURtC7ibLaZj7aUsZHm0tZnrcPt9cPQJjdxumDk7l8QiZnDEkhynUcfdWV0oJeX6Y9+hVPQK2O6RMeDwNmwqjLYPC5ujHWYDD0SIygB4GmFh/riqrZW9PMN8U1vLu+hNJaN3abMCojjlNzEpmcncik7ETiIo+jj7rPA1veg4o8qNoFW9+Hxn1gd0H6OIjLBER3nRQbpI+HYedD0sDOrqrBYOhGjKCfBPj8ii93VPDljgq+2lHJ2sJqWnx+RGBonxgt8DlJTMpJIDXmOEIoPi8ULIf8JVDyNdTtAUT3h/c2Q/UuXS5zEoy4GPqOApsT6vdqsU8ccODxPE160o+Yvqb3jcFwEmEE/SSk2eNjXWE1K3dWsrKgktxdVTS26PHXMxMimNA/gbOGpTImM57+iZHYbCcoqjVFsOF1WPcKlLUzcmT/aZA1WXerrNgBRavA54bYDD2OzaBzYPB5Jl5vMAQZI+ghgMfnZ2NJLat2VrJmdxWrCirZV98CQLTLwZjMOCb0T2B8v3hGpccd+eWmo1G7R4dqfC0QlaK9+nWv6B43USmQ0B+yToW4LN21cscycNfqvCHnQZ9R1jJSTwhiMBi6jRMWdBGZBTwC2IF/KqXuPyj/euABoNhKekwp9c8jHdMI+pHx+RWb99SysaSGDcW1fF1YxeY9dfj8+vtKiXExMj2WU3OSGJYWw5A+MaTHhXesu+Th8PvB1k5vHJ9Hh3NW/Uu/Adu4b39ebCYkDdAhmrLNeqCyoXN0HD++PyQP1l6+36fj+cbDNxhOiBMSdBGxA9uAc4EiYBVwlVJqU0CZ64GJSqmbO2qUEfRjp8HtZdOeWjYU17CxpJb1RdVsK61vy0+JcTE+K56hfWMYkBLF9MEpJEe7Ot+QulIo/Qb2boDSDbpR1uGClGFa7Ld9eOCgZfYwLejKBzFpuqtlTB/aGm1bxR6lj5E0UJdLyNH7lm6A7R/p4RNiM2HMFfpG4Ygwo1saeh1HEvSO/BsmA/lKqR3WwV4GLgI2HXEvQ6cT5XIwyeoZ00plQwvby+vZVFLL2sJqvt5dxaJNpW35GfERDEqNZmBKNANSohiQHIXNJiREhjE4Nfr4YvMxffQy6Jz28/0+qC3WQr9vq+5uaQ/TPXDKNsL2j6G5Gt1oa9cNtza7fkJw1+w/jth0GWXN7ZqQo1/A+urJ/WVccRCbpm8AcRmQMRGGz9Vj6LT3tGEw9GA64qF/G5illPofa/t7wKmB3rjlod8HlKO9+Z8rpQrbOdY8YB5Av379JuzatauTqmEIxO9XbCyp5dO8crburSO/rJ4d++pp9vgPKJcWF85F4zIY3y+e0RlxpJ1oyOZEUWr/jaBuD5Rv1WKeOFA3zMb0hcZK/eJVfanuvdNQDrUlunx1ITSU7T+eM1L3yQ+LgsgkPYZ9Qn9IGgxRSXpUzO0f6xtHxgTImqJ7AR18I2h9B6B8i34S6T/NDIdsCBonGnLpiKAnAfVKKbeI3Ah8Ryl11pGOa0Iu3Yvfryita2ZneQONLT6qGlt4Z/0ePssrxwrLkxgVxsj0WEZlxDGkTzSx4U5SYlwM7RuDy9GBQciCjVKwZ60er97TCC0NOrbf0qB779Tt1d033bX794lJ1+Lc2q0zrh+kDtdzy/r9uvG4bAu0BEw9GJUCo6+A7Gl6YLXIJH1DqdqpbyKx6fo45gnB0AWcqKBPBe5RSp1nbd8JoJS67zDl7UClUiruSMc1gn5y0NjiZfOeOqvxVcfmt5XW4fHt/1047cKwvrGMTI+lX1Ikw9NiGZAcRXp8BE57iIlW61NASwM4wnVPHptNT0SS/xFsXqDFvWEfIJCYA6kjoM8I/dlYCWtf0O0Efs/hzxMeB31G666f9WX6huJwafGP7mP17bfeE+gzQtuVOkL3LmrvKckRrm8UB+d5W8ARdmh5gw79eZrAFX2YfL81VlJovWdxooLuQIdRzkb3YlkFXK2U2hhQJk0ptcdavwS4XSk15UjHNYJ+8uL2+iisbKLB7aWkuol1RTWsK6xma2kdlQ0tbeVsAn1jw8lMjCQzIYKsBOvT2u4bG44j1AS/o3iaYM863bOnqVJ77UmD9ZNBTaF+uatssxbi6D4QlaxDRHWlurxSgIKWRijfrAdg87mPfM6YdP2ugCMcqgr000Njhe5F1GekngZRKfA06ON6GvXkK2HRuhHZGantbq7W5aKS9Q2mdYmI1+Gnur06fOWu0Q3PSQN1HVyxug7eZi2WEQlaDKNS9L6Nlbqb664vdLgsaaDu8ZR9OiDaVk+j1Qju0Q3h4fH6reZWUVVKH98RfqDQNtdA6SZ9nqgUvd1Qrm+8nkYdWotJ0z2yWup1V9u1L+oJaCZcp4fFcEbp7cYKPWfB5nf0eQaeqdteIhP1+Z3hevt4Rzz1tuhrXFuibYlJ04Pq2ez76+j3HXeDfmd0W5wDPIzutjhfKXWviPweWK2UWiAi9wFzAS9QCfxYKbXlSMc0gh6a1DZ72FxSy67KRoqqmiiyPgurGtlb20zgz8lhE9Liw8mMjyQrMYLMBP3p80OTx8fE/gkMSo0OPS+/s2m9aMW5eljlQFpFrakadn0Ge7/RgpHQX89nG5MGlTugdCNU5OvwkTMSwiK1kDsjtfhVbtcCanOCK0YLd1MlqAPbVY4bm3P/E4sjXN9c9uXp0UPtYfrchztXZJIuX1uibyY+t57la9j5EB4LlTv1uxLeY5iY3ebQ+9tdsOntQ2+WrjgYcaEW1s3vaOE9eP+Rl+j2EmeEfqLbt23/TTwsSjfS2+zaLne9vs5NVfqGejCOCP2E5XXr9p8LHoRTru14fQIwLxYZuoUWr5+S6qY2gS+qaqSwsomiKi36ZXWHeqB2m5AWF06/AK8+JTac1BgXqTEu+sSGkxztwmkXfH6FTeTE35rtjfis6RADvUK/T4tQY4Vemqq06Eb30aGoiAQtdJU7tDfsrtNhI0eEviE0V+vytSV6PTxONyxnnKLLtTRqT7hwpRb16FR9k2nt1WRzaHErWatvRnEZ+mkjIl6n7fhEi2Vchh58bshs3f7RsM/y1JO18IdFaTvr9urzOqP0UBbRKbqezbXaBmU9VUQk6CGoHVaXXr9f172pUtfLXQvfvAa5zx7YduKMhL6jdWjM06SfkpR/f8N7eLx1/Hi9HmM91dQW65tAbbH1xJaqe2JltqvJR8UIuuGkoNnjo7i6CbsIdpuwYkcFuysaKaxqZHdlIyXVWvSP9JMMs9vITo5kYEo0/ZOiSI4OIyXGRVKUi+SYMGLDnTS2+Nq6ZxoMx43PYz0xtGjRju6zP2wSRE60H7rB0CmEO+0MTNnfQJWVGHlIGa/PT0VDC2W1bkprmymrc1Ne58avFA6bUO/2sr28ga1761iyufSAxttARCAxMowcq/E2MSqMpKgwEqPDSIpykRQdRmJUGBFOO2EOG5FhdiKs6QQbW3y0eP2HTgpu6F3YnRCfFWwrjgkj6IaTCofdRp/YcPrEhjOaI3aUQilFbZOX8no3FfVu9tW3UNnYgk2gtNZNWW0zO/Y1sK6omsr6Furc3iMez+WwoRS0+HSsNzMhgikDkshJjsLlsBHmsOFy2Ii3XsrKSIgIje6chl6DEXRDyCIixEU6iYt0Mij1MF3TAnB7fVQ2tFBR30Jlg16aPT5afH7q3V6qGz1tnj3A2sJqFm8qpabp8N0TXQ4bcRHOQ5bYCCfRLgdRLgdRLjuRYQ6iwuwHbEe7HERaaS6HLbgvdRl6BEbQDb0Gl8NOWlwEaXEdn9FJKYXb66fF58ft0Z/ldW62ldZRXuempslDTaNHfzZ52FvbzNbSOmqbPNS7vW0vbR0Nu02IDLOTEBlGtMtBuNNGuNNOuNNOtMtBTLiDmHAnYXbBYddPCynRLlxOG0lRLnx+hdvrIyMhgvT4CGLDzZusvREj6AbDERCRNmHFGigyIz6CcVnxR9239WbQ4PbS2OKj3u2lscVLg9tHg9tLQ4uPxhavTnfr/MqGFhrcXtxeP80eHzVNHna4vdQ1e6lze2nxdqybod0mhNltuJz6CSIxKgynzYbTIbgcdlxW+MjlsONyBqw7bNZ2O+tHKNvY4iUqzEFchBObTXB7fThsNuymYbpbMYJuMHQRgTeDpE46plIKn1/R4vNTWuumxeunot6N2+cnNtzJ3ppmiqsbqW3y4vb6cHv91DR5qGxowetTNHv8B+S5Pf629WaPr8NPFIfDYRMcdqHZ48dpl7YG6domD5GW4MdFOIly6esSZm9tm9CN02EBbRVNLT4aW3z0iXWRGBVGU4sPj18R43IQEWbHaZ0nIsxOjMtBpMtBdJiWNJfTRr3by96aZuIjnWQmHNoA33o9gSOGu3x+FTI3JiPoBkMIIaIF02G3kZPc+veN6bTje31+LfReS+g9AesH3QBa85s9PlxOO00tPvbVu/H4/MRFOGlo8VFU1URlg5u0uHCaWvQTR0lNU9tTSIu1eE/0TnIUnHZ9c40Ms+Ow2Wj26JtFk8eH0y4MTIk+oN0jNtxBuNPO6oJKvimuweWwk5EQQXZSFOnx4W1PPxFWSCw+MoyYcAdhDhtOu17C7PqJKMxuI8YKgZXWNuPzKzISIrpkaGsj6AaDoQ2H3YbDbiOqC4bRPxI+v2oTd7dP3ygcdiE23ElZnZvKhhYiLa+8rtnb5q2H2W00e31Wmg5Nge56GhvuoE9sOCU1zVTUu7WAt/jw+PyEh9mJtAS+yeNjR3kD9W4vpbXNNLi9VDd5aGzxMSYjjnlnDMTj81NY2UhBRQOrCiq1rT5/24Qzx8ofLx7FNVP6d+YlBIygGwyGkwC7TYgIsxMRZgcObNDNcTnISY4KjmFHocXrp65ZN4jXNXvx+LTQe3wKj9ePx+en2eujvlk3kPeJdRHmsDGkT+c9VQViBN1gMBiOkzCHjaRoF0ldMTPYcdDLR0UyGAyGnoMRdIPBYOghBG1wLhEpB453DrpkYN9RS/UsTJ17B6bOvYMTqXN/pVRKexlBE/QTQURWH260sZ6KqXPvwNS5d9BVdTYhF4PBYOghGEE3GAyGHkKoCvpTwTYgCJg69w5MnXsHXVLnkIyhGwwGg+FQQtVDNxgMBsNBGEE3GAyGHkLICbqIzBKRrSKSLyJ3BNuezkJE5otImYhsCEhLFJHFIpJnfSZY6SIij1rXYL2InBI8y48fEckSkaUisklENorILVZ6j623iISLyEoRWWfV+XdWeo6IfGXV7RURCbPSXdZ2vpWfHdQKHCciYheRr0XkXWu7R9cXQEQKROQbEVkrIquttC79bYeUoIuIHXgcmA2MAK4SkRHBtarTeAaYdVDaHcBHSqnBwEfWNuj6D7aWecCT3WRjZ+MFblVKjQCmADdZ3+fR6l0EZAN/73aLTxw3cJZSaiwwDpglIlOAPwMPKaUGAVXADVb5G4AqK/0hq1wocguwOWC7p9e3lTOVUuMC+px37X9aKRUyCzAV+DBg+07gzmDb1Yn1ywY2BGxvBdKs9TRgq7X+D+Cq9sqF8gK8DZx7lHr/FPABlUBxd9UbcHTBMSOBNcCp6LcGHVZ62+8c+BCY2mqDVU6C/V0dYz0zLfE6C3gXkJ5c34B6FwDJB6V16X86pDx0IAMoDNgustJ6Kn2UUnus9b1AH2u9x10H69F6PPAVR673WOBL9BONy0prDd+8ISLlIlIhIo8FHPuHIrJZROqs8M4pVroSkUEB5Z4RkT9a6zNFpEhEbheRvcC/RSRBRN61zlFlrWcG7J8oIv8WkRIr/y0rfYOIXBhQziUiXrRYLQa2A9VKKa9VJPD7bPuurfwa6LQJkLqLh4H/BVrnz0uiZ9e3FQUsEpFcEZlnpXXpfzrUBL3XovRtu0f2MRWRaOB14GdKqdrAvHbqPQt4wVoSgUQrFPcuemygbPQf4WXr2JcD9wDXArHAXKCig6b1tc7RH/0YbAP+bW33A5qAxwLKP4f2ukcCqeiQAcB/gGsOqsNmIB2YDAzroD0hh4hcAJQppXKDbUsQOF0pdQo6nHKTiJwRmNkV/+lQGw+9GMgK2M600noqpSKSppTaIyJpQJmV3mOug4g40WL+glLqDSv5cPX2oUX2VaXUPhHxoGPvdWhxvC3A6/vM+vwf4C9KqVXWdv4xmOcHfquUclvbTZatrbbfCyy11tPQf9wkpVSVVeQT6/N54DciEmvdsL4HPKeUqhaRpeiQQ7yIOCz7A7/P1u+6SEQcQBwdvyGdDEwD5orIHPQ027HAI/Tc+rahlCq2PstE5E30zbtL/9Oh5qGvAgZbLeRhwJXAgiDb1JUsAK6z1q9Dx5hb06+1WsanADUBj3Ehg4gI8C9gs1LqwYCsw9XbhY6dV1j1LgcuRf8RdgWIeSBZ6JDG8VCulGoOsDdSRP4hIrtEpBb4FC1Mdus8lQFi3oZSqgT4HLhMRAaihf8FEYlAtxlsRt8Yvt1OnQOvxbeBjy3PLiRQSt2plMpUSmWj/68fK6W+Sw+tbysiEiUiMa3rwLeADXT1fzrYDQfH0dAwB9iG/pP+Otj2dGK9XgL2AB50/OwGdOzwIyAPWAIkWmUF3dtnO/ANMDHY9h9nnU9HP3KuB9Zay5z26g1EoOOpLejeMR6g1tp/BtrTOaThEt3Idsthzt8AjAnY/gD4o7U+Eyg6qPxvgGVAX2t7nHV+B7qByw/EH+ZcV1l1useyez36D363lT8AWIl+gvgv4LLSw63tfCt/QLC/txP4vmcC7/aG+lr1W2ctG1u1qqv/00GvuFnM0pHFEsRKdOy6b8DyKTpWvQ74PyDKEoVp1n6XoxubJlh/mkHo8aRBe833A3Z0XLvpKIL+F+B96/iJwJutgm7lvwe8CCSgJ8Y8I2DfCHT3vA3AtcG+nmbpmUuohVwMvZfrgH8rpXYrpfa2LuhGyauAC9FivRv9hPMdAKXUf4F70UJbB7yFFmPQfaMvBKqB71p5R+JhtDDvQ/e0+eCg/O+hnxy2oJ8YftaaoZRqjb/nAG9gMHQBZnAug6GbEJG7gSFKqWuOWthgOA5CrZeLwRCSiEgiul3ke8G2xdBzCZqHnpycrLKzs4NyboOhOykvL6eoqIjExET69+8fbHMMIU5ubu4+dZg5RYPmoWdnZ7N69epgnd5gMBhCEhHZdbg80yhqMBgMPQQj6AaDocupbfawq6Kh3TylFBuKa+js8K9Sik0ltUcv2IMwgm4wGLqc7/97FTMeWEaD+9CXed/8upgL/vYZ767v3Jedn16+gzmPLid3V2WnHvdkxgi6wWDoMvJK6/jx87nk7tIjIlw3fyWv5xYdUObNr/WQJT97ZS0bims65byrCyr508ItAG1eut+vuGfBRn715jd4fX4KKxu5Z8FGmj2+Q/b3+RX3vreJLXuPzcNfuqWMZ78oOGH7jxfTbdFgCFF8foUANptQ3diCzSbEhjuDbdYBPLlsOx9tKWNsVjxen5/88noeX5rPZRP0qMN7a5r5PH8fydFh7Ktv4eEl27j/sjHYREiMCjumc9W7vW3i/JcPt7alf11Yzbcn+FieV84zltiempPIyysLWbGjgsF9ojlvZN8DjrVqZyVPL9/JW2tL+OrOs7HZ5IB8n19hPyitqqGF7z+jx4C7Zkr/Q/K7g6B1W5w4caIyvVwMhuPD71dM/8tSzhiSwsT+Cdz633UAvHrjVCbnJB5l7+6hrtnDpHuXcMn4TO67dDQAj36Ux0NLtrHhnvOIcjl4fGk+D3y4lWW/nMkLX+3i6eU72/a/+4IR/OD0nA6dK7+sjtmPLMfj269nt547hK92VvJZ/r62tHCnDYfNRn07oZ/Dcdkpmfz1irFt259sK+e6+StZ/PMzGNwnBoBnvyjgtws2tpX56NYZDEyJ7vA5jgURyVX7Z0A6AOOhG056KurdvLq6CJ9fz48QHxnG1ZP7HeI1dTVLt5axsbiGMZnxnDEkhR3l9RRVNRHlcuC0C2My4zt8rNpmDy99tRuPz092chQXjEnv8L5KKX67YCPF1U28tHI3tc0e4iOd+P2KF77adUyC/sX2fWworiEhMowWn5+rJ/dDD4J5ZPx+xetrijh/TBqRYe3LyHvr99Ds8XP5xLY5QBiRFotS8OjHedwxaxiv5RYxOTuR7OQobj5zMDnJ0fj8fp5Ytp0nP9nOxOyEo17XyoYWfvLCGgDuuXAEdpvgsNuYOzadOWPSeHfdHh5asg2Af103iWiXg/VF1YgIWYmR7D5MY21mYiQPfLCVd9aVMCAlirlj01m6tYwHF+tj/XbBRk4bqOfeeGV1IUP7xDBtUDLzP9/JYx/n84NpOYzOjCN3VyUrth84AvDMoamMyog76nU+VoygG056Hl+6nfmf7zwgrX9SJNMHt/tuRZdQ1+zhx8/n0uzxExVmZ9Vd53D+o5/RFBB/3fGnOR2+ycz/bCcPL8lr2x6bGU9WYmSH9v1qZyXPfbm/K/J76/dwxpAU+idG8urqQmqaPMRFHD304vb6mPef3AO81bGZ8R0Smg837uW219azsaSWe+aObLfMf3OLGJQazfis+La08f30+j8+2cG0gcns3NfAT2YOBCAu0snVp/YD4IvtFby/YS9zH/v8qNf1759sZ1tpPd+ekMn10w706AemRPP/zh7E59v3MT4rnmmDknU9A2yCw/+OMuIjuPBvn/HAh1t5Ymk+DS37v+8vtlfwhSXUIvDwd8Yxe1QaH27cy5tfF/NNcQ0f/uwMbn7xa/bUNB9w3PjIsC4RdBNyMXQqFfVuvvevldQ2e7jl7MFcPlGP2f/P5Tv49+cFAFw7tT83ztB/4ldXF/LoR3kc6WdYXufmnBGpPHLleFq8fk67/2P8SrUbLz59UDJhDhv9EiP54RkDAHgtt4iHl2w75Bwzhqbw4xkD+cEzqxiXFc8Dl+9/rP7roq28sWb//AJur5999W5+NWcYf1q4hZQYF+V17gOOlxYXju0I3u2k7AQevnI8r64u5H9fW8+0QUncd8kYZvzfUhIiwzh/dBp/uHjU4S+Exa2vruODDXtY+etzmPPocnZVNPKjGQOZM7ovcx/7nJ+dM5gPNuylrnm/UA9IieKZ70/GbhNufXUdX+6owOPzU3ZQHU4flExBRcMB1yoyzM786ydhswnf//dK6pq9BwjUg1eM5dJT9nvhS7eU8dsFG9ld2cids4e1fdetfLBhDz96fg1D+kRTVNXEql+fQ5TrQN/yzjfW89JKPSPbuKx43vzJaYgI+WX13Pjcapo9/ray5XVuzhiSzNPXTuzQ08Wx4vMrfvP2Bl78andbWkZ8BMtum9m2LYDDrvuY+P2Kl1cV8qs3v6FPrIvSWjePXjWe2aP2x+ltIscdYzchF8MJk19WT05y1FF/hG+sKWbTnloy4iN4fGk+GQkRTMpO5K21WhyjXHae/GQ7107Npri6kceX5mMTYdIRwgR2G/zP9AE47Tacdhv3XjKKpVvKDym3u7KBV1bvn5ZxaN8YROCxj/MOOceuigZeWrmbqoYW8srqySur595LRhPm0H/K13KLiHDaGd8voW2frMQIfjh9APXNXoqrm3E5bUQ47fj8eujSevehvSVaKa5u5K21JcwcmspjH+fjtAu3zxpGv6RIfnP+CN5dX8LzX+3iJ2cOJC0u4rDHqXd7WfjNHi4en06Uy8HvLxrFwvV7uGJiJjnJUQzrG9Pm+V8yPgObCJUNbpZuLeez/H1kJkTw+poiJuckkpUQSUqMi3OGp7J5Ty33vb+Fz/L3kREfwZQBOpSgULy9toRnvyigoKKRbaX1bbb8aMZAnvp0O499nE9KjKst/ZGP8mjy+LhmSj+unNTvkDq0eqbbSuu5fELmIWIO8LNzhpAQGcYX2ytYW1jN81/tJjspkv+uLqKwqokLA0JUNoEfnJ7TJWIOYLcJP54xEL9fcdawVPbUNDOhfwJOe/udBG024ZLxGWwrraOu2UtCpJNZI/setnxnYjx0w1HJ3VXFZU9+wT0XjjjkkTYQpRTfeuhTosMdXDu1Pz9/RTfU3T5rGA8t3sb3T89m2sBkrp2/kuykSAoqGgH46+Vj23o9nAh7apqYet/H7eYd7EUWVjZy5v8tw+vf//t/96enMyojjsqGFk75w2J+NWcY884Y2N7hjpmKejen3f8xbq/2LB+5chwXjds/B/CuigZmPLCM284byk1nDjrcYXhl1W5uf/0bXv/xaUzon3BI/jOf7+SedzYxOSeRV2+cCujQyql/+ojTByWTlRjJU5/uYMUdZ5EaG37Avlc+tYIvd1Ty58tG850AIb7+3ytZtlXfQMdmxbOusJrLJ2TywOVjeS23iF9aDbKB3D5rGD+e2f61U0pxyh8WU9Xo4fUfT2VC/0RorITIQ2/qjS1epvzpI2oDnjYuGpfOI1eOb/8CNewDVww4XO3nBxt3HdSVQlQyRMQf1yFO2EMXkVnoeQDtwD+VUvcflN8PeBaIt8rcoZRaeFzWGoLOzn0NPLR4G16rEXJ7mW40WlfUfh/h57/cRe6uKiotb/dPl4zm4nEZDEyJ5q63NvDE0nxafH5GpMUybVAyaXHhbWIOMHt033aPe6ykxUWw9JczUUrR7PHT2KJFIMxhY1T6gfHKrMRIFv38DCobWnA57Fz42Gfc8vLX3DN3JP/4ZAcAI9M7L8aZFO1i0c/PoLzOTZjDxuiD4qf9k6I4NSeRf3++k40lh+mLrRSZBa/zZPRWTqmqhcQzIaYveJpg/auw/hWuzZzMlKvnkDZwXNturtpd/DH9Sx7a2I/lzjRmDknRYr5nHXzxN4hMBlcM/x7ooCpD0TchClSWDgw3VvID52IqJQY3Tl64OIdG0kmgFsq3cenQRKactYPy/hdi8zXhDYvDFhahvfCaYvjyCUgZBmljIToVfC1IeBwfXpdFfUM9AyrehXceg31bIXEgDDoHyjbBvm0Ql0VkRALLz5jG3ogh1PU9FfF7GVW1BDbugqRBgILaEti9ArYs1McBGDILrngOqnbC2zdB+VbwuiFjAsy+HxIHaOEPuLY07IPoFL1enAur/w35i6GlEeY8AEPOg8KV+po7wqFxHyQPha3vgc8DIy6ColVQU6QF2+bQ6d5myH0GSr4Gv3VjOv9BmHRDp/2+Wjmqh27Nl7gNPfdhEXpez6uUUpsCyjwFfK2UelJERgALlZ5D8LAYD/3k5aHF23j04zwGBXS7yiurZ2R6LO/+9PQDHm1rGj2M+8OitphrSoyLj2+dQYwV3/54Syl/fn8rEWF2nr52IikxLl5ZtZt/f16Awy7MHZt+eC/Y5wVfC4R1rLHwePH7FRc+9hkbrRdQYlw2JvW18di3hxGZcpjRERsrYeXTWnjcdRCfBTPv1H9kpSD337BzuRayATMgaTC4ovUffN82XUb5ILoPVBdC6jB2vvdX0tY/zueOqaxzjKZWYvjaMYZGiSLJX8FNzU8x3bsCryMSh7cRbE4YdxVsWgDN1Qfa5wiH8HgYMBO2fwQN2sOuk2h8aacQ7ymDinwQm168bg6YgL7PaEgZCgWfQf3ejl/MsBgYcwXMuB2evxRKNxx9H2cUZE7U59+5XF/DtLFQulHXo8pqEE8bB55Gff0OQWDgmdB/GlTuhLXPw8CzoGyzFtHB54GnATa+qYuHx8PoyyF7GpSshW0fQPkWiMvS56zI03YNna3PX5zb8WtwOAadC1mnQkJ/fWNJOr6nvyN56B0R9KnAPUqp86ztOwGUUvcFlPkHsEMp9Wer/F+VUqcd6bhG0E9e/ufZ1RRUNLDkFzPa0u5ZsJFnvijgJzMH8r+zhrWlP/flLn7z1v4/7brffuvIPSyqCmDPehj8LfjqSajaBcMu0H/GotXw1d8hLkOXK/gcmmu0UE6/FWxWDNLr1o/USkFL/X5Py+uGze/oz9Th0FSp/9x71kHeYn1jyJ6uj+mMhMQcfe7kwXiUsOCPV3AZH+G1uXDg00KQOEALw4Aztfe17UPoMxIKlkN9KcRmaAHasx7Sx8HQ82HHUtj1OUSltAkpoD3Kur3a5oOxOcHv0d6yp0mLTyvx/fW5/D4457cw5SYo+BS+ekp7hxkT4Ky7IGcG7MuDvA/19SteA3vWaiE8+27t+ZZv0dc1Jg0yToFTf6RvRqCvi7cFtrwDa1+C6t2QOoyq8T/hpVeeJzx1ID84NQ2aqvX1LdukhS5z0v4bQ94i7S238p3nIbov1JXA3g1gDwO7A8KiITxOLzkzwGmFf5qqwRlxYMhkz3rY+r4W6bgsmPgDUH6w2fXxIpP09xQT8KT3yV9g6b3gioXrFkC6FaLZsw4qtsO6l/WNzu/VnnS/qVpgm6r1zXH4hTDqMohI0L+nj/+o95twPfjc0Fyrv8eWBkjIhr3rdbkh52l7vG79hIPovMSB0G8K2E/8xa8TFfRvA7OUUv9jbX8POFUpdXNAmTRgEXouxSjgHKXUIbc0EZkHzAPo16/fhF27DjsKpKEL+XhLKaW1br4zMeuQ7mD5ZfWc8+AnzB2bzqNX7Y9TltY2c+ljy4n1lDFvfCQxzSU0uPqwast2tjhH8dD1Z1JS3cSpWVGw6zP9aDr4PNj8tn5krSnSnknus+Bt0h5SoFfpCNePpo4InR+ZpP9kfh9sex9GXgoz79Ahgq+f00LaXAstdfrPHJEAZVsOFMJAhswCdz2Ub9ael88N9WWA2n9OC1/ODOx9R+sQwa4VsGOZzhfrhmJzQs50OPNXWkwBNr4FC34K7lotYNP+H0z5iRbincstgc3VN6v08VpE7E6oL4eYPrDrCy0Mk/5HH6++THuopd9oYY7NgNNu1mUCaa7VN7T2GgR9Hu0hp41rP/8Y+GRbOeOy4o/eHdLvg+UPgiNMe8uZ7epO91C5U98w2onNA+BphsIvtdi23tRCgO4Q9F9Yx/qr5aH/CxillPK3e1CMhx4sWrx+htz1PgD/um4iZw/vozOaa2HDa9y+wsErxUn85bIxXDEp4EeetxjP6zfibK445Jg+Wxj2wedoEW+q1N5TIHaXDi/4vdoz7zdFC1j6eO3tuuu15xjdByZcp/dxRmrBUwoW3garnt5/vHHf1d5RdB+ISNSi5WnUwj50DsRlaq8uNl0LoCsGwmMPvRg1RZC/BPZ+o4U3Mgm+da8OjQTSsE+HJ1KG6YYsn1d7mQfjdeulvXMZDJ1Ed4RcNqJFv9Da3gFMUUqVHe64RtCDw4biGi7422cACH7eumk667ds5bQV8xjoL8CrbHyTfjnjr/2z9npb+ccMaKrCM/UW/GFRiLsOZXMi4TGErXoKaou1UDvCIXmIfgzfu143FEWlao+4qRpi047daKV06GDncu3x9T9iNM9g6NGcaC+XVcBgEckBioErgasPKrMbOBt4RkSGA+HAoR2FDd2CUoq1hdX0jQs/oE9zeZ2bJevyOde2mtv7riG74lP+896vmFDyIum2vbyeejPj6j9j/J5XYEkEXPiI3nHzu1pQZ/0Z56nttMyPuax9Q/pP3b9ui9Cx0eNBRHvz6eOPXtZg6MUcVdCVUl4RuRn4EN0lcb5SaqOI/B5YrZRaANwKPC0iP0c3lV+vgtXB3cDiTaXMey6X1BgXK+48G7tNUErxp388y711dxEZ5kZV2RHx84O9fwQbbDnjcS47y5qM/u2bYd0rkDoSdn4CWxdqMZ34/eBWzGAwHJEO9UO3+pQvPCjt7oD1TcC0zjXN0FE2FNfwVd5eru+7k9dX7+Lfe/oDirI6N5/l72PGkBTWF5Ryc+2DEBZO+Tl/I2XcbG7/v0e5x/MIBWmzGd4q5qC7nBV+Be/fpsMuU34CM/735H1Zw2AwAObV/5OCJmvAn3CnDbfXT7jTfkz7r37/Wc7Y/XfstmKuAK4A/BEOvlCjWbziNmb0n4F3wc8ZaNtDw7ffIGXY2QDM+vY8frHyWzz23YPCcfFZ8OMVuiEwIXt/lzKDwXBSY179DzL3vb+57c3EVo72+jfol2Ge+ONNXOv8iFj3Xrb5M3jBfhHlbgf3TagjLjoK91fzcfkb8TsjsXkaWZ5wCdNveaYLa2MwGLoaMzjXSUhlQwvvrCvhpa92Myk7gYgwB59u0+3Iz63YxY9mDGx3IKwPN+6luKqJiQ3LuNn/Al81DWOJOpdnPOfiwUGY3cbfLpkFNmFP4jR2LbiP5vAMnmk4hZ/Oub6ba2kwGLoTI+hB4tGP8njmiwJE4H9nDSMpKoyz/voJ35mYxSurC1meV87MoakH7LOjvJ4bn8slmRoWue5irRrI1S2/xi925s0YwD8+2cHpg5PbbgTZk+bwy9XxrN5VxeDUaKYM7L7xww0GQ/djQi6dwF1vfcP6ohqe+O4pZCYcfdyR1umqZo/qy1+vGNs244tSihavl0v+9AqlkkJilGv/G35KEde8m6ymLfwo+jOymzYx1/MnXvr19bgcNqJcDhrcXsKd9gM8e6/PT4PbR6TL3i3DdxoMhq7FhFw6gaYWH5v21HLAAEZAZYOH57/UA98/vjSfH04fwIDAQa1K66ht9gCQnRRFY4uP372zkXCnjVvOGXzA9F3iacL1/KUsVCvw4ETV2/g6+gwKwocxs/pN+niKwAF+t53bvTfQGD/ogIl02xtX2mG3ERdphNxg6A0YQe8gv393Y9sMKgfjsAnD02J5aWUhr64uYtkvZ5KVGHnAW5kAA1OicNYXk6Y83HvNHIb1DXhFvOAz+OBO/Rr66T/H6a4HbxOnfvM6p9YththMOOO3MPhcNtZG8t/5W/j1lOwurrXBYAglTMjlMDS1+Hh8aT6zR/fluRW7WLCuhNMHJXPNlEOHU02NddE3Npwvd1Tw4xfWcEq/BHKSo9i6t46tpXX8/ZpTWF1QxTPLNrLcdQtJUofKnITEpusBqPZ+A2tf1N0Fz/09jLxk/8G9bmis0AM+2fZ72nmldQxKje6yWVoMBsPJyQmN5dJVnOyC/ubXRW0z7thtQr/ESB67evxRJz24661vDpge7aJx6fzvrGHUVZVS9dg59PPtxj/gLGx71uqBrEAPJTruajjnHgiL6qIaGQyGnoCJoR8jb68tbhNzgIvGpvPgd8Z1aN8/Xjwa/H79ynzfMRCVBEoRs+R2YvzFMPsBbKfO04Vbh+/MnGSE3GAwnDBG0C2qGlrYVlrHqQOSuOXltW3pt503lEvGZxx+x0CUgtXz4dMHoG6PFvTTfgorn9KTI5z1G2gVc9BvYA6Y2an1MBgMvRcj6BZ3vvENH2zcyz+v3f8kc+n4jKO+sUl9mR6Lu64UltwDu7/Qs+Jknw5b3oM3fqhnVTn3DzD1pq6thMFg6NUYQbcorWsG4P+9/DVOu7DizrNJCugSCGgPfOXT8PV/tJA7wqE6YNal6D568tcJ39cNmF63bvCMStGz9RgMBkMXYgTdorVt+LSByUwdmERydMDIgjuXw+K7dRilbg9kTIScM/QUX5N/qEckbKrWDZuB0105XMGdgstgMPQqjKBbFFU1cuWkLO6/bMz+xJZGePNG2LxAz+nYfxoMOhvGXnXCczQaDAZDZ2MEHd3nfF99C5kJATPqeN3wynf1BMFn/lrHv01PFIPBcBJjBB3YXl4PQHayJdg7P4W3boKa3XDR4zD+miPsbTAYDCcHRtCBTSW1AIxIi4X6cnjtB9pDv+QfMPbKIFtnMBgMHcMIOrBpTy2RYXay453w4tXQXAPzPoE+I4JtmsFgMHSYXi/oSim+2L6PURlx2L55Rb/hOfcxI+YGgyHk6NXjqiql+NHzuWwrreficRnw9fOQPMTEzA0GQ0jSqwV9Q3EtH24sZUBKFBdlNeiZ7sdfY7okGgyGkKRXC/p/cwtxOWy8+ZNpRG16BcQOY0wjqMFgCE06JOgiMktEtopIvojccZgyV4jIJhHZKCIvdq6Znc8HG/bynxW7OG9kX+KkSY9HPvhciOkTbNMMBoPhuDhqo6iI2IHHgXOBImCViCxQSm0KKDMYuBOYppSqEpHU9o92cuD2+rjjjfUAXH9af3j1Wmgoh1N/FGTLDAaD4fjpSC+XyUC+UmoHgIi8DFwEbAoo80PgcaVUFYBSqqyzDe0MSqqbuOBvn1HX7MHjUzzz/Umc4t8AO5bCrPth4JnBNtFgMBiOm44IegYQOJlmEXDqQWWGAIjI54AduEcp9cHBBxKRecA8gH79+h2PvSfEhuIaKhta+M7ELAalRnPG4BR47149Y9CE73e7PQaDwdCZdFY/dAcwGJgJZAKfishopVR1YCGl1FPAU6CnoOukc3eYwqomAG6fPYzEqDA9xOLW93Xs3Bne3eYYDAZDp9KRRtFiICtgO9NKC6QIWKCU8iildgLb0AJ/UlFU1UhkmJ2ESKdOKN8C9aUw8OzgGmYwGAydQEcEfRUwWERyRCQMuBJYcFCZt9DeOSKSjA7B7Og8MzuHoqomshIikdZ+5pusauRMD55RBoPB0EkcVdCVUl7gZuBDYDPwqlJqo4j8XkTmWsU+BCpEZBOwFLhNKVXRVUYfD0optpXWkZUYqRNKN8Gy+/QY5wnZQbXNYDAYOoMOxdCVUguBhQel3R2wroBfWMtJydeF1eyqaOQnMwdaCc+B3QlX/Ce4hhkMBkMn0WveFP3v6iIinHbOH5MO3hZY/woMnQ1RycE2zWAwGDqFXiHoTS0+3llXwuzRfYl2OWDbB9BYAePMIFwGg6Hn0CsE/YONe6h3e7liotVZZ+0LEJMGA88KrmEGg8HQifQKQV+yuYy0uHAmZydCTRHkLdYzEdl7/XDwBoOhB9ErBH13RSOD+8RgswnkPgso82aowWDocfQKQS+qaiQzIUJvbHwTsqdDQv/gGmUwGAydTI8X9Hq3l6pGD1kJkVC+FSryYPiFwTbLYDAYOp0eFURWSvH8V7uZOyad/PJ6GtxeUmNdANpD32wN0z7s/CBaaTAYTgSPx0NRURHNzc3BNqVLCQ8PJzMzE6fT2eF9epSgry2s5jdvbWDpljI+3qJH8H3oO2MByImzwScvQsZEiE0PppkGg+EEKCoqIiYmhuzs7P3DePQwlFJUVFRQVFRETk5Oh/frEYL+nxUFLM/bx5lD9bwarWIO8PCSPOw2YejO/0Dldvjem8Ey02AwdALNzc09WswBRISkpCTKy8uPab+QF/Qd5fXc/fZGANYXVQNw/WnZRIbZeXV1EbsqGhneJwrnuucgZ4bpe24w9AB6spi3cjx1DGlB9/sV1/zzq7bt0lo30wcnc8/ckQBEOO38dfE2Lk/aDjt2wzn3BMlSg8Fg6HpCupfLih0VlNQ084Np+2NMf79mQtv6T84cxOKfn8F1rmUQkQjDLgiClQaDoSdRXV3NE088ccz7zZkzh+rq6s43KICQFvT/ri4kNtzB/84ayrVT+/PsDyYT5dr/0GG3CYOjmrBvfR/GXgUOVxCtNRgMPYHDCbrX6z3ifgsXLiQ+Pr6LrNKEbMjF7fXxwca9XHZKJuFOO7+/aNShhZSCRXcBCiaaN0MNhp7G797ZyKaS2k495oj0WH574cjD5t9xxx1s376dcePG4XQ6CQ8PJyEhgS1btrBt2zYuvvhiCgsLaW5u5pZbbmHevHkAZGdns3r1aurr65k9ezann346X3zxBRkZGbz99ttEREScsO0h66Fv21tPs8fPaQOPMPztx3/Qw+RO/yUkn3Qz4hkMhhDk/vvvZ+DAgaxdu5YHHniANWvW8Mgjj7Bt2zYA5s+fT25uLqtXr+bRRx+louLQuX7y8vK46aab2LhxI/Hx8bz++uudYlvIeuib9tQAMDI9tv0CKx6H5X+FCdfDzDu6zzCDwdBtHMmT7i4mT558QF/xRx99lDff1N2jCwsLycvLIykp6YB9cnJyGDduHAATJkygoKCgU2wJXUEvqSXa5aBf65RyAA0VsGetFvJdn+tG0PMfhF7QxclgMASHqKiotvVly5axZMkSVqxYQWRkJDNnzmz3jVaXa397nt1up6mpqVNsCVlB31nRyICUKD2CIkBRLjx3CbhrwBEBg8+DCx4Gmz2odhoMhp5FTEwMdXV17ebV1NSQkJBAZGQkW7Zs4csvv+xW20JW0IuqGhnWN0aPbf7ZQ7BnPUTEw7d+rz1zM7WcwWDoApKSkpg2bRqjRo0iIiKCPn36tOXNmjWLv//97wwfPpyhQ4cyZcqUbrVN9PzO3c/EiRPV6tWrj2tff0szD/7hZ8xJKWdE5RJwxUHWZPjWHyF1WCdbajAYTiY2b97M8OHDg21Gt9BeXUUkVyk1sb3yIemh1617m1/aX4JKK+GqlyB7WlBtMhgMhmATkoLesv0T3MrB6rlLmBa7D/qfFmyTDAaDIeh0qB+6iMwSka0iki8ih+0DKCKXiYgSkXYfBzqLiD2r+MI/kqjUHBh8runFYjAYDHRA0EXEDjwOzAZGAFeJyIh2ysUAtwBfHZzXqShFeH0h21U6seEh+YBhMBgMXUJHPPTJQL5SaodSqgV4GbionXJ/AP4MdO00Ik1VOHxN7FFJxEZ0fCYPg8Fg6Ol0RNAzgMKA7SIrrQ0ROQXIUkq9d6QDicg8EVktIquPdeD2NmqKAChWycSGG0E3GAyGVk54LBcRsQEPArceraxS6iml1ESl1MSUlJTjO6El6JX2FMIcITsUjcFgCFGOd/hcgIcffpjGxsZOtmg/HVHEYiArYDvTSmslBhgFLBORAmAKsKDLGkZr9anrwvscpaDBYDB0PiezoHekVXEVMFhEctBCfiVwdWumUqoGaHstU0SWAb9USh3fW0NHIzadtVHT8NnMm6AGQ6/n/Ttg7zede8y+o2H2/YfNDhw+99xzzyU1NZVXX30Vt9vNJZdcwu9+9zsaGhq44oorKCoqwufz8Zvf/IbS0lJKSko488wzSU5OZunSpZ1rNx0QdKWUV0RuBj4E7MB8pdRGEfk9sFoptaDTrToSw87nL8uTiPX6u/W0BoPBAHr43A0bNrB27VoWLVrEa6+9xsqVK1FKMXfuXD799FPKy8tJT0/nvfd0s2JNTQ1xcXE8+OCDLF26lOTkrnFIO9TvTym1EFh4UNrdhyk788TNOjK1zR5SY8K7+jQGg+Fk5wiedHewaNEiFi1axPjx4wGor68nLy+P6dOnc+utt3L77bdzwQUXMH369G6xJyQ7ctc0eRiUEh1sMwwGQy9HKcWdd97JjTfeeEjemjVrWLhwIXfddRdnn302d9/drg/cqYRkN5HqBg8JUWHBNsNgMPRCAofPPe+885g/fz719fUAFBcXU1ZWRklJCZGRkVxzzTXcdtttrFmz5pB9u4KQ89BbvH7q3F4SIo2gGwyG7idw+NzZs2dz9dVXM3XqVACio6N5/vnnyc/P57bbbsNms+F0OnnyyScBmDdvHrNmzSI9Pb1LGkVDbvjcsrpmJt/7EX+4eBTfm9K/CywzGAwnM2b43MMPnxtyIZeqBg8ACZHmLVGDwWAIJPQEvbEFgEQTcjEYDIYDCD1Bb9CCHm8E3WDotQQrVNydHE8dQ0/QG3XIJdH0cjEYeiXh4eFUVFT0aFFXSlFRUUF4+LG9bxNyvVxaQy7xJoZuMPRKMjMzKSoq4rhHbA0RwsPDyczMPKZ9Qq6Xi8+vqG0y/dANBkPvpEf1crHbxIi5wWAwtEPICbrBYDAY2scIusFgMPQQghZDF5FyYNdx7p4M7OtEc0IBU+fegalz7+BE6txfKdXulG9BE/QTQURWH65RoKdi6tw7MHXuHXRVnU3IxWAwGHoIRtANBoOhhxCqgv5UsA0IAqbOvQNT595Bl9Q5JGPoBoPBYDiUUPXQDQaDwXAQRtANBoOhhxBygi4is0Rkq4jki8gdwbansxCR+SJSJiIbAtISRWSxiORZnwlWuojIo9Y1WC8ipwTP8uNHRLJEZKmIbBKRjSJyi5XeY+stIuEislJE1ll1/p2VniMiX1l1e0VEwqx0l7Wdb+VnB7UCx4mI2EXkaxF519ru0fUFEJECEflGRNaKyGorrUt/2yEl6CJiBx4HZgMjgKtEZERwreo0ngFmHZR2B/CRUmow8JG1Dbr+g61lHvBkN9nY2XiBW5VSI4ApwE3W99mT6+0GzlJKjQXGAbNEZArwZ+AhpdQgoAq4wSp/A1BlpT9klQtFbgE2B2z39Pq2cqZSalxAn/Ou/W0rpUJmAaYCHwZs3wncGWy7OrF+2cCGgO2tQJq1ngZstdb/AVzVXrlQXoC3gXN7S72BSGANcCr6rUGHld72Owc+BKZa6w6rnATb9mOsZ6YlXmcB7wLSk+sbUO8CIPmgtC79bYeUhw5kAIUB20VWWk+lj1Jqj7W+F+hjrfe462A9Wo8HvqKH19sKP6wFyoDFwHagWinltYoE1qutzlZ+DZDUrQafOA8D/wv4re0kenZ9W1HAIhHJFZF5VlqX/rZDboKL3opSSolIj+xjKiLRwOvAz5RStSLSltcT662U8gHjRCQeeBMYFlyLug4RuQAoU0rlisjMIJvT3ZyulCoWkVRgsYhsCczsit92qHnoxUBWwHamldZTKRWRNADrs8xK7zHXQUScaDF/QSn1hpXc4+sNoJSqBpaiQw7xItLqYAXWq63OVn4cUNG9lp4Q04C5IlIAvIwOuzxCz61vG0qpYuuzDH3jnkwX/7ZDTdBXAYOtFvIw4EpgQZBt6koWANdZ69ehY8yt6ddaLeNTgJqAx7iQQbQr/i9gs1LqwYCsHltvEUmxPHNEJALdZrAZLezftoodXOfWa/Ft4GNlBVlDAaXUnUqpTKVUNvr/+rFS6rv00Pq2IiJRIhLTug58C9hAV/+2g91wcBwNDXOAbei446+DbU8n1uslYA/gQcfPbkDHDj8C8oAlQKJVVtC9fbYD3wATg23/cdb5dHSccT2w1lrm9OR6A2OAr606bwDuttIHACuBfOC/gMtKD7e28638AcGuwwnUfSbwbm+or1W/ddaysVWruvq3bV79NxgMhh5CqIVcDAaDwXAYjKAbDAZDD8EIusFgMPQQjKAbDAZDD8EIusFgMPQQjKAbDAZDD8EIusFgMPQQ/j+GpiIPI5YqLAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss learning curves\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Cross-Entropy Loss', pad=-40)\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "\n",
    "# plot accuracy learning curves\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy', pad=-40)\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have identified that the model is a good candidate for developing an ensemble, we can next look at developing a simple model averaging ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Averaging Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can develop a simple model averaging ensemble before developing a weighted average ensemble (covered in depth in Chapter 20). The results of the model averaging ensemble can be used as a point of comparison as we would expect a well-configured weighted average ensemble to perform better. First, we need to fit multiple models from which to develop an ensemble. We will define a function named `fit_model()` to create and fit a single model on the training dataset that we can repeatedly call to create as many models as we wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model on dataset\n",
    "def fit_model(trainX, trainy):\n",
    "    trainy_enc = to_categorical(trainy)\n",
    "    \n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, input_dim=2, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # fit model\n",
    "    model.fit(trainX, trainy_enc, epochs=500, verbose=0)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call this function to create a pool of 10 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "\n",
    "# fit all models\n",
    "n_members = 10\n",
    "members = [fit_model(trainX, trainy) for _ in range(n_members)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can develop a model averaging ensemble. We don't know how many members would be appropriate for this problem, so we can create ensembles with different sizes from one to 10 members and evaluate the performance of each on the test set. We can also evaluate the performance of each standalone model in the performance on the test set. This provides a useful point of comparison for the model averaging ensemble, as we expect that the ensemble will out-perform a randomly selected single model on average.\n",
    "\n",
    "Each model predicts the probabilities for each class label, e.g., has three outputs. A single prediction can be converted to a class label by using the `argmax()` function on the predicted probabilities, e.g., return the index in the prediction with the largest probability value. We can ensemble the predictions from multiple models by summing the probabilities for each class prediction and using the argmax() on the result. The ensemble predictions() function below implements this behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an ensemble prediction for multiclass classification\n",
    "def ensemble_predictions(members, testX):\n",
    "    # make predictions\n",
    "    yhats = [model.predict(testX) for model in members]\n",
    "    yhats = array(yhats)\n",
    "\n",
    "    # sum across ensemble members\n",
    "    summed = numpy.sum(yhats, axis=0)\n",
    "\n",
    "    # argmax across classes\n",
    "\n",
    "    result = argmax(summed, axis=1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can estimate the performance of an ensemble of a given size by selecting the required number of models from the list of all models, calling the `ensemble_predictions()` function to make a prediction, then calculating the accuracy of the prediction by comparing it to the true\n",
    "values. The `evaluate_n_members()` function below implements this behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a specific number of members in an ensemble\n",
    "def evaluate_n_members(members, n_members, testX, testy):\n",
    "    # select a subset of members\n",
    "    subset = members[:n_members]\n",
    "\n",
    "    # make prediction\n",
    "    yhat = ensemble_predictions(subset, testX)\n",
    "\n",
    "    # calculate accuracy\n",
    "    return accuracy_score(testy, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores of the ensembles of each size can be stored to be plotted later, and the scores for each model are collected, and the average performance reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 1: single=0.807, ensemble=0.807\n",
      "> 2: single=0.817, ensemble=0.814\n",
      "> 3: single=0.814, ensemble=0.812\n",
      "> 4: single=0.809, ensemble=0.813\n",
      "> 5: single=0.803, ensemble=0.817\n",
      "> 6: single=0.808, ensemble=0.817\n",
      "> 7: single=0.818, ensemble=0.817\n",
      "> 8: single=0.804, ensemble=0.816\n",
      "> 9: single=0.811, ensemble=0.815\n",
      "> 10: single=0.817, ensemble=0.814\n",
      "Accuracy 0.811 (0.005)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from numpy import array, argmax, mean, std\n",
    "import numpy\n",
    "\n",
    "# evaluate different numbers of ensembles on hold out set\n",
    "single_scores, ensemble_scores = list(), list()\n",
    "\n",
    "for i in range(1, len(members)+1):\n",
    "    # evaluate model with i members\n",
    "    ensemble_score = evaluate_n_members(members, i, testX, testy)\n",
    "\n",
    "    # evaluate the i'th model standalone\n",
    "    testy_enc = to_categorical(testy)\n",
    "    _, single_score = members[i-1].evaluate(testX, testy_enc, verbose=0)\n",
    "\n",
    "    # summarize this step\n",
    "    print('> %d: single=%.3f, ensemble=%.3f' % (i, single_score, ensemble_score))\n",
    "    ensemble_scores.append(ensemble_score)\n",
    "    single_scores.append(single_score)\n",
    "\n",
    "# summarize average accuracy of a single final model\n",
    "print('Accuracy %.3f (%.3f)' % (mean(single_scores), std(single_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create a graph that shows the accuracy of each model (blue dots) and the performance of the model averaging ensemble as the number of members is increased from one to 10 members (orange line). Tying all of this together, the complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 1: single=0.810, ensemble=0.810\n",
      "> 2: single=0.812, ensemble=0.805\n",
      "> 3: single=0.804, ensemble=0.812\n",
      "> 4: single=0.823, ensemble=0.819\n",
      "> 5: single=0.810, ensemble=0.818\n",
      "> 6: single=0.798, ensemble=0.814\n",
      "> 7: single=0.800, ensemble=0.811\n",
      "> 8: single=0.805, ensemble=0.811\n",
      "> 9: single=0.802, ensemble=0.812\n",
      "> 10: single=0.815, ensemble=0.814\n",
      "Accuracy 0.808 (0.007)\n"
     ]
    }
   ],
   "source": [
    "# model averaging ensemble for the blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from numpy import mean, std\n",
    "from numpy import array, argmax\n",
    "import numpy\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# fit model on dataset\n",
    "def fit_model(trainX, trainy):\n",
    "    trainy_enc = to_categorical(trainy)\n",
    "\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, input_dim=2, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # fit model\n",
    "    model.fit(trainX, trainy_enc, epochs=500, verbose=0)\n",
    "\n",
    "    return model\n",
    "\n",
    "# make an ensemble prediction for multi-class classification\n",
    "def ensemble_predictions(members, testX):\n",
    "    # make predictions\n",
    "    yhats = [model.predict(testX) for model in members]\n",
    "    yhats = array(yhats)\n",
    "\n",
    "    # sum across ensemble members\n",
    "    summed = numpy.sum(yhats, axis=0)\n",
    "\n",
    "    # argmax across classes\n",
    "    result = argmax(summed, axis=1)\n",
    "\n",
    "    return result\n",
    "\n",
    "# evaluate a specific number of members in an ensemble\n",
    "def evaluate_n_members(members, n_members, testX, testy):\n",
    "    # select a subset of members\n",
    "    subset = members[:n_members]\n",
    "\n",
    "    # make prediction\n",
    "    yhat = ensemble_predictions(subset, testX)\n",
    "\n",
    "    # calculate accuracy\n",
    "    return accuracy_score(testy, yhat)\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "\n",
    "# fit all models\n",
    "n_members = 10\n",
    "members = [fit_model(trainX, trainy) for _ in range(n_members)]\n",
    "\n",
    "# evaluate different numbers of ensembles on hold out set\n",
    "single_scores, ensemble_scores = list(), list()\n",
    "for i in range(1, len(members)+1):\n",
    "    # evaluate model with i members\n",
    "    ensemble_score = evaluate_n_members(members, i, testX, testy)\n",
    "\n",
    "    # evaluate the i'th model standalone\n",
    "    testy_enc = to_categorical(testy)\n",
    "    _, single_score = members[i-1].evaluate(testX, testy_enc, verbose=0)\n",
    "\n",
    "    # summarize this step\n",
    "    print('> %d: single=%.3f, ensemble=%.3f' % (i, single_score, ensemble_score))\n",
    "    ensemble_scores.append(ensemble_score)\n",
    "    single_scores.append(single_score)\n",
    "\n",
    "# summarize average accuracy of a single final model\n",
    "print('Accuracy %.3f (%.3f)' % (mean(single_scores), std(single_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first reports the performance of every single model and the model averaging ensemble of a given size with 1, 2, 3, etc., members.\n",
    "\n",
    "**Note:** Your specific results may vary, given the stochastic nature of the learning algorithm. Consider running the example a few times and compare the average performance.\n",
    "\n",
    "On this run, the average performance of the single models is reported at about 80.4%, and we can see that an ensemble with between five and nine members will achieve a performance between 80.8% and 81%. As expected, the performance of a modest-sized model averaging ensemble out-performs the performance of a randomly selected single model on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a graph compares the accuracy of single models (blue dots) to the model averaging ensemble of increasing size (orange line). On this run, the orange line of the ensembles clearly shows better or comparable performance (if dots are hidden) than the single models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot score vs number of ensemble members\n",
    "x_axis = [i for i in range(1, len(members)+1)]\n",
    "pyplot.plot(x_axis, single_scores, marker='o', linestyle='None')\n",
    "pyplot.plot(x_axis, ensemble_scores, marker='o')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to develop a model averaging ensemble, we can extend the approach one step further by weighting the contributions of the ensemble members."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Weighted Average Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model averaging ensemble allows each ensemble member to contribute an equal amount to the prediction of the ensemble. We can update the example so that instead, the contribution of each ensemble member is weighted by a coefficient that indicates the trust or expected performance of the model. Weight values are small values between 0 and 1 and are treated as a percentage, such that the weights across all ensemble members sum to one. First, we must update the `ensemble_predictions()` function to use a vector of weights for each ensemble member. Instead of simply summing the predictions across each ensemble member, we must calculate a weighted sum. We can implement this manually using for loops, but this is inefficient; for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculated a weighted sum of predictions\n",
    "def weighted_sum(weights, yhats):\n",
    "    rows = list()\n",
    "    for j in range(yhats.shape[1]):\n",
    "        # enumerate values\n",
    "        row = list()\n",
    "        \n",
    "        for k in range(yhats.shape[2]):\n",
    "            # enumerate members\n",
    "            value = 0.0\n",
    "            for i in range(yhats.shape[0]):\n",
    "                value += weights[i] * yhats[i,j,k]\n",
    "            row.append(value)\n",
    "        rows.append(row)\n",
    "    return array(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we can use efficient NumPy functions to implement the weighted sum such as `einsum()` or `tensordot().` Full discussion of these functions is a little out of scope, so please refer to the API documentation for more information on how to use these functions as they are challenging if you are new to linear algebra and/or NumPy. We will use the `tensordot()` function to apply the tensor product with the required summing; the updated `ensemble_predictions()` function is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import tensordot\n",
    "\n",
    "# make an ensemble prediction for multiclass classification\n",
    "def ensemble_predictions(members, weights, testX):\n",
    "    # make predictions\n",
    "    yhats = [model.predict(testX) for model in members]\n",
    "    yhats = array(yhats)\n",
    "\n",
    "    # weighted sum across ensemble members\n",
    "    summed = tensordot(yhats, weights, axes=((0),(0)))\n",
    "\n",
    "    # argmax across classes\n",
    "    result = argmax(summed, axis=1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we must update `evaluate_ensemble()` to pass along the weights when predicting the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a specific number of members in an ensemble\n",
    "def evaluate_ensemble(members, weights, testX, testy):\n",
    "    # make prediction\n",
    "    yhat = ensemble_predictions(members, weights, testX)\n",
    "\n",
    "    # calculate accuracy\n",
    "    return accuracy_score(testy, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a modest-sized ensemble of five members that appeared to perform well in the model averaging ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit all models\n",
    "n_members = 5\n",
    "members = [fit_model(trainX, trainy) for _ in range(n_members)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then estimate the performance of each model on the test dataset as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal Weights Score: 0.818\n"
     ]
    }
   ],
   "source": [
    "# evaluate averaging ensemble (equal weights)\n",
    "weights = [1.0/n_members for _ in range(n_members)]\n",
    "score = evaluate_ensemble(members, weights, testX, testy)\n",
    "print('Equal Weights Score: %.3f' % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can develop a weighted average ensemble. An exhaustive but straightforward approach to finding weights for the ensemble members is to grid search values. We can define a course grid of weight values from 0.0 to 1.0 in steps of 0.1, then generate all possible five-element vectors\n",
    "with those values. Generating all possible combinations is called a Cartesian product, which can be implemented in Python using the `itertools.product()` function from the standard library.\n",
    "\n",
    "A limitation of this approach is that the vectors of weights will not sum to one (called the unit norm), as required. We can force each generated weight vector to have a unit norm by calculating the sum of the absolute weight values (the L1 norm) and dividing each weight by that value. The `normalize()` function below implements this hack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize a vector to have unit norm\n",
    "def normalize(weights):\n",
    "    # calculate l1 vector norm\n",
    "    result = norm(weights, 1)\n",
    "\n",
    "    # check for a vector of all zeros\n",
    "    if result == 0.0:\n",
    "        return weights\n",
    "\n",
    "    # return normalized vector (unit norm)\n",
    "    return weights / result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now enumerate each weight vector generated by the Cartesian product, normalize it, and evaluate it by making a prediction and keeping the best to be used in our final weight averaging ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search weights\n",
    "def grid_search(members, testX, testy):\n",
    "    # define weights to consider\n",
    "    w = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    best_score, best_weights = 0.0, None\n",
    "    \n",
    "    # iterate all possible combinations (cartesian product)\n",
    "    for weights in product(w, repeat=len(members)):\n",
    "        # skip if all weights are equal\n",
    "        if len(set(weights)) == 1:\n",
    "            continue\n",
    "        \n",
    "        # hack, normalize weight vector\n",
    "        weights = normalize(weights)\n",
    "\n",
    "        # evaluate weights\n",
    "        score = evaluate_ensemble(members, weights, testX, testy)\n",
    "        if score > best_score:\n",
    "            best_score, best_weights = score, weights\n",
    "            print('>%s %.3f' % (best_weights, best_score))\n",
    "    \n",
    "    return list(best_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once discovered, we can report the performance of our weight average ensemble on the test dataset, which we would expect to be better than the best single model and ideally better than the model averaging ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "from itertools import product\n",
    "\n",
    "# grid search weights\n",
    "weights = grid_search(members, testX, testy)\n",
    "score = evaluate_ensemble(members, weights, testX, testy)\n",
    "print('Grid Search Weights: %s, Score: %.3f' % (weights, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: 0.818\n",
      "Model 2: 0.801\n",
      "Model 3: 0.810\n",
      "Model 4: 0.812\n",
      "Model 5: 0.805\n"
     ]
    }
   ],
   "source": [
    "# grid search for coefficients in a weighted average ensemble for the blobs problem\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from numpy import array, argmax, tensordot\n",
    "from numpy.linalg import norm\n",
    "from itertools import product\n",
    "\n",
    "# fit model on dataset\n",
    "def fit_model(trainX, trainy):\n",
    "    trainy_enc = to_categorical(trainy)\n",
    "\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, input_dim=2, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # fit model\n",
    "    model.fit(trainX, trainy_enc, epochs=500, verbose=0)\n",
    "\n",
    "    return model\n",
    "\n",
    "# make an ensemble prediction for multi-class classification\n",
    "def ensemble_predictions(members, weights, testX):\n",
    "    # make predictions\n",
    "    yhats = [model.predict(testX) for model in members]\n",
    "    yhats = array(yhats)\n",
    "\n",
    "    # weighted sum across ensemble members\n",
    "    summed = tensordot(yhats, weights, axes=((0),(0)))\n",
    "\n",
    "    # argmax across classes\n",
    "    result = argmax(summed, axis=1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# evaluate a specific number of members in an ensemble\n",
    "def evaluate_ensemble(members, weights, testX, testy):\n",
    "    # make prediction\n",
    "    yhat = ensemble_predictions(members, weights, testX)\n",
    "\n",
    "    # calculate accuracy\n",
    "    return accuracy_score(testy, yhat)\n",
    "\n",
    "# normalize a vector to have unit norm\n",
    "def normalize(weights):\n",
    "    # calculate l1 vector norm\n",
    "    result = norm(weights, 1)\n",
    "\n",
    "    # check for a vector of all zeros\n",
    "    if result == 0.0:\n",
    "        return weights\n",
    "\n",
    "    # return normalized vector (unit norm)\n",
    "    return weights / result\n",
    "\n",
    "# grid search weights\n",
    "def grid_search(members, testX, testy):\n",
    "    # define weights to consider\n",
    "    w = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    best_score, best_weights = 0.0, None\n",
    "\n",
    "    # iterate all possible combinations (cartesian product)\n",
    "    for weights in product(w, repeat=len(members)):\n",
    "        \n",
    "        # skip if all weights are equal\n",
    "        if len(set(weights)) == 1:\n",
    "            continue\n",
    "\n",
    "        # hack, normalize weight vector\n",
    "        weights = normalize(weights)\n",
    "\n",
    "        # evaluate weights\n",
    "        score = evaluate_ensemble(members, weights, testX, testy)\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score, best_weights = score, weights\n",
    "            print('>%s %.3f' % (best_weights, best_score))\n",
    "    \n",
    "    return list(best_weights)\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "\n",
    "# fit all models\n",
    "n_members = 5\n",
    "members = [fit_model(trainX, trainy) for _ in range(n_members)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first creates the five single models and evaluates their performance on the test dataset.\n",
    "\n",
    "**Note:** Your specific results may vary, given the stochastic nature of the learning algorithm. Consider running the example a few times and compare the average performance.\n",
    "\n",
    "We can see that model 2 has the best solo performance of about 81.7% accuracy on this run. Next, a model averaging ensemble is created with a performance of approximately 80.7%, which is reasonable compared to most of the models, but not all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: 0.818\n",
      "Model 2: 0.801\n",
      "Model 3: 0.810\n",
      "Model 4: 0.812\n",
      "Model 5: 0.805\n",
      "Equal Weights Score: 0.812\n"
     ]
    }
   ],
   "source": [
    "# evaluate each single model on the test set\n",
    "testy_enc = to_categorical(testy)\n",
    "for i in range(n_members):\n",
    "    _, test_acc = members[i].evaluate(testX, testy_enc, verbose=0)\n",
    "    print('Model %d: %.3f' % (i+1, test_acc))\n",
    "\n",
    "# evaluate averaging ensemble (equal weights)\n",
    "weights = [1.0/n_members for _ in range(n_members)]\n",
    "score = evaluate_ensemble(members, weights, testX, testy)\n",
    "print('Equal Weights Score: %.3f' % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the grid search is performed. It is pretty slow and may take about twenty minutes on modern hardware. The process could easily be made parallel using libraries such as Joblib. Each time a new top-performing set of weights is discovered, it is reported along with its performance on the test dataset. During the run, we can see that the process discovered that using model 2 alone resulted in a good performance until it was replaced with something better. We can see that the best performance was achieved on this run using the weights that focus only on the first and second models with an accuracy of 81.8% on the test dataset. This out-performs both the single models and the model averaging ensemble on the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# grid search weights\n",
    "weights = grid_search(members, testX, testy)\n",
    "score = evaluate_ensemble(members, weights, testX, testy)\n",
    "print('Grid Search Weights: %s, Score: %.3f' % (weights, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternate approach to finding weights would be a random search, which is more effective generally for model hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Average MLP Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative to searching for weight values is to use a directed optimization process. Optimization is a search process, but instead of sampling the space of possible solutions randomly or exhaustively, the search process uses any available information to make the next step in the search, such as toward a set of weights with lower error. The SciPy library offers many excellent optimization algorithms, including local and global search methods.\n",
    "\n",
    "SciPy provides an implementation of the Differential Evolution method. This is one of the few stochastic global search algorithms that just works for function optimization with continuous inputs, and it works well. The `differential_evolution()` SciPy function requires that function is specified to evaluate a set of weights and return a score to be minimized. We can minimize the classification error (1 - accuracy). As with the grid search, we most normalize the weight vector before we evaluate it. The loss function() function below will be used as the evaluation function during the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function for optimization process, designed to be minimized\n",
    "def loss_function(weights, members, testX, testy):\n",
    "    # normalize weights\n",
    "    normalized = normalize(weights)\n",
    "    \n",
    "    # calculate error rate\n",
    "    return 1.0 - evaluate_ensemble(members, normalized, testX, testy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must also specify the bounds of the optimization process. We can define the bounds as a five-dimensional hypercube (e.g., five weights for the five ensemble members) with values between 0.0 and 1.0.\n",
    "\n",
    "```\n",
    "# define bounds on each weight\n",
    "bound_w = [(0.0, 1.0) for _ in range(n_members)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our loss function requires three parameters in addition to the weights, which we will provide as a tuple to then be passed along to the call to the `loss_function()` each time a set of weights is evaluated.\n",
    "\n",
    "```\n",
    "# arguments to the loss function\n",
    "search_arg = (members, testX, testy)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now call our optimization process. We will limit the total number of iterations of the algorithms to 1,000 and use a smaller than default tolerance to detect if the search process has converged.\n",
    "\n",
    "```\n",
    "# global optimization of ensemble weights\n",
    "result = differential_evolution(loss_function, bound_w, search_arg, maxiter=1000, tol=1e-7)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the call to `differential_evolution()` is a dictionary that contains all kinds of information about the search. Importantly, the x key contains the optimal set of weights found during the search. We can retrieve the best set of weights, then report them and their performance on the test set when used in a weighted ensemble.\n",
    "\n",
    "```\n",
    "# get the chosen weights\n",
    "weights = normalize(result['x'])\n",
    "print('Optimized Weights: %s' % weights)\n",
    "\n",
    "# evaluate chosen weights\n",
    "score = evaluate_ensemble(members, weights, testX, testy)\n",
    "print('Optimized Weights Score: %.3f' % score)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tying all of this together, the complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global optimization to find coefficients for weighted ensemble on blobs problem\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from numpy import array, argmax, tensordot\n",
    "from numpy.linalg import norm\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "# fit model on dataset\n",
    "def fit_model(trainX, trainy):\n",
    "    trainy_enc = to_categorical(trainy)\n",
    "    \n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, input_dim=2, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # fit model\n",
    "    model.fit(trainX, trainy_enc, epochs=500, verbose=0)\n",
    "\n",
    "    return model\n",
    "\n",
    "# make an ensemble prediction for multi-class classification\n",
    "def ensemble_predictions(members, weights, testX):\n",
    "    # make predictions\n",
    "    yhats = [model.predict(testX) for model in members]\n",
    "    yhats = array(yhats)\n",
    "\n",
    "    # weighted sum across ensemble members\n",
    "    summed = tensordot(yhats, weights, axes=((0),(0)))\n",
    "\n",
    "    # argmax across classes\n",
    "    result = argmax(summed, axis=1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# evaluate a specific number of members in an ensemble\n",
    "def evaluate_ensemble(members, weights, testX, testy):\n",
    "    # make prediction\n",
    "    yhat = ensemble_predictions(members, weights, testX)\n",
    "\n",
    "    # calculate accuracy\n",
    "    return accuracy_score(testy, yhat)\n",
    "\n",
    "# normalize a vector to have unit norm\n",
    "def normalize(weights):\n",
    "    # calculate l1 vector norm\n",
    "    result = norm(weights, 1)\n",
    "\n",
    "    # check for a vector of all zeros\n",
    "    if result == 0.0:\n",
    "        return weights\n",
    "\n",
    "    # return normalized vector (unit norm)\n",
    "    return weights / result\n",
    "\n",
    "# loss function for optimization process, designed to be minimized\n",
    "def loss_function(weights, members, testX, testy):\n",
    "    # normalize weights\n",
    "    normalized = normalize(weights)\n",
    "\n",
    "    # calculate error rate\n",
    "    return 1.0 - evaluate_ensemble(members, normalized, testX, testy)\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "\n",
    "# split into train and test\n",
    "n_train = 100\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "\n",
    "# fit all models\n",
    "n_members = 5\n",
    "members = [fit_model(trainX, trainy) for _ in range(n_members)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first creates five single models and evaluates the performance of each on the test dataset.\n",
    "\n",
    "**Note:** Your specific results may vary, given the stochastic nature of the learning algorithm. Consider running the example a few times and compare the average performance.\n",
    "\n",
    "We can see on this run that models 3 and 4 both perform best with an accuracy of about 82.2%. Next, a model averaging ensemble with all five members is evaluated on the test set, reporting an accuracy of 81.8%, which is better than some, but not all, single models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1: 0.810\n",
      "Model 2: 0.816\n",
      "Model 3: 0.810\n",
      "Model 4: 0.815\n",
      "Model 5: 0.815\n",
      "Equal Weights Score: 0.816\n"
     ]
    }
   ],
   "source": [
    "# evaluate each single model on the test set\n",
    "testy_enc = to_categorical(testy)\n",
    "for i in range(n_members):\n",
    "    _, test_acc = members[i].evaluate(testX, testy_enc, verbose=0)\n",
    "    print('Model %d: %.3f' % (i+1, test_acc))\n",
    "\n",
    "# evaluate averaging ensemble (equal weights)\n",
    "weights = [1.0/n_members for _ in range(n_members)]\n",
    "score = evaluate_ensemble(members, weights, testX, testy)\n",
    "print('Equal Weights Score: %.3f' % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization process is relatively quick. We can see that the process found a set of weights that pays most attention to models 3 and 4 and spreads the remaining attention out among the other models, achieving an accuracy of about 82.4%, out-performing the model averaging ensemble and individual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Weights: [0.26649152 0.09657973 0.0100151  0.35467773 0.27223591]\n",
      "Optimized Weights Score: 0.820\n"
     ]
    }
   ],
   "source": [
    "# define bounds on each weight\n",
    "bound_w = [(0.0, 1.0) for _ in range(n_members)]\n",
    "\n",
    "# arguments to the loss function\n",
    "search_arg = (members, testX, testy)\n",
    "\n",
    "# global optimization of ensemble weights\n",
    "result = differential_evolution(loss_function, bound_w, search_arg, maxiter=1000, tol=1e-7)\n",
    "\n",
    "# get the chosen weights\n",
    "weights = normalize(result['x'])\n",
    "print('Optimized Weights: %s' % weights)\n",
    "\n",
    "# evaluate chosen weights\n",
    "score = evaluate_ensemble(members, weights, testX, testy)\n",
    "print('Optimized Weights Score: %.3f' % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that we have treated the test dataset as though it were a validation dataset in these examples, and this was done to keep the examples focused and technically simpler. In practice, the ensemble's choice and tuning of the weights would be chosen by a validation dataset, and single models, model averaging ensembles, and weighted ensembles would be compared on a separate test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section lists some ideas for extending the tutorial that you may wish to explore.\n",
    "\n",
    "* **Parallelize Grid Search**. Update the grid search example to use the Joblib library to parallelize weight evaluation.\n",
    "* **Implement Random Search**. Update the grid search example to use a random search of weight coefficients.\n",
    "* **Try a Local Search**. Try a local search procedure provided by the SciPy library instead of the global search and compare performance.\n",
    "* **Repeat Global Optimization**. Repeat the global optimization procedure multiple times for a given set of models to see if differing sets of weights can be found across the runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you discovered how to develop a weighted average ensemble of deep learning neural network models in Python with Keras. Specifically, you learned:\n",
    "\n",
    "* Model averaging ensembles are limited because they require that each ensemble member contribute equally to predictions.\n",
    "* Weighted average ensembles allow the contribution of each ensemble member to a prediction to be weighted proportionally to the trust or performance of the member on a holdout dataset.\n",
    "* How to implement a weighted average ensemble in Keras and compare results to a model averaging ensemble and standalone models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
