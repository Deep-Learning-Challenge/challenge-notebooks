{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Deep-Learning-Challenge/challenge-notebooks/blob/master/4.Advanced%20Topics/3.Better%20Predictions/1.%20Combine%20Models%20From%20Multiple%20Runs%20with%20Model%20Averaging%20Ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Models From Multiple Runs With Model Averaging Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning neural network models are highly flexible nonlinear algorithms capable of learning a nearly infinite number of mapping functions. Frustration with this flexibility is the high variance in a final model. The same neural network model trained on the same dataset may nd one of many different possible good enough solutions each time it is run. In this tutorial, you will discover how to develop a model averaging ensemble in Keras to reduce the variance in a final model. Model averaging is an ensemble learning technique that reduces the variance in a final neural network model, sacrificing spread (and possibly better scores) in the model's performance for confidence in what performance to expect from the model. After completing this tutorial, you will know:\n",
    "\n",
    "* Model averaging is an ensemble learning technique that can be used to reduce the expected variance of deep learning neural network models.\n",
    "* How to implement model averaging in Keras for classification and regression predictive modeling problems.\n",
    "* How to work through a multiclass classification problem and use model averaging to reduce the variance of the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Averaging Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning neural network models are nonlinear methods that learn via a stochastic training algorithm. This means that they are highly flexible, capable of learning complex relationships between variables and approximating any mapping function, given enough resources. A downside of this flexibility is that the models suffer high variance. This means that the models are highly dependent on the specific training data used to train the model and on the initial conditions (random initial weights) and serendipity during the training process. The result is a final model that makes different predictions each time the same model configuration is trained on the same dataset.\n",
    "\n",
    "This can be frustrating when training a final model to make predictions on new data, such as operationally or in a machine learning competition. The high variance of the approach can be addressed by training multiple models for the problem and combining their predictions. This approach is called model averaging and belongs to a family of techniques called ensemble learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to develop a model averaging ensemble in Keras is to train multiple models on the same dataset then combine the predictions from each of the trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Multiple Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training multiple models may be resource-intensive, depending on the size of the model and the size of the training data. You may have to train the models sequentially on the same hardware. For very large models, it may be worth training the models in parallel using cloud infrastructure\n",
    "such as Amazon Web Services.\n",
    "\n",
    "The number of models required for the ensemble may vary based on the complexity of the problem and model. A benefit of the approach is that you can continue creating models, adding them to the ensemble, and evaluating their impact on the performance by making predictions on a holdout test set. You can train the models sequentially for small models and keep them in memory for use in your experiment. For example:\n",
    "\n",
    "```\n",
    "...\n",
    "# train models and keep them in memory\n",
    "n_members = 10\n",
    "models = list()\n",
    "for _ in range(n_members):\n",
    "    # define and fit model\n",
    "    model = ...\n",
    "    \n",
    "    # store model in memory as ensemble member\n",
    "    models.add(models)\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large models, perhaps trained on different hardware, you can save each model to a file.\n",
    "\n",
    "```\n",
    "...\n",
    "# train models and keep them to file\n",
    "n_members = 10\n",
    "for i in range(n_members):\n",
    "    # define and fit model\n",
    "    model = ...\n",
    "\n",
    "    # save model to file\n",
    "    filename = 'model_' + str(i + 1) + '.h5'\n",
    "    model.save(filename)\n",
    "    \n",
    "    print('Saved: %s' % filename)\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models can then be loaded later. Small models can all be loaded into memory simultaneously, whereas very large models may have to be loaded one at a time to make a prediction, then later to have the predictions combined.\n",
    "\n",
    "```\n",
    "from tensorflow.keras.models import load_model\n",
    "...\n",
    "\n",
    "# load pre-trained ensemble members\n",
    "n_members = 10\n",
    "models = list()\n",
    "\n",
    "for i in range(n_members):\n",
    "    # load model\n",
    "    filename = 'model_' + str(i + 1) + '.h5'\n",
    "    model = load_model(filename)\n",
    "\n",
    "    # store in memory\n",
    "    models.append(model)\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the models have been prepared, each model can be used to predict, and the predictions can be combined. In a regression problem where each model predicts a real-valued output, the values can be collected, and the average is calculated.\n",
    "\n",
    "```\n",
    "...\n",
    "# make predictions\n",
    "yhats = [model.predict(testX) for model in models]\n",
    "yhats = array(yhats)\n",
    "\n",
    "# calculate average\n",
    "outcomes = mean(yhats)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of a classification problem, there are two options: to combine the predicted class labels or to combine the predicted probabilities. The class labels can be combined by calculating the statistical mode (most frequent value), for example:\n",
    "\n",
    "```\n",
    "...\n",
    "# make predictions\n",
    "yhats = [model.predict_classes(testX) for model in models]\n",
    "yhats = array(yhats)\n",
    "\n",
    "# calculate mode\n",
    "outcomes, _ = mode(yhats)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A downside of this approach is that for small ensembles or problems with a large number of classes, the sample of predictions may not be large enough for the mode to be meaningful. A sigmoid activation function is used on the output layer in a binary classification problem, and the average of the predicted probabilities can be calculated much like a regression problem. In the case of a multiclass classification problem with more than two classes, a softmax activation function is used on the output layer, and the sum of the probabilities for each predicted class can be calculated before taking the argmax to get the class value, for example:\n",
    "\n",
    "```\n",
    "...\n",
    "# make predictions\n",
    "yhats = [model.predict(testX) for model in models]\n",
    "yhats = array(yhats)\n",
    "\n",
    "# sum across ensembles\n",
    "summed = numpy.sum(yhats, axis=0)\n",
    "\n",
    "# argmax across classes\n",
    "outcomes = argmax(summed, axis=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These approaches for combining predictions of Keras models will work just as well for Multilayer Perceptron, Convolutional, and Recurrent Neural Networks. Now that we know how to average predictions from multiple neural network models in Keras, let us work through a case study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Averaging Ensemble Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will demonstrate how to use the model average ensemble to reduce the variance of an MLP on a simple multiclass classification problem. This example provides a template for applying the model average ensemble to your neural network for classification and regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a small multiclass classification problem as the basis to demonstrate a model averaging ensemble. The scikit-learn class provides the `make_blobs()` function that can be used to create a multiclass classification problem with the prescribed number of samples, input variables, classes, and variance of samples within a class. We use this problem with 500 examples, with two input variables (to represent the x and y coordinates of the points) and a standard deviation of 2.0 for points within each group. We will use the same random state (seed for the pseudorandom number generator) to ensure that we always get the same 500 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=500, centers=3, n_features=2, cluster_std=2, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are the input and output elements of a dataset that we can model. In order to get a feeling for the complexity of the problem, we can graph each point on a two-dimensional scatter plot and color each point by class value. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib import pyplot\n",
    "from numpy import where\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=500, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "\n",
    "# scatter plot for each class value\n",
    "for class_value in range(3):\n",
    "    # select indices of points with the class label\n",
    "    row_ix = where(y == class_value)\n",
    "\n",
    "    # scatter plot for points with a different color\n",
    "    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n",
    "\n",
    "# show plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example creates a scatter plot of the entire dataset. We can see that the standard deviation of 2.0 means that the classes are not linearly separable (separable by a line), causing many ambiguous points. This is desirable because the problem is non-trivial and will allow a neural network model to find many different *good enough* candidate solutions resulting in a high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Model for Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined a problem, we can define a model to address it. We will define a model that is perhaps under-constrained and not tuned to the problem. This is intentional to demonstrate the high variance of a neural network model seen on truly large and challenging supervised learning problems. \n",
    "\n",
    "The problem is a multiclass classification problem, and we will model it using a softmax activation function on the output layer. This means that the model will predict a vector with three elements with the probability that the sample belongs to each of the three classes. Therefore, the first step is to one-hot encode the class values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we must split the dataset into training and test sets. We will use the test set both to evaluate the model's performance and to plot its performance during training with a learning curve. We will use 30% of the data for training and 70% for the test set. This is an example of a challenging problem where we have more unlabeled examples than labeled ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "n_train = int(0.3 * X.shape[0])\n",
    "\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can define and compile the model. The model will expect samples with two input variables. The model then has a single hidden layer with 15 nodes and a rectified linear activation function, an output layer with three nodes to predict the probability of each of the three classes, and a softmax activation function. Because the problem is multiclass, we will use the categorical cross-entropy loss function to optimize the model and the efficient Adam flavor of stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(15, input_dim=2, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is fit for 200 training epochs, and we will evaluate each epoch on the test set, using the test set as a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=200, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the run, we will evaluate the model's performance on both the train and the test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then finally, we will plot model loss and accuracy learning curves over each training epoch on both the training and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss learning curves\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Cross-Entropy Loss', pad=-40)\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "\n",
    "# plot accuracy learning curves\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy', pad=-40)\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit high variance mlp on blobs classification problem\n",
    "from sklearn.datasets import make_blobs\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=500, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "y = to_categorical(y)\n",
    "\n",
    "# split into train and test\n",
    "n_train = int(0.3 * X.shape[0])\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(15, input_dim=2, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=200, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example, it first prints the performance of the final model on the train and test datasets.\n",
    "\n",
    "**Note:** Your specific results may vary, given the stochastic nature of the learning algorithm. Consider running the example a few times and compare the average performance.\n",
    "\n",
    "In this case, we can see that the model achieved about 84% accuracy on the training dataset and 76 % accuracy on the test dataset, which is not terrible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A line plot also shows the learning curves for the model accuracy on the train and test sets over each training epoch. We can see that the model is not really overfitting but is perhaps a little underfit and may benefit from an increase in capacity, more training, and perhaps some regularization. We intentionally hold back all of these improvements to force high variance for our case study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss learning curves\n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Cross-Entropy Loss', pad=-40)\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "\n",
    "# plot accuracy learning curves\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy', pad=-40)\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Variance of MLP Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to demonstrate that the model indeed has a variance in its prediction. We can demonstrate this by repeating the fit and evaluation of the same model configuration on the same dataset and summarizing the final performance of the model. To do this, we first split the fit and evaluation of the model out as a function that we can call repeatedly. The `evaluate_model()` function below takes the train, and the test dataset fits a model, then evaluates it, retuning the model's accuracy on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate a neural net model on the dataset\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15, input_dim=2, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # fit model\n",
    "    model.fit(trainX, trainy, epochs=200, verbose=0)\n",
    "\n",
    "    # evaluate the model\n",
    "    _, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "    \n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call this function 30 times, saving the test accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# repeated evaluation\n",
    "n_repeats = 30\n",
    "scores = list()\n",
    "for _ in range(n_repeats):\n",
    "    score = evaluate_model(trainX, trainy, testX, testy)\n",
    "    print('> %.3f' % score)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once collected, we can summarize the distribution scores, first in terms of the mean and standard deviation, assuming the distribution is Gaussian, which is very reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "# summarize the distribution of scores\n",
    "print('Scores Mean: %.3f, Standard Deviation: %.3f' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then summarize the distribution both as a histogram to show the shape of the distribution and as a box and whisker plot to show the spread and body of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of distribution\n",
    "pyplot.hist(scores, bins=10)\n",
    "pyplot.show()\n",
    "\n",
    "# boxplot of distribution\n",
    "pyplot.boxplot(scores)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete example of summarizing the variance of the MLP model on the chosen blobs dataset is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# demonstrate high variance of mlp model on blobs classification problem\n",
    "from sklearn.datasets import make_blobs\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from numpy import mean, std\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# fit and evaluate a neural net model on the dataset\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15, input_dim=2, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # fit model\n",
    "    model.fit(trainX, trainy, epochs=200, verbose=0)\n",
    "\n",
    "    # evaluate the model\n",
    "    _, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "    \n",
    "    return test_acc\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=500, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "y = to_categorical(y)\n",
    "\n",
    "# split into train and test\n",
    "n_train = int(0.3 * X.shape[0])\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "\n",
    "# repeated evaluation\n",
    "n_repeats = 30\n",
    "scores = list()\n",
    "for _ in range(n_repeats):\n",
    "    score = evaluate_model(trainX, trainy, testX, testy)\n",
    "    print('> %.3f' % score)\n",
    "    scores.append(score)\n",
    "\n",
    "# summarize the distribution of scores\n",
    "print('Scores Mean: %.3f, Standard Deviation: %.3f' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first prints the accuracy of each model on the test set, finishing with the mean and standard deviation of the sample of accuracy scores.\n",
    "\n",
    "**Note:** Your specific results may vary, given the stochastic nature of the learning algorithm. Consider running the example a few times and compare the average performance.\n",
    "\n",
    "In this case, we can see that the average of the sample is 76.9%, with a standard deviation of about 1.9%. Assuming a Gaussian distribution, we would expect 99% of accuracy scores to fall between about 73% and 81% (i.e., three standard deviations above and below the mean). We can take the standard deviation of the model's accuracy on the test set as an estimate for the variance of the predictions made by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A histogram of the accuracy scores is also created, showing a very rough Gaussian shape, perhaps with a longer right tail. A large sample and a different number of bins on the plot might better expose the true underlying shape of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of distribution\n",
    "pyplot.hist(scores, bins=10)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A box and whisker plot is also created, showing a line at the median at about 76.5% accuracy on the test set and the interquartile range or middle 50% of the samples between about 78% and 76%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot of distribution\n",
    "pyplot.boxplot(scores)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis of the sample of test scores demonstrates a variance in the performance of the same model trained on the same dataset. A spread of likely scores of about eight percentage points (81% to 73%) on the test set could reasonably be considered large, e.g., a high variance result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Averaging Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use model averaging to reduce the variance of the model and possibly reduce the model's generalization error. Specifically, this would result in a smaller standard deviation on the holdout test set and better performance on the training set. We can check both of these assumptions. First, we must develop a function to prepare and return a fit model on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model on dataset\n",
    "def fit_model(trainX, trainy):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15, input_dim=2, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # fit model\n",
    "    model.fit(trainX, trainy, epochs=200, verbose=0)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a function to take a list of ensemble members and predict an out-of-sample dataset. This could be one or more samples arranged in a two-dimensional array of samples and input features. Tip: you can use this function yourself to test ensembles and make predictions with ensembles on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an ensemble prediction for multiclass classification\n",
    "def ensemble_predictions(members, testX):\n",
    "    # make predictions\n",
    "    yhats = [model.predict(testX) for model in members]\n",
    "    yhats = array(yhats)\n",
    "\n",
    "    # sum across ensemble members\n",
    "    summed = numpy.sum(yhats, axis=0)\n",
    "\n",
    "    # argmax across classes\n",
    "    result = argmax(summed, axis=1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not know how many ensemble members will be appropriate for this problem. Therefore, we can perform a sensitivity analysis of the number of ensemble members and how it impacts test accuracy. This means we need a function that can evaluate a specified number of ensemble members and return the accuracy of a prediction combined from those members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a specific number of members in an ensemble\n",
    "def evaluate_n_members(members, n_members, testX, testy):\n",
    "    # select a subset of members\n",
    "    subset = members[:n_members]\n",
    "    print(len(subset))\n",
    "\n",
    "    # make prediction\n",
    "    yhat = ensemble_predictions(subset, testX)\n",
    "    \n",
    "    # calculate accuracy\n",
    "    return accuracy_score(testy, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create a line plot of the number of ensemble members (x-axis) versus the prediction accuracy averaged across that many members on the test dataset (y-axis).\n",
    "\n",
    "```\n",
    "# plot score vs number of ensemble members\n",
    "x_axis = [i for i in range(1, n_members+1)]\n",
    "pyplot.plot(x_axis, scores)\n",
    "pyplot.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model averaging ensemble and a study of ensemble size on test accuracy\n",
    "from sklearn.datasets import make_blobs\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot\n",
    "from numpy import array, argmax\n",
    "import numpy\n",
    "\n",
    "# fit model on dataset\n",
    "def fit_model(trainX, trainy):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15, input_dim=2, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # fit model\n",
    "    model.fit(trainX, trainy, epochs=200, verbose=0)\n",
    "\n",
    "    return model\n",
    "\n",
    "# make an ensemble prediction for multi-class classification\n",
    "def ensemble_predictions(members, testX):\n",
    "    # make predictions\n",
    "    yhats = [model.predict(testX) for model in members]\n",
    "    yhats = array(yhats)\n",
    "\n",
    "    # sum across ensemble members\n",
    "    summed = numpy.sum(yhats, axis=0)\n",
    "\n",
    "    # argmax across classes\n",
    "    result = argmax(summed, axis=1)\n",
    "\n",
    "    return result\n",
    "\n",
    "# evaluate a specific number of members in an ensemble\n",
    "def evaluate_n_members(members, n_members, testX, testy):\n",
    "    # select a subset of members\n",
    "    subset = members[:n_members]\n",
    "\n",
    "    print(len(subset))\n",
    "    # make prediction\n",
    "    yhat = ensemble_predictions(subset, testX)\n",
    "\n",
    "    # calculate accuracy\n",
    "    return accuracy_score(testy, yhat)\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=500, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "\n",
    "# split into train and test\n",
    "n_train = int(0.3 * X.shape[0])\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "trainy = to_categorical(trainy)\n",
    "\n",
    "# fit all models\n",
    "n_members = 20\n",
    "members = [fit_model(trainX, trainy) for _ in range(n_members)]\n",
    "\n",
    "# evaluate different numbers of ensembles\n",
    "scores = list()\n",
    "for i in range(1, n_members+1):\n",
    "    score = evaluate_n_members(members, i, testX, testy)\n",
    "    print('> %.3f' % score)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first fits 20 models on the same training dataset, which may take less than a minute on modern hardware. Then, different-sized ensembles are tested from 1 to all 20 members, and test accuracy results are printed for each ensemble size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a line plot shows the relationship between ensemble size and performance on the test set. We can see that performance improves to about five members, after which performance plateaus around 77% accuracy. This is close to the average test set performance observed during the analysis of the repeated evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot score vs number of ensemble members\n",
    "x_axis = [i for i in range(1, n_members+1)]\n",
    "pyplot.plot(x_axis, scores)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can update the repeated evaluation experiment to use an ensemble of five models instead of a single model and compare the distribution of scores. The complete example of a repeated evaluated five-member ensemble of the blobs dataset is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# repeated evaluation of model averaging ensemble on blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot\n",
    "from numpy import array, argmax\n",
    "import numpy\n",
    "\n",
    "# fit model on dataset\n",
    "def fit_model(trainX, trainy):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15, input_dim=2, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # fit model\n",
    "    model.fit(trainX, trainy, epochs=200, verbose=0)\n",
    "\n",
    "    return model\n",
    "\n",
    "# make an ensemble prediction for multi-class classification\n",
    "def ensemble_predictions(members, testX):\n",
    "    # make predictions\n",
    "    yhats = [model.predict(testX) for model in members]\n",
    "    yhats = array(yhats)\n",
    "\n",
    "    # sum across ensemble members\n",
    "    summed = numpy.sum(yhats, axis=0)\n",
    "\n",
    "    # argmax across classes\n",
    "    result = argmax(summed, axis=1)\n",
    "\n",
    "    return result\n",
    "\n",
    "# evaluate ensemble model\n",
    "def evaluate_members(members, testX, testy):\n",
    "    # make prediction\n",
    "    yhat = ensemble_predictions(members, testX)\n",
    "    \n",
    "    # calculate accuracy\n",
    "    return accuracy_score(testy, yhat)\n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = make_blobs(n_samples=500, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "\n",
    "# split into train and test\n",
    "n_train = int(0.3 * X.shape[0])\n",
    "trainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "trainy, testy = y[:n_train], y[n_train:]\n",
    "trainy = to_categorical(trainy)\n",
    "\n",
    "# repeated evaluation\n",
    "n_repeats = 30\n",
    "n_members = 5\n",
    "scores = list()\n",
    "\n",
    "for _ in range(n_repeats):\n",
    "    # fit all models\n",
    "    members = [fit_model(trainX, trainy) for _ in range(n_members)]\n",
    "\n",
    "    # evaluate ensemble\n",
    "    score = evaluate_members(members, testX, testy)\n",
    "    print('> %.3f' % score)\n",
    "    scores.append(score)\n",
    "\n",
    "# summarize the distribution of scores\n",
    "print('Scores Mean: %.3f, Standard Deviation: %.3f' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example may take a few minutes as five models are fit and evaluated, and this process is repeated 30 times. The performance of each model on the test set is printed to provide an indication of progress. The mean and standard deviation of the model performance is printed\n",
    "at the end of the run.\n",
    "\n",
    "**Note:** Your specific results may vary, given the stochastic nature of the learning algorithm. Consider running the example a few times and compare the average performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that the average performance of a five-member ensemble on the dataset is 76.7%. This is very close to the average of 77% seen for a single model. The important difference is the standard deviation shrinking from 1.9% for a single model to 0.5% with an ensemble of five models. We might expect that a given ensemble of five models on this problem to have a performance fall between about 74% and about 78%, with a likelihood of 99%.\n",
    "\n",
    "Averaging the same model trained on the same dataset gives us a spread for improved reliability, a property often highly desired in a final model to be used operationally. More models in the ensemble will further decrease the standard deviation of the accuracy of an ensemble on the test dataset given the law of large numbers, at least to the point of diminishing returns. This demonstrates that for this specific model and prediction problem, that a model averaging ensemble with five members is sufficient to reduce the variance of the model. This reduction in variance, in turn, also means a better on-average performance when preparing a final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section lists some ideas for extending the tutorial that you may wish to explore.\n",
    "\n",
    "* **Average Class Prediction**. Update the example to average the class integer prediction instead of the class probability prediction and compare results.\n",
    "* **Save and Load Models**. Update the example to save ensemble members to a file, then load them from a separate script for evaluation.\n",
    "* **Sensitivity of Variance**. Create a new example that performs a sensitivity analysis of the number of ensemble members on the standard deviation of model performance on the test set over a given number of repeats and report the point of diminishing returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you discovered how to develop a model averaging ensemble in Keras to reduce the variance in a final model. Specifically, you learned:\n",
    "\n",
    "* Model averaging is an ensemble learning technique that can be used to reduce the expected variance of deep learning neural network models.\n",
    "* How to implement model averaging in Keras for classification and regression predictive modeling problems.\n",
    "* How to work through a multiclass classification problem and use model averaging to reduce the variance of the final model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
